{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/fpolygon/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/fpolygon/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the preprocess_text function\n",
    "def preprocess_text(text):\n",
    "    # Remove HTML tags and special characters\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words and word.isalnum()]\n",
    "    \n",
    "    # Lemmatize the tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "    \n",
    "    # Join the tokens back together\n",
    "    preprocessed_text = ' '.join(lemmatized_tokens)\n",
    "    return preprocessed_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame: \n",
      "                                              review sentiment\n",
      "0  One of the other reviewers has mentioned that ...  positive\n",
      "1  A wonderful little production. <br /><br />The...  positive\n",
      "2  I thought this was a wonderful way to spend ti...  positive\n",
      "3  Basically there's a family where a little boy ...  negative\n",
      "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
      "\n",
      "Preprocessed DataFrame: \n",
      "                                              review  sentiment  \\\n",
      "0  One of the other reviewers has mentioned that ...          1   \n",
      "1  A wonderful little production. <br /><br />The...          1   \n",
      "2  I thought this was a wonderful way to spend ti...          1   \n",
      "3  Basically there's a family where a little boy ...          0   \n",
      "4  Petter Mattei's \"Love in the Time of Money\" is...          1   \n",
      "\n",
      "                                 preprocessed_review  \n",
      "0  One reviewer mentioned watching 1 Oz episode y...  \n",
      "1  wonderful little production filming technique ...  \n",
      "2  thought wonderful way spend time hot summer we...  \n",
      "3  Basically there family little boy Jake think t...  \n",
      "4  Petter Matteis Love Time Money visually stunni...  \n",
      "\n",
      "Preprocessing complete. Data saved to 'data/processed/IMDB Dataset Preprocessed Exploratory.csv'\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the IMDB Dataset\n",
    "df = pd.read_csv(\"/Users/fpolygon/Documents/Sentiment Analysis of Movie Reviews/data/raw/IMDB Dataset.csv\")\n",
    "print(\"Original DataFrame: \")\n",
    "print(df.head())\n",
    "\n",
    "# Apply preprocessing to the entire DataFrame\n",
    "df['preprocessed_review'] = df['review'].apply(preprocess_text)\n",
    "\n",
    "# Convert sentiment labels to 1 (positive) and 0 (negative)\n",
    "df['sentiment'] = df['sentiment'].map({'positive': 1, 'negative': 0})\n",
    "\n",
    "print(\"\\nPreprocessed DataFrame: \")\n",
    "print(df.head())\n",
    "\n",
    "# Save the preprocessed data\n",
    "df.to_csv(\"/Users/fpolygon/Documents/Sentiment Analysis of Movie Reviews/data/exploratory/IMDB Dataset Preprocessed Exploratory.csv\", index=False)\n",
    "\n",
    "print(\"\\nPreprocessing complete. Data saved to 'data/processed/IMDB Dataset Preprocessed Exploratory.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag-of-Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of Words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "df = pd.read_csv(\"/Users/fpolygon/Documents/Sentiment Analysis of Movie Reviews/data/exploratory/IMDB Dataset Preprocessed Exploratory.csv\")\n",
    "\n",
    "# Get the sentiment labels from the preprocessed DataFrame\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the preprocessed reviews\n",
    "bow_features = vectorizer.fit_transform(df['preprocessed_review'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-20 00:36:08,009 - INFO - Starting Bag of Words Naive Bayes classification\n",
      "2024-08-20 00:36:08,009 - INFO - Fitting and transforming preprocessed reviews using CountVectorizer\n",
      "2024-08-20 00:36:10,875 - INFO - Shape of BoW features: (50000, 216000)\n",
      "2024-08-20 00:36:10,875 - INFO - Extracting sentiment labels\n",
      "2024-08-20 00:36:10,876 - INFO - Number of labels: 50000\n",
      "2024-08-20 00:36:10,876 - INFO - Splitting data into training and testing sets\n",
      "2024-08-20 00:36:10,893 - INFO - Training set shape: (40000, 216000), Testing set shape: (10000, 216000)\n",
      "2024-08-20 00:36:10,894 - INFO - Creating and training the Naive Bayes model\n",
      "2024-08-20 00:36:10,909 - INFO - Model training completed\n",
      "2024-08-20 00:36:10,909 - INFO - Making predictions on the testing set\n",
      "2024-08-20 00:36:10,916 - INFO - Predictions completed\n",
      "2024-08-20 00:36:10,916 - INFO - Evaluating model performance\n",
      "2024-08-20 00:36:10,925 - INFO - BoW Naive Bayes Model Performance:\n",
      "2024-08-20 00:36:10,926 - INFO - Accuracy: 0.8612\n",
      "2024-08-20 00:36:10,926 - INFO - Precision: 0.8615\n",
      "2024-08-20 00:36:10,926 - INFO - Recall: 0.8612\n",
      "2024-08-20 00:36:10,927 - INFO - F1 Score: 0.8612\n",
      "2024-08-20 00:36:10,927 - INFO - Bag of Words Naive Bayes classification completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW Naive Bayes Model Performance:\n",
      "Accuracy: 0.8612\n",
      "Precision: 0.8615\n",
      "Recall: 0.8612\n",
      "F1 Score: 0.8612\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Bag of Words\n",
    "logging.info(\"Starting Bag of Words Naive Bayes classification\")\n",
    "\n",
    "# Fit and transform the preprocessed reviews\n",
    "logging.info(\"Fitting and transforming preprocessed reviews using CountVectorizer\")\n",
    "vectorizer = CountVectorizer()\n",
    "bow_features = vectorizer.fit_transform(df['preprocessed_review'])\n",
    "logging.info(f\"Shape of BoW features: {bow_features.shape}\")\n",
    "\n",
    "# Get the sentiment labels from the preprocessed DataFrame\n",
    "logging.info(\"Extracting sentiment labels\")\n",
    "labels = df['sentiment']\n",
    "logging.info(f\"Number of labels: {len(labels)}\")\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "logging.info(\"Splitting data into training and testing sets\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(bow_features, labels, test_size=0.2, random_state=42)\n",
    "logging.info(f\"Training set shape: {X_train.shape}, Testing set shape: {X_test.shape}\")\n",
    "\n",
    "# Create and train the Naive Bayes model\n",
    "logging.info(\"Creating and training the Naive Bayes model\")\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)\n",
    "logging.info(\"Model training completed\")\n",
    "\n",
    "# Make predictions on the testing set\n",
    "logging.info(\"Making predictions on the testing set\")\n",
    "y_pred = model.predict(X_test)\n",
    "logging.info(\"Predictions completed\")\n",
    "\n",
    "# Evaluate the model's performance\n",
    "logging.info(\"Evaluating model performance\")\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')  # Using weighted average for multi-class\n",
    "recall = recall_score(y_test, y_pred, average='weighted')  # Using weighted average for multi-class\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')  # Using weighted average for multi-class\n",
    "\n",
    "logging.info(\"BoW Naive Bayes Model Performance:\")\n",
    "logging.info(f\"Accuracy: {accuracy:.4f}\")\n",
    "logging.info(f\"Precision: {precision:.4f}\")\n",
    "logging.info(f\"Recall: {recall:.4f}\")\n",
    "logging.info(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "print(\"BoW Naive Bayes Model Performance:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "logging.info(\"Bag of Words Naive Bayes classification completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-20 00:36:10,933 - INFO - Starting Bag of Words Logistic Regression classification\n",
      "2024-08-20 00:36:10,933 - INFO - Extracting sentiment labels\n",
      "2024-08-20 00:36:10,934 - INFO - Number of labels: 50000\n",
      "2024-08-20 00:36:10,934 - INFO - Splitting data into training and testing sets\n",
      "2024-08-20 00:36:10,951 - INFO - Training set shape: (40000, 216000), Testing set shape: (10000, 216000)\n",
      "2024-08-20 00:36:10,952 - INFO - Creating and training the Logistic Regression model\n",
      "2024-08-20 00:36:10,953 - INFO - Logistic Regression parameters: {'C': 1.0, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 2000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': None, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}\n",
      "2024-08-20 00:36:16,256 - INFO - Model training completed\n",
      "2024-08-20 00:36:16,257 - INFO - Making predictions on the testing set\n",
      "2024-08-20 00:36:16,261 - INFO - Predictions completed\n",
      "2024-08-20 00:36:16,264 - INFO - Evaluating model performance\n",
      "2024-08-20 00:36:16,287 - INFO - BoW Logistic Regression Model Performance:\n",
      "2024-08-20 00:36:16,288 - INFO - Accuracy: 0.8854\n",
      "2024-08-20 00:36:16,290 - INFO - Precision: 0.8855\n",
      "2024-08-20 00:36:16,291 - INFO - Recall: 0.8854\n",
      "2024-08-20 00:36:16,292 - INFO - F1 Score: 0.8854\n",
      "2024-08-20 00:36:16,293 - INFO - Bag of Words Logistic Regression classification completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW Logistic Regression Model Performance:\n",
      "Accuracy: 0.8854\n",
      "Precision: 0.8855\n",
      "Recall: 0.8854\n",
      "F1 Score: 0.8854\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "logging.info(\"Starting Bag of Words Logistic Regression classification\")\n",
    "\n",
    "# Get the sentiment labels from the preprocessed DataFrame\n",
    "logging.info(\"Extracting sentiment labels\")\n",
    "labels = df['sentiment']\n",
    "logging.info(f\"Number of labels: {len(labels)}\")\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "logging.info(\"Splitting data into training and testing sets\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(bow_features, labels, test_size=0.2, random_state=42)\n",
    "logging.info(f\"Training set shape: {X_train.shape}, Testing set shape: {X_test.shape}\")\n",
    "\n",
    "# Create and train the logistic regression model\n",
    "logging.info(\"Creating and training the Logistic Regression model\")\n",
    "model = LogisticRegression(max_iter=2000)  # Increase max_iter to allow more iterations\n",
    "logging.info(f\"Logistic Regression parameters: {model.get_params()}\")\n",
    "model.fit(X_train, y_train)\n",
    "logging.info(\"Model training completed\")\n",
    "\n",
    "# Make predictions on the testing set\n",
    "logging.info(\"Making predictions on the testing set\")\n",
    "y_pred = model.predict(X_test)\n",
    "logging.info(\"Predictions completed\")\n",
    "\n",
    "# Evaluate the model's performance\n",
    "logging.info(\"Evaluating model performance\")\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')  # Using weighted average for multi-class\n",
    "recall = recall_score(y_test, y_pred, average='weighted')  # Using weighted average for multi-class\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')  # Using weighted average for multi-class\n",
    "\n",
    "logging.info(\"BoW Logistic Regression Model Performance:\")\n",
    "logging.info(f\"Accuracy: {accuracy:.4f}\")\n",
    "logging.info(f\"Precision: {precision:.4f}\")\n",
    "logging.info(f\"Recall: {recall:.4f}\")\n",
    "logging.info(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "print(\"BoW Logistic Regression Model Performance:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "logging.info(\"Bag of Words Logistic Regression classification completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-20 00:36:16,359 - INFO - Starting Bag of Words K-Nearest Neighbors classification\n",
      "2024-08-20 00:36:16,360 - INFO - Fitting and transforming preprocessed reviews using CountVectorizer\n",
      "2024-08-20 00:36:19,281 - INFO - Shape of BoW features: (50000, 216000)\n",
      "2024-08-20 00:36:19,282 - INFO - Extracting sentiment labels\n",
      "2024-08-20 00:36:19,282 - INFO - Number of labels: 50000\n",
      "2024-08-20 00:36:19,282 - INFO - Splitting data into training and testing sets\n",
      "2024-08-20 00:36:19,302 - INFO - Training set shape: (40000, 216000), Testing set shape: (10000, 216000)\n",
      "2024-08-20 00:36:19,302 - INFO - Creating and training the K-Nearest Neighbors model\n",
      "2024-08-20 00:36:19,303 - INFO - KNN parameters: {'algorithm': 'auto', 'leaf_size': 30, 'metric': 'minkowski', 'metric_params': None, 'n_jobs': None, 'n_neighbors': 5, 'p': 2, 'weights': 'uniform'}\n",
      "2024-08-20 00:36:19,309 - INFO - Model training completed\n",
      "2024-08-20 00:36:19,309 - INFO - Making predictions on the testing set\n",
      "2024-08-20 00:36:35,849 - INFO - Predictions completed\n",
      "2024-08-20 00:36:35,850 - INFO - Evaluating model performance\n",
      "2024-08-20 00:36:35,859 - INFO - BoW K-Nearest Neighbors Model Performance:\n",
      "2024-08-20 00:36:35,860 - INFO - Accuracy: 0.6168\n",
      "2024-08-20 00:36:35,860 - INFO - Precision: 0.6265\n",
      "2024-08-20 00:36:35,860 - INFO - Recall: 0.5932\n",
      "2024-08-20 00:36:35,860 - INFO - F1 Score: 0.6094\n",
      "2024-08-20 00:36:35,861 - INFO - Bag of Words K-Nearest Neighbors classification completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW K-Nearest Neighbors Model Performance:\n",
      "Accuracy: 0.6168\n",
      "Precision: 0.6265\n",
      "Recall: 0.5932\n",
      "F1 Score: 0.6094\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "logging.info(\"Starting Bag of Words K-Nearest Neighbors classification\")\n",
    "\n",
    "# Fit and transform the preprocessed reviews\n",
    "logging.info(\"Fitting and transforming preprocessed reviews using CountVectorizer\")\n",
    "vectorizer = CountVectorizer()\n",
    "bow_features = vectorizer.fit_transform(df['preprocessed_review'])\n",
    "logging.info(f\"Shape of BoW features: {bow_features.shape}\")\n",
    "\n",
    "# Get the sentiment labels from the preprocessed DataFrame\n",
    "logging.info(\"Extracting sentiment labels\")\n",
    "labels = df['sentiment']\n",
    "logging.info(f\"Number of labels: {len(labels)}\")\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "logging.info(\"Splitting data into training and testing sets\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(bow_features, labels, test_size=0.2, random_state=42)\n",
    "logging.info(f\"Training set shape: {X_train.shape}, Testing set shape: {X_test.shape}\")\n",
    "\n",
    "# Create and train the K-Nearest Neighbors model\n",
    "logging.info(\"Creating and training the K-Nearest Neighbors model\")\n",
    "model = KNeighborsClassifier(n_neighbors=5)  # You can adjust the number of neighbors\n",
    "logging.info(f\"KNN parameters: {model.get_params()}\")\n",
    "model.fit(X_train, y_train)\n",
    "logging.info(\"Model training completed\")\n",
    "\n",
    "# Make predictions on the testing set\n",
    "logging.info(\"Making predictions on the testing set\")\n",
    "y_pred = model.predict(X_test)\n",
    "logging.info(\"Predictions completed\")\n",
    "\n",
    "# Evaluate the model's performance\n",
    "logging.info(\"Evaluating model performance\")\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "logging.info(\"BoW K-Nearest Neighbors Model Performance:\")\n",
    "logging.info(f\"Accuracy: {accuracy:.4f}\")\n",
    "logging.info(f\"Precision: {precision:.4f}\")\n",
    "logging.info(f\"Recall: {recall:.4f}\")\n",
    "logging.info(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "print(\"BoW K-Nearest Neighbors Model Performance:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "logging.info(\"Bag of Words K-Nearest Neighbors classification completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-20 00:36:35,868 - INFO - Starting Bag of Words Support Vector Machine classification\n",
      "2024-08-20 00:36:35,868 - INFO - Fitting and transforming preprocessed reviews using CountVectorizer\n",
      "2024-08-20 00:36:38,716 - INFO - Shape of BoW features: (50000, 216000)\n",
      "2024-08-20 00:36:38,716 - INFO - Extracting sentiment labels\n",
      "2024-08-20 00:36:38,717 - INFO - Number of labels: 50000\n",
      "2024-08-20 00:36:38,717 - INFO - Splitting data into training and testing sets\n",
      "2024-08-20 00:36:38,735 - INFO - Training set shape: (40000, 216000), Testing set shape: (10000, 216000)\n",
      "2024-08-20 00:36:38,735 - INFO - Creating and training the Support Vector Machine model\n",
      "2024-08-20 00:36:38,737 - INFO - SVM parameters: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'linear', 'max_iter': -1, 'probability': False, 'random_state': None, 'shrinking': True, 'tol': 0.001, 'verbose': False}\n",
      "2024-08-20 01:43:15,406 - INFO - Model training completed\n",
      "2024-08-20 01:43:15,406 - INFO - Making predictions on the testing set\n",
      "2024-08-20 01:44:08,906 - INFO - Predictions completed\n",
      "2024-08-20 01:44:08,906 - INFO - Evaluating model performance\n",
      "2024-08-20 01:44:08,916 - INFO - Support Vector Machine Model Performance:\n",
      "2024-08-20 01:44:08,916 - INFO - Accuracy: 0.8672\n",
      "2024-08-20 01:44:08,917 - INFO - Precision: 0.8633\n",
      "2024-08-20 01:44:08,917 - INFO - Recall: 0.8750\n",
      "2024-08-20 01:44:08,917 - INFO - F1 Score: 0.8691\n",
      "2024-08-20 01:44:08,918 - INFO - Bag of Words Support Vector Machine classification completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support Vector Machine Model Performance:\n",
      "Accuracy: 0.8672\n",
      "Precision: 0.8633\n",
      "Recall: 0.8750\n",
      "F1 Score: 0.8691\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "logging.info(\"Starting Bag of Words Support Vector Machine classification\")\n",
    "\n",
    "# Fit and transform the preprocessed reviews\n",
    "logging.info(\"Fitting and transforming preprocessed reviews using CountVectorizer\")\n",
    "vectorizer = CountVectorizer()\n",
    "bow_features = vectorizer.fit_transform(df['preprocessed_review'])\n",
    "logging.info(f\"Shape of BoW features: {bow_features.shape}\")\n",
    "\n",
    "# Get the sentiment labels from the preprocessed DataFrame\n",
    "logging.info(\"Extracting sentiment labels\")\n",
    "labels = df['sentiment']\n",
    "logging.info(f\"Number of labels: {len(labels)}\")\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "logging.info(\"Splitting data into training and testing sets\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(bow_features, labels, test_size=0.2, random_state=42)\n",
    "logging.info(f\"Training set shape: {X_train.shape}, Testing set shape: {X_test.shape}\")\n",
    "\n",
    "# Create and train the Support Vector Machine model\n",
    "logging.info(\"Creating and training the Support Vector Machine model\")\n",
    "model = SVC(kernel='linear')  # You can choose 'linear', 'rbf', 'poly', etc.\n",
    "logging.info(f\"SVM parameters: {model.get_params()}\")\n",
    "model.fit(X_train, y_train)\n",
    "logging.info(\"Model training completed\")\n",
    "\n",
    "# Make predictions on the testing set\n",
    "logging.info(\"Making predictions on the testing set\")\n",
    "y_pred = model.predict(X_test)\n",
    "logging.info(\"Predictions completed\")\n",
    "\n",
    "# Evaluate the model's performance\n",
    "logging.info(\"Evaluating model performance\")\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "logging.info(\"Support Vector Machine Model Performance:\")\n",
    "logging.info(f\"Accuracy: {accuracy:.4f}\")\n",
    "logging.info(f\"Precision: {precision:.4f}\")\n",
    "logging.info(f\"Recall: {recall:.4f}\")\n",
    "logging.info(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "print(\"Support Vector Machine Model Performance:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "logging.info(\"Bag of Words Support Vector Machine classification completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-20 02:11:55,106 - INFO - Starting Bag of Words Decision Tree classification\n",
      "2024-08-20 02:11:55,107 - INFO - Fitting and transforming preprocessed reviews using CountVectorizer\n",
      "2024-08-20 02:11:58,076 - INFO - Shape of BoW features: (50000, 216000)\n",
      "2024-08-20 02:11:58,076 - INFO - Extracting sentiment labels\n",
      "2024-08-20 02:11:58,077 - INFO - Number of labels: 50000\n",
      "2024-08-20 02:11:58,077 - INFO - Splitting data into training and testing sets\n",
      "2024-08-20 02:11:58,096 - INFO - Training set shape: (40000, 216000), Testing set shape: (10000, 216000)\n",
      "2024-08-20 02:11:58,096 - INFO - Creating and training the Decision Tree model\n",
      "2024-08-20 02:11:58,097 - INFO - Decision Tree parameters: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}\n",
      "2024-08-20 02:12:38,317 - INFO - Model training completed\n",
      "2024-08-20 02:12:38,318 - INFO - Making predictions on the testing set\n",
      "2024-08-20 02:12:38,351 - INFO - Predictions completed\n",
      "2024-08-20 02:12:38,351 - INFO - Evaluating model performance\n",
      "2024-08-20 02:12:38,360 - INFO - Decision Tree Model Performance:\n",
      "2024-08-20 02:12:38,361 - INFO - Accuracy: 0.7248\n",
      "2024-08-20 02:12:38,361 - INFO - Precision: 0.7264\n",
      "2024-08-20 02:12:38,361 - INFO - Recall: 0.7281\n",
      "2024-08-20 02:12:38,361 - INFO - F1 Score: 0.7273\n",
      "2024-08-20 02:12:38,362 - INFO - Bag of Words Decision Tree classification completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Model Performance:\n",
      "Accuracy: 0.7248\n",
      "Precision: 0.7264\n",
      "Recall: 0.7281\n",
      "F1 Score: 0.7273\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"numpy.int64\") to str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 65\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m     64\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m20\u001b[39m,\u001b[38;5;241m10\u001b[39m))\n\u001b[0;32m---> 65\u001b[0m \u001b[43mplot_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_feature_names_out\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclasses_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilled\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrounded\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m plt\u001b[38;5;241m.\u001b[39msavefig(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdecision_tree.png\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     67\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDecision tree visualization saved as \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdecision_tree.png\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.4/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     ):\n\u001b[0;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    223\u001b[0m     )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.4/lib/python3.12/site-packages/sklearn/tree/_export.py:211\u001b[0m, in \u001b[0;36mplot_tree\u001b[0;34m(decision_tree, max_depth, feature_names, class_names, label, filled, impurity, node_ids, proportion, rounded, precision, ax, fontsize)\u001b[0m\n\u001b[1;32m    196\u001b[0m check_is_fitted(decision_tree)\n\u001b[1;32m    198\u001b[0m exporter \u001b[38;5;241m=\u001b[39m _MPLTreeExporter(\n\u001b[1;32m    199\u001b[0m     max_depth\u001b[38;5;241m=\u001b[39mmax_depth,\n\u001b[1;32m    200\u001b[0m     feature_names\u001b[38;5;241m=\u001b[39mfeature_names,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    209\u001b[0m     fontsize\u001b[38;5;241m=\u001b[39mfontsize,\n\u001b[1;32m    210\u001b[0m )\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mexporter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexport\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecision_tree\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43max\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43max\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.4/lib/python3.12/site-packages/sklearn/tree/_export.py:644\u001b[0m, in \u001b[0;36m_MPLTreeExporter.export\u001b[0;34m(self, decision_tree, ax)\u001b[0m\n\u001b[1;32m    642\u001b[0m ax\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m    643\u001b[0m ax\u001b[38;5;241m.\u001b[39mset_axis_off()\n\u001b[0;32m--> 644\u001b[0m my_tree \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecision_tree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecision_tree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    645\u001b[0m draw_tree \u001b[38;5;241m=\u001b[39m buchheim(my_tree)\n\u001b[1;32m    647\u001b[0m \u001b[38;5;66;03m# important to make sure we're still\u001b[39;00m\n\u001b[1;32m    648\u001b[0m \u001b[38;5;66;03m# inside the axis after drawing the box\u001b[39;00m\n\u001b[1;32m    649\u001b[0m \u001b[38;5;66;03m# this makes sense because the width of a box\u001b[39;00m\n\u001b[1;32m    650\u001b[0m \u001b[38;5;66;03m# is about the same as the distance between boxes\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.4/lib/python3.12/site-packages/sklearn/tree/_export.py:620\u001b[0m, in \u001b[0;36m_MPLTreeExporter._make_tree\u001b[0;34m(self, node_id, et, criterion, depth)\u001b[0m\n\u001b[1;32m    617\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_make_tree\u001b[39m(\u001b[38;5;28mself\u001b[39m, node_id, et, criterion, depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m    618\u001b[0m     \u001b[38;5;66;03m# traverses _tree.Tree recursively, builds intermediate\u001b[39;00m\n\u001b[1;32m    619\u001b[0m     \u001b[38;5;66;03m# \"_reingold_tilford.Tree\" object\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m     name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode_to_str\u001b[49m\u001b[43m(\u001b[49m\u001b[43met\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    621\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m et\u001b[38;5;241m.\u001b[39mchildren_left[node_id] \u001b[38;5;241m!=\u001b[39m _tree\u001b[38;5;241m.\u001b[39mTREE_LEAF \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[1;32m    622\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_depth \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m depth \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_depth\n\u001b[1;32m    623\u001b[0m     ):\n\u001b[1;32m    624\u001b[0m         children \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    625\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_tree(\n\u001b[1;32m    626\u001b[0m                 et\u001b[38;5;241m.\u001b[39mchildren_left[node_id], et, criterion, depth\u001b[38;5;241m=\u001b[39mdepth \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    630\u001b[0m             ),\n\u001b[1;32m    631\u001b[0m         ]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.4/lib/python3.12/site-packages/sklearn/tree/_export.py:393\u001b[0m, in \u001b[0;36m_BaseTreeExporter.node_to_str\u001b[0;34m(self, tree, node_id, criterion)\u001b[0m\n\u001b[1;32m    387\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    388\u001b[0m         class_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\n\u001b[1;32m    389\u001b[0m             characters[\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m    390\u001b[0m             np\u001b[38;5;241m.\u001b[39margmax(value),\n\u001b[1;32m    391\u001b[0m             characters[\u001b[38;5;241m2\u001b[39m],\n\u001b[1;32m    392\u001b[0m         )\n\u001b[0;32m--> 393\u001b[0m     \u001b[43mnode_string\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mclass_name\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;66;03m# Clean up any trailing newlines\u001b[39;00m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m node_string\u001b[38;5;241m.\u001b[39mendswith(characters[\u001b[38;5;241m4\u001b[39m]):\n",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate str (not \"numpy.int64\") to str"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import logging\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "logging.info(\"Starting Bag of Words Decision Tree classification\")\n",
    "\n",
    "# Fit and transform the preprocessed reviews\n",
    "logging.info(\"Fitting and transforming preprocessed reviews using CountVectorizer\")\n",
    "vectorizer = CountVectorizer()\n",
    "bow_features = vectorizer.fit_transform(df['preprocessed_review'])\n",
    "logging.info(f\"Shape of BoW features: {bow_features.shape}\")\n",
    "\n",
    "# Get the sentiment labels from the preprocessed DataFrame\n",
    "logging.info(\"Extracting sentiment labels\")\n",
    "labels = df['sentiment']\n",
    "logging.info(f\"Number of labels: {len(labels)}\")\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "logging.info(\"Splitting data into training and testing sets\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(bow_features, labels, test_size=0.2, random_state=42)\n",
    "logging.info(f\"Training set shape: {X_train.shape}, Testing set shape: {X_test.shape}\")\n",
    "\n",
    "# Create and train the Decision Tree model\n",
    "logging.info(\"Creating and training the Decision Tree model\")\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "logging.info(f\"Decision Tree parameters: {model.get_params()}\")\n",
    "model.fit(X_train, y_train)\n",
    "logging.info(\"Model training completed\")\n",
    "\n",
    "# Make predictions on the testing set\n",
    "logging.info(\"Making predictions on the testing set\")\n",
    "y_pred = model.predict(X_test)\n",
    "logging.info(\"Predictions completed\")\n",
    "\n",
    "# Evaluate the model's performance\n",
    "logging.info(\"Evaluating model performance\")\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "logging.info(\"Decision Tree Model Performance:\")\n",
    "logging.info(f\"Accuracy: {accuracy:.4f}\")\n",
    "logging.info(f\"Precision: {precision:.4f}\")\n",
    "logging.info(f\"Recall: {recall:.4f}\")\n",
    "logging.info(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "print(\"Decision Tree Model Performance:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "logging.info(\"Bag of Words Decision Tree classification completed\")\n",
    "\n",
    "# Optional: Visualize the decision tree\n",
    "from sklearn.tree import plot_tree\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(20,10))\n",
    "plot_tree(model, feature_names=vectorizer.get_feature_names_out(), class_names=model.classes_, filled=True, rounded=True)\n",
    "plt.savefig('decision_tree.png')\n",
    "logging.info(\"Decision tree visualization saved as 'decision_tree.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagging DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-20 02:14:31,859 - INFO - Starting Bag of Words Bagging Decision Tree classification\n",
      "2024-08-20 02:14:31,860 - INFO - Fitting and transforming preprocessed reviews using CountVectorizer\n",
      "2024-08-20 02:14:34,820 - INFO - Shape of BoW features: (50000, 216000)\n",
      "2024-08-20 02:14:34,820 - INFO - Extracting sentiment labels\n",
      "2024-08-20 02:14:34,821 - INFO - Number of labels: 50000\n",
      "2024-08-20 02:14:34,821 - INFO - Splitting data into training and testing sets\n",
      "2024-08-20 02:14:34,839 - INFO - Training set shape: (40000, 216000), Testing set shape: (10000, 216000)\n",
      "2024-08-20 02:14:34,840 - INFO - Creating and training the Bagging Decision Tree model\n",
      "2024-08-20 02:14:34,841 - INFO - Bagging Classifier parameters: {'bootstrap': True, 'bootstrap_features': False, 'estimator': None, 'max_features': 1.0, 'max_samples': 1.0, 'n_estimators': 50, 'n_jobs': None, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}\n",
      "2024-08-20 02:34:54,762 - INFO - Model training completed\n",
      "2024-08-20 02:34:54,763 - INFO - Making predictions on the testing set\n",
      "2024-08-20 02:34:56,556 - INFO - Predictions completed\n",
      "2024-08-20 02:34:56,557 - INFO - Evaluating model performance\n",
      "2024-08-20 02:34:56,567 - INFO - Bagging Decision Tree Model Performance:\n",
      "2024-08-20 02:34:56,567 - INFO - Accuracy: 0.8051\n",
      "2024-08-20 02:34:56,567 - INFO - Precision: 0.8029\n",
      "2024-08-20 02:34:56,567 - INFO - Recall: 0.8127\n",
      "2024-08-20 02:34:56,568 - INFO - F1 Score: 0.8078\n",
      "2024-08-20 02:34:56,568 - INFO - Bag of Words Bagging Decision Tree classification completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging Decision Tree Model Performance:\n",
      "Accuracy: 0.8051\n",
      "Precision: 0.8029\n",
      "Recall: 0.8127\n",
      "F1 Score: 0.8078\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "logging.info(\"Starting Bag of Words Bagging Decision Tree classification\")\n",
    "\n",
    "# Fit and transform the preprocessed reviews\n",
    "logging.info(\"Fitting and transforming preprocessed reviews using CountVectorizer\")\n",
    "vectorizer = CountVectorizer()\n",
    "bow_features = vectorizer.fit_transform(df['preprocessed_review'])\n",
    "logging.info(f\"Shape of BoW features: {bow_features.shape}\")\n",
    "\n",
    "# Get the sentiment labels from the preprocessed DataFrame\n",
    "logging.info(\"Extracting sentiment labels\")\n",
    "labels = df['sentiment']\n",
    "logging.info(f\"Number of labels: {len(labels)}\")\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "logging.info(\"Splitting data into training and testing sets\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(bow_features, labels, test_size=0.2, random_state=42)\n",
    "logging.info(f\"Training set shape: {X_train.shape}, Testing set shape: {X_test.shape}\")\n",
    "\n",
    "# Create and train the Bagging Decision Tree model\n",
    "logging.info(\"Creating and training the Bagging Decision Tree model\")\n",
    "base_estimator = DecisionTreeClassifier(random_state=42)\n",
    "model = BaggingClassifier(n_estimators=50, random_state=42)\n",
    "logging.info(f\"Bagging Classifier parameters: {model.get_params()}\")\n",
    "model.fit(X_train, y_train)\n",
    "logging.info(\"Model training completed\")\n",
    "\n",
    "# Make predictions on the testing set\n",
    "logging.info(\"Making predictions on the testing set\")\n",
    "y_pred = model.predict(X_test)\n",
    "logging.info(\"Predictions completed\")\n",
    "\n",
    "# Evaluate the model's performance\n",
    "logging.info(\"Evaluating model performance\")\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "logging.info(\"Bagging Decision Tree Model Performance:\")\n",
    "logging.info(f\"Accuracy: {accuracy:.4f}\")\n",
    "logging.info(f\"Precision: {precision:.4f}\")\n",
    "logging.info(f\"Recall: {recall:.4f}\")\n",
    "logging.info(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "print(\"Bagging Decision Tree Model Performance:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "logging.info(\"Bag of Words Bagging Decision Tree classification completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-20 02:34:56,575 - INFO - Starting Bag of Words Gradient Boosting classification\n",
      "2024-08-20 02:34:56,576 - INFO - Fitting and transforming preprocessed reviews using CountVectorizer\n",
      "2024-08-20 02:34:59,148 - INFO - Shape of BoW features: (50000, 216000)\n",
      "2024-08-20 02:34:59,148 - INFO - Extracting sentiment labels\n",
      "2024-08-20 02:34:59,148 - INFO - Number of labels: 50000\n",
      "2024-08-20 02:34:59,149 - INFO - Splitting data into training and testing sets\n",
      "2024-08-20 02:34:59,168 - INFO - Training set shape: (40000, 216000), Testing set shape: (10000, 216000)\n",
      "2024-08-20 02:34:59,169 - INFO - Creating and training the Gradient Boosting Classifier model\n",
      "2024-08-20 02:34:59,173 - INFO - Gradient Boosting Classifier parameters: {'ccp_alpha': 0.0, 'criterion': 'friedman_mse', 'init': None, 'learning_rate': 0.1, 'loss': 'log_loss', 'max_depth': 3, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_iter_no_change': None, 'random_state': 42, 'subsample': 1.0, 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}\n",
      "2024-08-20 02:49:22,237 - INFO - Model training completed\n",
      "2024-08-20 02:49:22,238 - INFO - Making predictions on the testing set\n",
      "2024-08-20 02:49:22,278 - INFO - Predictions completed\n",
      "2024-08-20 02:49:22,278 - INFO - Evaluating model performance\n",
      "2024-08-20 02:49:22,287 - INFO - BoW Gradient Boosting Classifier Model Performance:\n",
      "2024-08-20 02:49:22,287 - INFO - Accuracy: 0.8146\n",
      "2024-08-20 02:49:22,287 - INFO - Precision: 0.8177\n",
      "2024-08-20 02:49:22,287 - INFO - Recall: 0.8146\n",
      "2024-08-20 02:49:22,288 - INFO - F1 Score: 0.8140\n",
      "2024-08-20 02:49:22,288 - INFO - Bag of Words Gradient Boosting classification completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW Gradient Boosting Classifier Model Performance:\n",
      "Accuracy: 0.8146\n",
      "Precision: 0.8177\n",
      "Recall: 0.8146\n",
      "F1 Score: 0.8140\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "logging.info(\"Starting Bag of Words Gradient Boosting classification\")\n",
    "\n",
    "# Fit and transform the preprocessed reviews\n",
    "logging.info(\"Fitting and transforming preprocessed reviews using CountVectorizer\")\n",
    "vectorizer = CountVectorizer()\n",
    "bow_features = vectorizer.fit_transform(df['preprocessed_review'])\n",
    "logging.info(f\"Shape of BoW features: {bow_features.shape}\")\n",
    "\n",
    "# Get the sentiment labels from the preprocessed DataFrame\n",
    "logging.info(\"Extracting sentiment labels\")\n",
    "labels = df['sentiment']\n",
    "logging.info(f\"Number of labels: {len(labels)}\")\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "logging.info(\"Splitting data into training and testing sets\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(bow_features, labels, test_size=0.2, random_state=42)\n",
    "logging.info(f\"Training set shape: {X_train.shape}, Testing set shape: {X_test.shape}\")\n",
    "\n",
    "# Create and train the Gradient Boosting Decision Tree model\n",
    "logging.info(\"Creating and training the Gradient Boosting Classifier model\")\n",
    "model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "logging.info(f\"Gradient Boosting Classifier parameters: {model.get_params()}\")\n",
    "model.fit(X_train, y_train)\n",
    "logging.info(\"Model training completed\")\n",
    "\n",
    "# Make predictions on the testing set\n",
    "logging.info(\"Making predictions on the testing set\")\n",
    "y_pred = model.predict(X_test)\n",
    "logging.info(\"Predictions completed\")\n",
    "\n",
    "# Evaluate the model's performance\n",
    "logging.info(\"Evaluating model performance\")\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "logging.info(\"BoW Gradient Boosting Classifier Model Performance:\")\n",
    "logging.info(f\"Accuracy: {accuracy:.4f}\")\n",
    "logging.info(f\"Precision: {precision:.4f}\")\n",
    "logging.info(f\"Recall: {recall:.4f}\")\n",
    "logging.info(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "print(\"BoW Gradient Boosting Classifier Model Performance:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "logging.info(\"Bag of Words Gradient Boosting classification completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-20 02:49:22,294 - INFO - Starting Bag of Words Random Forest classification\n",
      "2024-08-20 02:49:22,295 - INFO - Fitting and transforming preprocessed reviews using CountVectorizer\n",
      "2024-08-20 02:49:24,822 - INFO - Shape of BoW features: (50000, 216000)\n",
      "2024-08-20 02:49:24,823 - INFO - Extracting sentiment labels\n",
      "2024-08-20 02:49:24,823 - INFO - Number of labels: 50000\n",
      "2024-08-20 02:49:24,823 - INFO - Splitting data into training and testing sets\n",
      "2024-08-20 02:49:24,842 - INFO - Training set shape: (40000, 216000), Testing set shape: (10000, 216000)\n",
      "2024-08-20 02:49:24,842 - INFO - Creating and training the Random Forest Classifier model\n",
      "2024-08-20 02:49:24,843 - INFO - Random Forest Classifier parameters: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}\n",
      "2024-08-20 02:52:23,425 - INFO - Model training completed\n",
      "2024-08-20 02:52:23,425 - INFO - Making predictions on the testing set\n",
      "2024-08-20 02:52:23,968 - INFO - Predictions completed\n",
      "2024-08-20 02:52:23,968 - INFO - Evaluating model performance\n",
      "2024-08-20 02:52:23,977 - INFO - BoW Random Forest Model Performance:\n",
      "2024-08-20 02:52:23,978 - INFO - Accuracy: 0.8580\n",
      "2024-08-20 02:52:23,978 - INFO - Precision: 0.8604\n",
      "2024-08-20 02:52:23,978 - INFO - Recall: 0.8573\n",
      "2024-08-20 02:52:23,978 - INFO - F1 Score: 0.8588\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW Random Forest Model Performance:\n",
      "Accuracy: 0.8580\n",
      "Precision: 0.8604\n",
      "Recall: 0.8573\n",
      "F1 Score: 0.8588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-20 02:52:24,268 - INFO - Top 10 important features:\n",
      "2024-08-20 02:52:24,268 - INFO - bad: 0.0097\n",
      "2024-08-20 02:52:24,268 - INFO - worst: 0.0084\n",
      "2024-08-20 02:52:24,269 - INFO - awful: 0.0054\n",
      "2024-08-20 02:52:24,269 - INFO - great: 0.0053\n",
      "2024-08-20 02:52:24,269 - INFO - terrible: 0.0044\n",
      "2024-08-20 02:52:24,269 - INFO - waste: 0.0042\n",
      "2024-08-20 02:52:24,269 - INFO - excellent: 0.0034\n",
      "2024-08-20 02:52:24,269 - INFO - worse: 0.0032\n",
      "2024-08-20 02:52:24,270 - INFO - nothing: 0.0032\n",
      "2024-08-20 02:52:24,270 - INFO - boring: 0.0030\n",
      "2024-08-20 02:52:24,270 - INFO - Bag of Words Random Forest classification completed\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "logging.info(\"Starting Bag of Words Random Forest classification\")\n",
    "\n",
    "# Fit and transform the preprocessed reviews\n",
    "logging.info(\"Fitting and transforming preprocessed reviews using CountVectorizer\")\n",
    "vectorizer = CountVectorizer()\n",
    "bow_features = vectorizer.fit_transform(df['preprocessed_review'])\n",
    "logging.info(f\"Shape of BoW features: {bow_features.shape}\")\n",
    "\n",
    "# Get the sentiment labels from the preprocessed DataFrame\n",
    "logging.info(\"Extracting sentiment labels\")\n",
    "labels = df['sentiment']\n",
    "logging.info(f\"Number of labels: {len(labels)}\")\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "logging.info(\"Splitting data into training and testing sets\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(bow_features, labels, test_size=0.2, random_state=42)\n",
    "logging.info(f\"Training set shape: {X_train.shape}, Testing set shape: {X_test.shape}\")\n",
    "\n",
    "# Create and train the Random Forest model\n",
    "logging.info(\"Creating and training the Random Forest Classifier model\")\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "logging.info(f\"Random Forest Classifier parameters: {model.get_params()}\")\n",
    "model.fit(X_train, y_train)\n",
    "logging.info(\"Model training completed\")\n",
    "\n",
    "# Make predictions on the testing set\n",
    "logging.info(\"Making predictions on the testing set\")\n",
    "y_pred = model.predict(X_test)\n",
    "logging.info(\"Predictions completed\")\n",
    "\n",
    "# Evaluate the model's performance\n",
    "logging.info(\"Evaluating model performance\")\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "logging.info(\"BoW Random Forest Model Performance:\")\n",
    "logging.info(f\"Accuracy: {accuracy:.4f}\")\n",
    "logging.info(f\"Precision: {precision:.4f}\")\n",
    "logging.info(f\"Recall: {recall:.4f}\")\n",
    "logging.info(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "print(\"BoW Random Forest Model Performance:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "# Optional: Feature Importance\n",
    "feature_importance = model.feature_importances_\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "feature_importance_dict = dict(zip(feature_names, feature_importance))\n",
    "top_features = sorted(feature_importance_dict.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "logging.info(\"Top 10 important features:\")\n",
    "for feature, importance in top_features:\n",
    "    logging.info(f\"{feature}: {importance:.4f}\")\n",
    "\n",
    "logging.info(\"Bag of Words Random Forest classification completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-20 02:52:24,276 - INFO - Starting Bag of Words Voting Classifier classification\n",
      "2024-08-20 02:52:24,277 - INFO - Fitting and transforming preprocessed reviews using CountVectorizer\n",
      "2024-08-20 02:52:26,800 - INFO - Shape of BoW features: (50000, 216000)\n",
      "2024-08-20 02:52:26,800 - INFO - Extracting sentiment labels\n",
      "2024-08-20 02:52:26,801 - INFO - Number of labels: 50000\n",
      "2024-08-20 02:52:26,801 - INFO - Splitting data into training and testing sets\n",
      "2024-08-20 02:52:26,820 - INFO - Training set shape: (40000, 216000), Testing set shape: (10000, 216000)\n",
      "2024-08-20 02:52:26,821 - INFO - Defining base models\n",
      "2024-08-20 02:52:26,821 - INFO - Creating and training the Voting Classifier model\n",
      "2024-08-20 02:52:26,834 - INFO - Voting Classifier parameters: {'estimators': [('lr', LogisticRegression(max_iter=1000, random_state=42)), ('dt', DecisionTreeClassifier(random_state=42)), ('svm', SVC(kernel='linear', probability=True, random_state=42))], 'flatten_transform': True, 'n_jobs': None, 'verbose': False, 'voting': 'soft', 'weights': None, 'lr': LogisticRegression(max_iter=1000, random_state=42), 'dt': DecisionTreeClassifier(random_state=42), 'svm': SVC(kernel='linear', probability=True, random_state=42), 'lr__C': 1.0, 'lr__class_weight': None, 'lr__dual': False, 'lr__fit_intercept': True, 'lr__intercept_scaling': 1, 'lr__l1_ratio': None, 'lr__max_iter': 1000, 'lr__multi_class': 'deprecated', 'lr__n_jobs': None, 'lr__penalty': 'l2', 'lr__random_state': 42, 'lr__solver': 'lbfgs', 'lr__tol': 0.0001, 'lr__verbose': 0, 'lr__warm_start': False, 'dt__ccp_alpha': 0.0, 'dt__class_weight': None, 'dt__criterion': 'gini', 'dt__max_depth': None, 'dt__max_features': None, 'dt__max_leaf_nodes': None, 'dt__min_impurity_decrease': 0.0, 'dt__min_samples_leaf': 1, 'dt__min_samples_split': 2, 'dt__min_weight_fraction_leaf': 0.0, 'dt__monotonic_cst': None, 'dt__random_state': 42, 'dt__splitter': 'best', 'svm__C': 1.0, 'svm__break_ties': False, 'svm__cache_size': 200, 'svm__class_weight': None, 'svm__coef0': 0.0, 'svm__decision_function_shape': 'ovr', 'svm__degree': 3, 'svm__gamma': 'scale', 'svm__kernel': 'linear', 'svm__max_iter': -1, 'svm__probability': True, 'svm__random_state': 42, 'svm__shrinking': True, 'svm__tol': 0.001, 'svm__verbose': False}\n",
      "2024-08-20 06:41:30,181 - INFO - Model training completed\n",
      "2024-08-20 06:41:30,182 - INFO - Making predictions on the testing set\n",
      "2024-08-20 06:42:22,046 - INFO - Predictions completed\n",
      "2024-08-20 06:42:22,046 - INFO - Evaluating model performance\n",
      "2024-08-20 06:42:22,057 - INFO - BoW Voting Classifier Model Performance:\n",
      "2024-08-20 06:42:22,057 - INFO - Accuracy: 0.8730\n",
      "2024-08-20 06:42:22,058 - INFO - Precision: 0.8667\n",
      "2024-08-20 06:42:22,058 - INFO - Recall: 0.8839\n",
      "2024-08-20 06:42:22,058 - INFO - F1 Score: 0.8752\n",
      "2024-08-20 06:42:22,059 - INFO - Evaluating individual classifier performance\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW Voting Classifier Model Performance:\n",
      "Accuracy: 0.8730\n",
      "Precision: 0.8667\n",
      "Recall: 0.8839\n",
      "F1 Score: 0.8752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-20 06:42:27,665 - INFO - Logistic Regression Accuracy: 0.8854\n",
      "2024-08-20 06:43:01,549 - INFO - Decision Tree Accuracy: 0.7248\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "logging.info(\"Starting Bag of Words Voting Classifier classification\")\n",
    "\n",
    "# Fit and transform the preprocessed reviews\n",
    "logging.info(\"Fitting and transforming preprocessed reviews using CountVectorizer\")\n",
    "vectorizer = CountVectorizer()\n",
    "bow_features = vectorizer.fit_transform(df['preprocessed_review'])\n",
    "logging.info(f\"Shape of BoW features: {bow_features.shape}\")\n",
    "\n",
    "# Get the sentiment labels from the preprocessed DataFrame\n",
    "logging.info(\"Extracting sentiment labels\")\n",
    "labels = df['sentiment']\n",
    "logging.info(f\"Number of labels: {len(labels)}\")\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "logging.info(\"Splitting data into training and testing sets\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(bow_features, labels, test_size=0.2, random_state=42)\n",
    "logging.info(f\"Training set shape: {X_train.shape}, Testing set shape: {X_test.shape}\")\n",
    "\n",
    "# Define the base models\n",
    "logging.info(\"Defining base models\")\n",
    "log_clf = LogisticRegression(max_iter=1000, random_state=42)\n",
    "dt_clf = DecisionTreeClassifier(random_state=42)\n",
    "svm_clf = SVC(kernel='linear', probability=True, random_state=42)\n",
    "\n",
    "# Create and train the Voting Classifier model\n",
    "logging.info(\"Creating and training the Voting Classifier model\")\n",
    "model = VotingClassifier(estimators=[\n",
    "    ('lr', log_clf),\n",
    "    ('dt', dt_clf),\n",
    "    ('svm', svm_clf)\n",
    "], voting='soft')  # Use 'soft' voting to consider predicted probabilities\n",
    "logging.info(f\"Voting Classifier parameters: {model.get_params()}\")\n",
    "model.fit(X_train, y_train)\n",
    "logging.info(\"Model training completed\")\n",
    "\n",
    "# Make predictions on the testing set\n",
    "logging.info(\"Making predictions on the testing set\")\n",
    "y_pred = model.predict(X_test)\n",
    "logging.info(\"Predictions completed\")\n",
    "\n",
    "# Evaluate the model's performance\n",
    "logging.info(\"Evaluating model performance\")\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "logging.info(\"BoW Voting Classifier Model Performance:\")\n",
    "logging.info(f\"Accuracy: {accuracy:.4f}\")\n",
    "logging.info(f\"Precision: {precision:.4f}\")\n",
    "logging.info(f\"Recall: {recall:.4f}\")\n",
    "logging.info(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "print(\"BoW Voting Classifier Model Performance:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "# Optional: Individual classifier performance\n",
    "logging.info(\"Evaluating individual classifier performance\")\n",
    "for clf_name, clf in [('Logistic Regression', log_clf), ('Decision Tree', dt_clf), ('SVM', svm_clf)]:\n",
    "    clf.fit(X_train, y_train)\n",
    "    clf_pred = clf.predict(X_test)\n",
    "    clf_accuracy = accuracy_score(y_test, clf_pred)\n",
    "    logging.info(f\"{clf_name} Accuracy: {clf_accuracy:.4f}\")\n",
    "\n",
    "logging.info(\"Bag of Words Voting Classifier classification completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nerual Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-20 10:55:56,286 - INFO - Starting Bag of Words Neural Network classification\n",
      "2024-08-20 10:55:56,287 - INFO - Fitting and transforming preprocessed reviews using CountVectorizer\n",
      "2024-08-20 10:55:59,141 - INFO - Shape of BoW features: (50000, 216000)\n",
      "2024-08-20 10:55:59,142 - INFO - Encoding sentiment labels\n",
      "2024-08-20 10:55:59,144 - INFO - Number of classes: 2\n",
      "2024-08-20 10:55:59,144 - INFO - Splitting data into training and testing sets\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "logging.info(\"Starting Bag of Words Neural Network classification\")\n",
    "\n",
    "# Fit and transform the preprocessed reviews\n",
    "logging.info(\"Fitting and transforming preprocessed reviews using CountVectorizer\")\n",
    "vectorizer = CountVectorizer()\n",
    "bow_features = vectorizer.fit_transform(df['preprocessed_review'])\n",
    "logging.info(f\"Shape of BoW features: {bow_features.shape}\")\n",
    "\n",
    "# Get the sentiment labels from the preprocessed DataFrame and encode them\n",
    "logging.info(\"Encoding sentiment labels\")\n",
    "labels = df['sentiment']\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(labels)\n",
    "logging.info(f\"Number of classes: {len(label_encoder.classes_)}\")\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "logging.info(\"Splitting data into training and testing sets\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(bow_features.toarray(), encoded_labels, test_size=0.2, random_state=42)\n",
    "logging.info(f\"Training set shape: {X_train.shape}, Testing set shape: {X_test.shape}\")\n",
    "\n",
    "# Build the neural network model\n",
    "logging.info(\"Building the neural network model\")\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=X_train.shape[1], activation='relu'))  # Input layer\n",
    "model.add(Dropout(0.5))  # Dropout for regularization\n",
    "model.add(Dense(64, activation='relu'))  # Hidden layer\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))  # Output layer for binary classification\n",
    "\n",
    "# Compile the model\n",
    "logging.info(\"Compiling the model\")\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Display model summary\n",
    "model.summary(print_fn=logging.info)\n",
    "\n",
    "# Train the model\n",
    "logging.info(\"Training the model\")\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2, verbose=0)\n",
    "\n",
    "# Log training history\n",
    "logging.info(\"Training history:\")\n",
    "for epoch, (loss, accuracy, val_loss, val_accuracy) in enumerate(zip(\n",
    "    history.history['loss'], history.history['accuracy'],\n",
    "    history.history['val_loss'], history.history['val_accuracy']), 1):\n",
    "    logging.info(f\"Epoch {epoch}: loss={loss:.4f}, accuracy={accuracy:.4f}, val_loss={val_loss:.4f}, val_accuracy={val_accuracy:.4f}\")\n",
    "\n",
    "# Make predictions on the testing set\n",
    "logging.info(\"Making predictions on the testing set\")\n",
    "y_pred_prob = model.predict(X_test)\n",
    "y_pred = (y_pred_prob > 0.5).astype(\"int32\").flatten()\n",
    "logging.info(\"Predictions completed\")\n",
    "\n",
    "# Evaluate the model's performance\n",
    "logging.info(\"Evaluating model performance\")\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "logging.info(\"Neural Network Model Performance:\")\n",
    "logging.info(f\"Accuracy: {accuracy:.4f}\")\n",
    "logging.info(f\"Precision: {precision:.4f}\")\n",
    "logging.info(f\"Recall: {recall:.4f}\")\n",
    "logging.info(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "print(\"Neural Network Model Performance:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "logging.info(\"Bag of Words Neural Network classification completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-20 11:01:40,645 - INFO - Starting Bag of Words Convolutional Neural Network classification\n",
      "2024-08-20 11:01:40,645 - INFO - Converting reviews to Bag-of-Words features\n",
      "2024-08-20 11:01:43,580 - INFO - Shape of BoW features: (50000, 10000)\n",
      "2024-08-20 11:01:43,581 - INFO - Encoding sentiment labels\n",
      "2024-08-20 11:01:43,583 - INFO - Number of classes: 2\n",
      "2024-08-20 11:01:43,583 - INFO - Splitting data into training and testing sets\n",
      "2024-08-20 11:01:45,670 - INFO - Training set shape: (40000, 10000), Testing set shape: (10000, 10000)\n",
      "2024-08-20 11:01:45,672 - INFO - Reshaping input data for Conv1D layer\n",
      "2024-08-20 11:01:45,673 - INFO - Reshaped training set: (40000, 10000, 1), Reshaped testing set: (10000, 10000, 1)\n",
      "2024-08-20 11:01:45,673 - INFO - Building the Convolutional Neural Network model\n",
      "/Users/fpolygon/.pyenv/versions/3.12.4/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "2024-08-20 11:01:45,724 - INFO - Compiling the model\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-20 11:01:45,733 - INFO - Model: \"sequential\"\n",
      "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
      "┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃\n",
      "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
      "│ conv1d (Conv1D)                 │ (None, 9996, 128)      │           768 │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ global_max_pooling1d            │ (None, 128)            │             0 │\n",
      "│ (GlobalMaxPooling1D)            │                        │               │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ dense (Dense)                   │ (None, 64)             │         8,256 │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ dropout (Dropout)               │ (None, 64)             │             0 │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ dense_1 (Dense)                 │ (None, 1)              │            65 │\n",
      "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
      " Total params: 9,089 (35.50 KB)\n",
      " Trainable params: 9,089 (35.50 KB)\n",
      " Non-trainable params: 0 (0.00 B)\n",
      "\n",
      "2024-08-20 11:01:45,734 - INFO - Training the model\n",
      "2024-08-20 11:16:10,739 - INFO - Training history:\n",
      "2024-08-20 11:16:10,741 - INFO - Epoch 1: loss=0.6949, accuracy=0.4993, val_loss=0.6932, val_accuracy=0.5000\n",
      "2024-08-20 11:16:10,741 - INFO - Epoch 2: loss=0.6932, accuracy=0.4978, val_loss=0.6932, val_accuracy=0.5000\n",
      "2024-08-20 11:16:10,742 - INFO - Epoch 3: loss=0.6932, accuracy=0.5010, val_loss=0.6931, val_accuracy=0.5001\n",
      "2024-08-20 11:16:10,742 - INFO - Epoch 4: loss=0.6932, accuracy=0.5015, val_loss=0.6932, val_accuracy=0.5000\n",
      "2024-08-20 11:16:10,742 - INFO - Epoch 5: loss=0.6932, accuracy=0.4989, val_loss=0.6932, val_accuracy=0.5000\n",
      "2024-08-20 11:16:10,742 - INFO - Epoch 6: loss=0.6932, accuracy=0.4967, val_loss=0.6932, val_accuracy=0.5000\n",
      "2024-08-20 11:16:10,743 - INFO - Epoch 7: loss=0.6933, accuracy=0.4992, val_loss=0.6932, val_accuracy=0.5000\n",
      "2024-08-20 11:16:10,743 - INFO - Epoch 8: loss=0.6932, accuracy=0.4988, val_loss=0.6931, val_accuracy=0.5000\n",
      "2024-08-20 11:16:10,743 - INFO - Epoch 9: loss=0.6932, accuracy=0.4935, val_loss=0.6932, val_accuracy=0.5000\n",
      "2024-08-20 11:16:10,743 - INFO - Epoch 10: loss=0.6932, accuracy=0.4996, val_loss=0.6931, val_accuracy=0.5000\n",
      "2024-08-20 11:16:10,743 - INFO - Making predictions on the testing set\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 24ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-20 11:16:18,401 - INFO - Predictions completed\n",
      "2024-08-20 11:16:18,401 - INFO - Evaluating model performance\n",
      "/Users/fpolygon/.pyenv/versions/3.12.4/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "2024-08-20 11:16:18,414 - INFO - Convolutional Neural Network Model Performance:\n",
      "2024-08-20 11:16:18,414 - INFO - Accuracy: 0.4961\n",
      "2024-08-20 11:16:18,414 - INFO - Precision: 0.0000\n",
      "2024-08-20 11:16:18,414 - INFO - Recall: 0.0000\n",
      "2024-08-20 11:16:18,415 - INFO - F1 Score: 0.0000\n",
      "2024-08-20 11:16:18,415 - INFO - Bag of Words Convolutional Neural Network classification completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convolutional Neural Network Model Performance:\n",
      "Accuracy: 0.4961\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "F1 Score: 0.0000\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "logging.info(\"Starting Bag of Words Convolutional Neural Network classification\")\n",
    "\n",
    "# Convert the reviews into bag-of-words features\n",
    "logging.info(\"Converting reviews to Bag-of-Words features\")\n",
    "vectorizer = CountVectorizer(max_features=10000)  # Limit to top 10,000 words\n",
    "bow_features = vectorizer.fit_transform(df['preprocessed_review'])\n",
    "logging.info(f\"Shape of BoW features: {bow_features.shape}\")\n",
    "\n",
    "# Get the sentiment labels from the preprocessed DataFrame and encode them\n",
    "logging.info(\"Encoding sentiment labels\")\n",
    "labels = df['sentiment']\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(labels)\n",
    "logging.info(f\"Number of classes: {len(label_encoder.classes_)}\")\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "logging.info(\"Splitting data into training and testing sets\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(bow_features.toarray(), encoded_labels, test_size=0.2, random_state=42)\n",
    "logging.info(f\"Training set shape: {X_train.shape}, Testing set shape: {X_test.shape}\")\n",
    "\n",
    "# Reshape input data to 2D for Conv1D layer (samples, timesteps, features)\n",
    "logging.info(\"Reshaping input data for Conv1D layer\")\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "logging.info(f\"Reshaped training set: {X_train.shape}, Reshaped testing set: {X_test.shape}\")\n",
    "\n",
    "# Build the Convolutional Neural Network model\n",
    "logging.info(\"Building the Convolutional Neural Network model\")\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters=128, kernel_size=5, activation='relu', input_shape=(X_train.shape[1], 1)))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))  # Output layer for binary classification\n",
    "\n",
    "# Compile the model\n",
    "logging.info(\"Compiling the model\")\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Display model summary\n",
    "model.summary(print_fn=logging.info)\n",
    "\n",
    "# Train the model\n",
    "logging.info(\"Training the model\")\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2, verbose=0)\n",
    "\n",
    "# Log training history\n",
    "logging.info(\"Training history:\")\n",
    "for epoch, (loss, accuracy, val_loss, val_accuracy) in enumerate(zip(\n",
    "    history.history['loss'], history.history['accuracy'],\n",
    "    history.history['val_loss'], history.history['val_accuracy']), 1):\n",
    "    logging.info(f\"Epoch {epoch}: loss={loss:.4f}, accuracy={accuracy:.4f}, val_loss={val_loss:.4f}, val_accuracy={val_accuracy:.4f}\")\n",
    "\n",
    "# Make predictions on the testing set\n",
    "logging.info(\"Making predictions on the testing set\")\n",
    "y_pred_prob = model.predict(X_test)\n",
    "y_pred = (y_pred_prob > 0.5).astype(\"int32\").flatten()\n",
    "logging.info(\"Predictions completed\")\n",
    "\n",
    "# Evaluate the model's performance\n",
    "logging.info(\"Evaluating model performance\")\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "logging.info(\"Convolutional Neural Network Model Performance:\")\n",
    "logging.info(f\"Accuracy: {accuracy:.4f}\")\n",
    "logging.info(f\"Precision: {precision:.4f}\")\n",
    "logging.info(f\"Recall: {recall:.4f}\")\n",
    "logging.info(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "print(\"Convolutional Neural Network Model Performance:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "logging.info(\"Bag of Words Convolutional Neural Network classification completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-20 11:17:08,581 - INFO - Starting LSTM Recurrent Neural Network classification\n",
      "2024-08-20 11:17:08,581 - INFO - Parameters: max_words=10000, max_len=100\n",
      "2024-08-20 11:17:08,582 - INFO - Preparing tokenizer and fitting on reviews\n",
      "2024-08-20 11:17:11,933 - INFO - Vocabulary size: 216044\n",
      "2024-08-20 11:17:11,933 - INFO - Converting text to sequences of integers\n",
      "2024-08-20 11:17:13,772 - INFO - Padding sequences\n",
      "2024-08-20 11:17:13,945 - INFO - Shape of padded sequences: (50000, 100)\n",
      "2024-08-20 11:17:13,946 - INFO - Encoding sentiment labels\n",
      "2024-08-20 11:17:13,948 - INFO - Number of classes: 2\n",
      "2024-08-20 11:17:13,948 - INFO - Splitting data into training and testing sets\n",
      "2024-08-20 11:17:13,987 - INFO - Training set shape: (40000, 100), Testing set shape: (10000, 100)\n",
      "2024-08-20 11:17:13,987 - INFO - Building the LSTM Recurrent Neural Network model\n",
      "/Users/fpolygon/.pyenv/versions/3.12.4/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n",
      "2024-08-20 11:17:13,995 - INFO - Compiling the model\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-20 11:17:14,001 - INFO - Model: \"sequential_1\"\n",
      "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
      "┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃\n",
      "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
      "│ embedding (Embedding)           │ ?                      │   0 (unbuilt) │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ lstm (LSTM)                     │ ?                      │   0 (unbuilt) │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ dropout_1 (Dropout)             │ ?                      │   0 (unbuilt) │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ dense_2 (Dense)                 │ ?                      │   0 (unbuilt) │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ dropout_2 (Dropout)             │ ?                      │   0 (unbuilt) │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ dense_3 (Dense)                 │ ?                      │   0 (unbuilt) │\n",
      "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
      " Total params: 0 (0.00 B)\n",
      " Trainable params: 0 (0.00 B)\n",
      " Non-trainable params: 0 (0.00 B)\n",
      "\n",
      "2024-08-20 11:17:14,002 - INFO - Training the model\n",
      "2024-08-20 11:30:22,806 - INFO - Training history:\n",
      "2024-08-20 11:30:22,806 - INFO - Epoch 1: loss=0.3857, accuracy=0.8309, val_loss=0.3281, val_accuracy=0.8540\n",
      "2024-08-20 11:30:22,806 - INFO - Epoch 2: loss=0.2303, accuracy=0.9124, val_loss=0.3173, val_accuracy=0.8714\n",
      "2024-08-20 11:30:22,807 - INFO - Epoch 3: loss=0.1634, accuracy=0.9398, val_loss=0.4025, val_accuracy=0.8611\n",
      "2024-08-20 11:30:22,807 - INFO - Epoch 4: loss=0.1144, accuracy=0.9593, val_loss=0.4216, val_accuracy=0.8481\n",
      "2024-08-20 11:30:22,807 - INFO - Epoch 5: loss=0.0834, accuracy=0.9706, val_loss=0.5506, val_accuracy=0.8363\n",
      "2024-08-20 11:30:22,808 - INFO - Epoch 6: loss=0.0588, accuracy=0.9807, val_loss=0.6060, val_accuracy=0.8543\n",
      "2024-08-20 11:30:22,808 - INFO - Epoch 7: loss=0.0429, accuracy=0.9864, val_loss=0.8585, val_accuracy=0.8589\n",
      "2024-08-20 11:30:22,808 - INFO - Epoch 8: loss=0.0429, accuracy=0.9864, val_loss=0.6849, val_accuracy=0.8543\n",
      "2024-08-20 11:30:22,808 - INFO - Epoch 9: loss=0.0292, accuracy=0.9911, val_loss=0.7458, val_accuracy=0.8514\n",
      "2024-08-20 11:30:22,809 - INFO - Epoch 10: loss=0.0286, accuracy=0.9912, val_loss=0.9087, val_accuracy=0.8619\n",
      "2024-08-20 11:30:22,809 - INFO - Making predictions on the testing set\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 19ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-20 11:30:28,835 - INFO - Predictions completed\n",
      "2024-08-20 11:30:28,835 - INFO - Evaluating model performance\n",
      "2024-08-20 11:30:28,843 - INFO - Recurrent Neural Network (LSTM) Model Performance:\n",
      "2024-08-20 11:30:28,844 - INFO - Accuracy: 0.8646\n",
      "2024-08-20 11:30:28,844 - INFO - Precision: 0.8764\n",
      "2024-08-20 11:30:28,844 - INFO - Recall: 0.8514\n",
      "2024-08-20 11:30:28,844 - INFO - F1 Score: 0.8637\n",
      "2024-08-20 11:30:28,845 - INFO - LSTM Recurrent Neural Network classification completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recurrent Neural Network (LSTM) Model Performance:\n",
      "Accuracy: 0.8646\n",
      "Precision: 0.8764\n",
      "Recall: 0.8514\n",
      "F1 Score: 0.8637\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "logging.info(\"Starting LSTM Recurrent Neural Network classification\")\n",
    "\n",
    "# Set parameters\n",
    "max_words = 10000  # Maximum number of words to consider in the tokenizer\n",
    "max_len = 100  # Maximum length of the sequences (padded/truncated)\n",
    "logging.info(f\"Parameters: max_words={max_words}, max_len={max_len}\")\n",
    "\n",
    "# Prepare the tokenizer and fit on the reviews\n",
    "logging.info(\"Preparing tokenizer and fitting on reviews\")\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(df['preprocessed_review'])\n",
    "logging.info(f\"Vocabulary size: {len(tokenizer.word_index)}\")\n",
    "\n",
    "# Convert text to sequences of integers\n",
    "logging.info(\"Converting text to sequences of integers\")\n",
    "sequences = tokenizer.texts_to_sequences(df['preprocessed_review'])\n",
    "\n",
    "# Pad sequences to ensure they all have the same length\n",
    "logging.info(\"Padding sequences\")\n",
    "X = pad_sequences(sequences, maxlen=max_len)\n",
    "logging.info(f\"Shape of padded sequences: {X.shape}\")\n",
    "\n",
    "# Encode the sentiment labels\n",
    "logging.info(\"Encoding sentiment labels\")\n",
    "labels = df['sentiment']\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(labels)\n",
    "logging.info(f\"Number of classes: {len(label_encoder.classes_)}\")\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "logging.info(\"Splitting data into training and testing sets\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "logging.info(f\"Training set shape: {X_train.shape}, Testing set shape: {X_test.shape}\")\n",
    "\n",
    "# Build the RNN model (using LSTM)\n",
    "logging.info(\"Building the LSTM Recurrent Neural Network model\")\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=max_words, output_dim=128, input_length=max_len))  # Embedding layer\n",
    "model.add(LSTM(128, return_sequences=False))  # LSTM layer\n",
    "model.add(Dropout(0.5))  # Dropout for regularization\n",
    "model.add(Dense(64, activation='relu'))  # Fully connected layer\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))  # Output layer for binary classification\n",
    "\n",
    "# Compile the model\n",
    "logging.info(\"Compiling the model\")\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Display model summary\n",
    "model.summary(print_fn=logging.info)\n",
    "\n",
    "# Train the model\n",
    "logging.info(\"Training the model\")\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2, verbose=0)\n",
    "\n",
    "# Log training history\n",
    "logging.info(\"Training history:\")\n",
    "for epoch, (loss, accuracy, val_loss, val_accuracy) in enumerate(zip(\n",
    "    history.history['loss'], history.history['accuracy'],\n",
    "    history.history['val_loss'], history.history['val_accuracy']), 1):\n",
    "    logging.info(f\"Epoch {epoch}: loss={loss:.4f}, accuracy={accuracy:.4f}, val_loss={val_loss:.4f}, val_accuracy={val_accuracy:.4f}\")\n",
    "\n",
    "# Make predictions on the testing set\n",
    "logging.info(\"Making predictions on the testing set\")\n",
    "y_pred_prob = model.predict(X_test)\n",
    "y_pred = (y_pred_prob > 0.5).astype(\"int32\").flatten()\n",
    "logging.info(\"Predictions completed\")\n",
    "\n",
    "# Evaluate the model's performance\n",
    "logging.info(\"Evaluating model performance\")\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "logging.info(\"Recurrent Neural Network (LSTM) Model Performance:\")\n",
    "logging.info(f\"Accuracy: {accuracy:.4f}\")\n",
    "logging.info(f\"Precision: {precision:.4f}\")\n",
    "logging.info(f\"Recall: {recall:.4f}\")\n",
    "logging.info(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "print(\"Recurrent Neural Network (LSTM) Model Performance:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "logging.info(\"LSTM Recurrent Neural Network classification completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "[CV] END ..........................alpha=0.1, fit_prior=True; total time=   0.1s\n",
      "[CV] END ..........................alpha=0.1, fit_prior=True; total time=   0.1s\n",
      "[CV] END ..........................alpha=0.1, fit_prior=True; total time=   0.1s\n",
      "[CV] END ..........................alpha=0.1, fit_prior=True; total time=   0.1s\n",
      "[CV] END ..........................alpha=0.1, fit_prior=True; total time=   0.1s\n",
      "[CV] END .........................alpha=0.1, fit_prior=False; total time=   0.1s\n",
      "[CV] END .........................alpha=0.1, fit_prior=False; total time=   0.1s\n",
      "[CV] END .........................alpha=0.1, fit_prior=False; total time=   0.1s\n",
      "[CV] END ..........................alpha=0.5, fit_prior=True; total time=   0.1s\n",
      "[CV] END ..........................alpha=0.5, fit_prior=True; total time=   0.1s\n",
      "[CV] END .........................alpha=0.1, fit_prior=False; total time=   0.1s\n",
      "[CV] END .........................alpha=0.1, fit_prior=False; total time=   0.1s\n",
      "[CV] END ..........................alpha=0.5, fit_prior=True; total time=   0.1s\n",
      "[CV] END ..........................alpha=0.5, fit_prior=True; total time=   0.1s\n",
      "[CV] END .........................alpha=0.5, fit_prior=False; total time=   0.1s\n",
      "[CV] END ..........................alpha=0.5, fit_prior=True; total time=   0.1s\n",
      "[CV] END .........................alpha=0.5, fit_prior=False; total time=   0.1s\n",
      "[CV] END .........................alpha=0.5, fit_prior=False; total time=   0.1s\n",
      "[CV] END .........................alpha=0.5, fit_prior=False; total time=   0.1s\n",
      "[CV] END .........................alpha=0.5, fit_prior=False; total time=   0.1s\n",
      "[CV] END ..........................alpha=1.0, fit_prior=True; total time=   0.1s\n",
      "[CV] END ..........................alpha=1.0, fit_prior=True; total time=   0.1s\n",
      "[CV] END ..........................alpha=1.0, fit_prior=True; total time=   0.1s\n",
      "[CV] END ..........................alpha=1.0, fit_prior=True; total time=   0.1s\n",
      "[CV] END ..........................alpha=1.0, fit_prior=True; total time=   0.1s\n",
      "[CV] END .........................alpha=1.0, fit_prior=False; total time=   0.1s\n",
      "[CV] END .........................alpha=1.0, fit_prior=False; total time=   0.1s\n",
      "[CV] END .........................alpha=1.0, fit_prior=False; total time=   0.1s\n",
      "[CV] END .........................alpha=1.0, fit_prior=False; total time=   0.1s\n",
      "[CV] END .........................alpha=1.0, fit_prior=False; total time=   0.0s\n",
      "[CV] END ..........................alpha=2.0, fit_prior=True; total time=   0.0s\n",
      "[CV] END ..........................alpha=2.0, fit_prior=True; total time=   0.1s\n",
      "[CV] END ..........................alpha=2.0, fit_prior=True; total time=   0.1s\n",
      "[CV] END ..........................alpha=2.0, fit_prior=True; total time=   0.1s\n",
      "[CV] END ..........................alpha=2.0, fit_prior=True; total time=   0.1s\n",
      "[CV] END .........................alpha=2.0, fit_prior=False; total time=   0.1s\n",
      "[CV] END .........................alpha=2.0, fit_prior=False; total time=   0.0s\n",
      "[CV] END .........................alpha=2.0, fit_prior=False; total time=   0.0s\n",
      "[CV] END .........................alpha=2.0, fit_prior=False; total time=   0.1s\n",
      "[CV] END .........................alpha=2.0, fit_prior=False; total time=   0.0s\n",
      "[CV] END ..........................alpha=5.0, fit_prior=True; total time=   0.0s\n",
      "[CV] END ..........................alpha=5.0, fit_prior=True; total time=   0.1s\n",
      "[CV] END ..........................alpha=5.0, fit_prior=True; total time=   0.1s\n",
      "[CV] END ..........................alpha=5.0, fit_prior=True; total time=   0.0s\n",
      "[CV] END ..........................alpha=5.0, fit_prior=True; total time=   0.0s\n",
      "[CV] END .........................alpha=5.0, fit_prior=False; total time=   0.0s\n",
      "[CV] END .........................alpha=5.0, fit_prior=False; total time=   0.0s\n",
      "[CV] END .........................alpha=5.0, fit_prior=False; total time=   0.0s\n",
      "[CV] END .........................alpha=5.0, fit_prior=False; total time=   0.0s\n",
      "[CV] END .........................alpha=5.0, fit_prior=False; total time=   0.0s\n",
      "Best Model Parameters from Grid Search: {'alpha': 1.0, 'fit_prior': False}\n",
      "BoW Naive Bayes Model Performance after Grid Search:\n",
      "Accuracy: 0.8613\n",
      "Precision: 0.8732624693376942\n",
      "Recall: 0.8477872593768605\n",
      "F1 Score: 0.8603363206122243\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Fit and transform the preprocessed reviews\n",
    "vectorizer = CountVectorizer()\n",
    "bow_features = vectorizer.fit_transform(df['preprocessed_review'])\n",
    "\n",
    "# Get the sentiment labels from the preprocessed DataFrame\n",
    "labels = df['sentiment']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(bow_features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the Naive Bayes model\n",
    "model = MultinomialNB()\n",
    "\n",
    "# Define the hyperparameters grid\n",
    "param_grid = {\n",
    "    'alpha': [0.1, 0.5, 1.0, 2.0, 5.0],  # Smoothing parameter\n",
    "    'fit_prior': [True, False]  # Whether to learn class prior probabilities or not\n",
    "}\n",
    "\n",
    "# Set up Grid Search with cross-validation\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='f1', n_jobs=-1, verbose=2)\n",
    "\n",
    "# Fit Grid Search\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model from Grid Search\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the testing set with the best model\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)  # Specify the positive label\n",
    "recall = recall_score(y_test, y_pred)  # Specify the positive label\n",
    "f1 = f1_score(y_test, y_pred)  # Specify the positive label\n",
    "\n",
    "print(\"Best Model Parameters from Grid Search:\", grid_search.best_params_)\n",
    "print(\"BoW Naive Bayes Model Performance after Grid Search:\")\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV] END .........alpha=0.12789638921577903, fit_prior=False; total time=   0.1s\n",
      "[CV] END .........alpha=0.12789638921577903, fit_prior=False; total time=   0.1s\n",
      "[CV] END .........alpha=0.12789638921577903, fit_prior=False; total time=   0.1s\n",
      "[CV] END .........alpha=0.12789638921577903, fit_prior=False; total time=   0.1s\n",
      "[CV] END .........alpha=0.12789638921577903, fit_prior=False; total time=   0.1s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV] END ..........alpha=1.8200614603794572, fit_prior=False; total time=   0.0s\n",
      "[CV] END ..........alpha=1.8200614603794572, fit_prior=False; total time=   0.0s\n",
      "[CV] END ..........alpha=1.8200614603794572, fit_prior=False; total time=   0.1s\n",
      "[CV] END ..........alpha=1.8200614603794572, fit_prior=False; total time=   0.1s\n",
      "[CV] END ..........alpha=1.8200614603794572, fit_prior=False; total time=   0.1s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV] END ...........alpha=0.158704633925133, fit_prior=False; total time=   0.0s\n",
      "[CV] END ...........alpha=0.158704633925133, fit_prior=False; total time=   0.0s\n",
      "[CV] END ...........alpha=0.158704633925133, fit_prior=False; total time=   0.0s\n",
      "[CV] END ...........alpha=0.158704633925133, fit_prior=False; total time=   0.0s\n",
      "[CV] END ...........alpha=0.158704633925133, fit_prior=False; total time=   0.0s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV] END ...........alpha=1.5582330240128093, fit_prior=True; total time=   0.0s\n",
      "[CV] END ...........alpha=1.5582330240128093, fit_prior=True; total time=   0.0s\n",
      "[CV] END ...........alpha=1.5582330240128093, fit_prior=True; total time=   0.0s\n",
      "[CV] END ...........alpha=1.5582330240128093, fit_prior=True; total time=   0.0s\n",
      "[CV] END ...........alpha=1.5582330240128093, fit_prior=True; total time=   0.0s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV] END ...........alpha=1.4387016966373685, fit_prior=True; total time=   0.0s\n",
      "[CV] END ...........alpha=1.4387016966373685, fit_prior=True; total time=   0.0s\n",
      "[CV] END ...........alpha=1.4387016966373685, fit_prior=True; total time=   0.1s\n",
      "[CV] END ...........alpha=1.4387016966373685, fit_prior=True; total time=   0.0s\n",
      "[CV] END ...........alpha=1.4387016966373685, fit_prior=True; total time=   0.0s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV] END ...........alpha=0.957457714843176, fit_prior=False; total time=   0.0s\n",
      "[CV] END ...........alpha=0.957457714843176, fit_prior=False; total time=   0.0s\n",
      "[CV] END ...........alpha=0.957457714843176, fit_prior=False; total time=   0.0s\n",
      "[CV] END ...........alpha=0.957457714843176, fit_prior=False; total time=   0.0s\n",
      "[CV] END ...........alpha=0.957457714843176, fit_prior=False; total time=   0.0s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV] END .........alpha=0.46289093820761845, fit_prior=False; total time=   0.0s\n",
      "[CV] END .........alpha=0.46289093820761845, fit_prior=False; total time=   0.0s\n",
      "[CV] END .........alpha=0.46289093820761845, fit_prior=False; total time=   0.0s\n",
      "[CV] END .........alpha=0.46289093820761845, fit_prior=False; total time=   0.0s\n",
      "[CV] END .........alpha=0.46289093820761845, fit_prior=False; total time=   0.0s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV] END .........alpha=0.29283877399606956, fit_prior=False; total time=   0.0s\n",
      "[CV] END .........alpha=0.29283877399606956, fit_prior=False; total time=   0.0s\n",
      "[CV] END .........alpha=0.29283877399606956, fit_prior=False; total time=   0.0s\n",
      "[CV] END .........alpha=0.29283877399606956, fit_prior=False; total time=   0.0s\n",
      "[CV] END .........alpha=0.29283877399606956, fit_prior=False; total time=   0.0s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV] END ..........alpha=3.7914595314089237, fit_prior=False; total time=   0.0s\n",
      "[CV] END ..........alpha=3.7914595314089237, fit_prior=False; total time=   0.0s\n",
      "[CV] END ..........alpha=3.7914595314089237, fit_prior=False; total time=   0.0s\n",
      "[CV] END ..........alpha=3.7914595314089237, fit_prior=False; total time=   0.0s\n",
      "[CV] END ..........alpha=3.7914595314089237, fit_prior=False; total time=   0.0s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV] END ........alpha=0.010228173595489182, fit_prior=False; total time=   0.0s\n",
      "[CV] END ........alpha=0.010228173595489182, fit_prior=False; total time=   0.0s\n",
      "[CV] END ........alpha=0.010228173595489182, fit_prior=False; total time=   0.0s\n",
      "[CV] END ........alpha=0.010228173595489182, fit_prior=False; total time=   0.0s\n",
      "[CV] END ........alpha=0.010228173595489182, fit_prior=False; total time=   0.0s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV] END ...........alpha=4.9986558794166385, fit_prior=True; total time=   0.0s\n",
      "[CV] END ...........alpha=4.9986558794166385, fit_prior=True; total time=   0.0s\n",
      "[CV] END ...........alpha=4.9986558794166385, fit_prior=True; total time=   0.0s\n",
      "[CV] END ...........alpha=4.9986558794166385, fit_prior=True; total time=   0.0s\n",
      "[CV] END ...........alpha=4.9986558794166385, fit_prior=True; total time=   0.0s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV] END ..........alpha=0.03771168695288503, fit_prior=True; total time=   0.0s\n",
      "[CV] END ..........alpha=0.03771168695288503, fit_prior=True; total time=   0.1s\n",
      "[CV] END ..........alpha=0.03771168695288503, fit_prior=True; total time=   0.1s\n",
      "[CV] END ..........alpha=0.03771168695288503, fit_prior=True; total time=   0.1s\n",
      "[CV] END ..........alpha=0.03771168695288503, fit_prior=True; total time=   0.1s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV] END ..........alpha=1.0461423339253706, fit_prior=False; total time=   0.0s\n",
      "[CV] END ..........alpha=1.0461423339253706, fit_prior=False; total time=   0.1s\n",
      "[CV] END ..........alpha=1.0461423339253706, fit_prior=False; total time=   0.0s\n",
      "[CV] END ..........alpha=1.0461423339253706, fit_prior=False; total time=   0.1s\n",
      "[CV] END ..........alpha=1.0461423339253706, fit_prior=False; total time=   0.0s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV] END ..........alpha=1.5111205502715355, fit_prior=False; total time=   0.0s\n",
      "[CV] END ..........alpha=1.5111205502715355, fit_prior=False; total time=   0.1s\n",
      "[CV] END ..........alpha=1.5111205502715355, fit_prior=False; total time=   0.0s\n",
      "[CV] END ..........alpha=1.5111205502715355, fit_prior=False; total time=   0.0s\n",
      "[CV] END ..........alpha=1.5111205502715355, fit_prior=False; total time=   0.0s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV] END ............alpha=1.774578130006896, fit_prior=True; total time=   0.0s\n",
      "[CV] END ............alpha=1.774578130006896, fit_prior=True; total time=   0.0s\n",
      "[CV] END ............alpha=1.774578130006896, fit_prior=True; total time=   0.0s\n",
      "[CV] END ............alpha=1.774578130006896, fit_prior=True; total time=   0.0s\n",
      "[CV] END ............alpha=1.774578130006896, fit_prior=True; total time=   0.0s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV] END .........alpha=0.11953613462753016, fit_prior=False; total time=   0.0s\n",
      "[CV] END .........alpha=0.11953613462753016, fit_prior=False; total time=   0.0s\n",
      "[CV] END .........alpha=0.11953613462753016, fit_prior=False; total time=   0.0s\n",
      "[CV] END .........alpha=0.11953613462753016, fit_prior=False; total time=   0.0s\n",
      "[CV] END .........alpha=0.11953613462753016, fit_prior=False; total time=   0.0s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV] END ...........alpha=1.0138807589242531, fit_prior=True; total time=   0.0s\n",
      "[CV] END ...........alpha=1.0138807589242531, fit_prior=True; total time=   0.0s\n",
      "[CV] END ...........alpha=1.0138807589242531, fit_prior=True; total time=   0.0s\n",
      "[CV] END ...........alpha=1.0138807589242531, fit_prior=True; total time=   0.0s\n",
      "[CV] END ...........alpha=1.0138807589242531, fit_prior=True; total time=   0.0s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV] END .........alpha=0.01311689849659441, fit_prior=False; total time=   0.0s\n",
      "[CV] END .........alpha=0.01311689849659441, fit_prior=False; total time=   0.0s\n",
      "[CV] END .........alpha=0.01311689849659441, fit_prior=False; total time=   0.0s\n",
      "[CV] END .........alpha=0.01311689849659441, fit_prior=False; total time=   0.0s\n",
      "[CV] END .........alpha=0.01311689849659441, fit_prior=False; total time=   0.0s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV] END ..........alpha=0.9459039995919293, fit_prior=False; total time=   0.0s\n",
      "[CV] END ..........alpha=0.9459039995919293, fit_prior=False; total time=   0.0s\n",
      "[CV] END ..........alpha=0.9459039995919293, fit_prior=False; total time=   0.0s\n",
      "[CV] END ..........alpha=0.9459039995919293, fit_prior=False; total time=   0.1s\n",
      "[CV] END ..........alpha=0.9459039995919293, fit_prior=False; total time=   0.0s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV] END ...........alpha=1.4071246619302262, fit_prior=True; total time=   0.1s\n",
      "[CV] END ...........alpha=1.4071246619302262, fit_prior=True; total time=   0.1s\n",
      "[CV] END ...........alpha=1.4071246619302262, fit_prior=True; total time=   0.0s\n",
      "[CV] END ...........alpha=1.4071246619302262, fit_prior=True; total time=   0.1s\n",
      "[CV] END ...........alpha=1.4071246619302262, fit_prior=True; total time=   0.1s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV] END .........alpha=0.03293548724089455, fit_prior=False; total time=   0.0s\n",
      "[CV] END .........alpha=0.03293548724089455, fit_prior=False; total time=   0.0s\n",
      "[CV] END .........alpha=0.03293548724089455, fit_prior=False; total time=   0.1s\n",
      "[CV] END .........alpha=0.03293548724089455, fit_prior=False; total time=   0.1s\n",
      "[CV] END .........alpha=0.03293548724089455, fit_prior=False; total time=   0.0s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV] END .........alpha=0.011323366463468439, fit_prior=True; total time=   0.0s\n",
      "[CV] END .........alpha=0.011323366463468439, fit_prior=True; total time=   0.0s\n",
      "[CV] END .........alpha=0.011323366463468439, fit_prior=True; total time=   0.0s\n",
      "[CV] END .........alpha=0.011323366463468439, fit_prior=True; total time=   0.0s\n",
      "[CV] END .........alpha=0.011323366463468439, fit_prior=True; total time=   0.0s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV] END ..........alpha=0.7912271407022267, fit_prior=False; total time=   0.0s\n",
      "[CV] END ..........alpha=0.7912271407022267, fit_prior=False; total time=   0.0s\n",
      "[CV] END ..........alpha=0.7912271407022267, fit_prior=False; total time=   0.0s\n",
      "[CV] END ..........alpha=0.7912271407022267, fit_prior=False; total time=   0.0s\n",
      "[CV] END ..........alpha=0.7912271407022267, fit_prior=False; total time=   0.0s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV] END ..........alpha=0.1435497912114086, fit_prior=False; total time=   0.0s\n",
      "[CV] END ..........alpha=0.1435497912114086, fit_prior=False; total time=   0.0s\n",
      "[CV] END ..........alpha=0.1435497912114086, fit_prior=False; total time=   0.0s\n",
      "[CV] END ..........alpha=0.1435497912114086, fit_prior=False; total time=   0.0s\n",
      "[CV] END ..........alpha=0.1435497912114086, fit_prior=False; total time=   0.0s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV] END ..........alpha=0.04960789455735967, fit_prior=True; total time=   0.0s\n",
      "[CV] END ..........alpha=0.04960789455735967, fit_prior=True; total time=   0.0s\n",
      "[CV] END ..........alpha=0.04960789455735967, fit_prior=True; total time=   0.0s\n",
      "[CV] END ..........alpha=0.04960789455735967, fit_prior=True; total time=   0.0s\n",
      "[CV] END ..........alpha=0.04960789455735967, fit_prior=True; total time=   0.0s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV] END ...........alpha=0.8990196457474859, fit_prior=True; total time=   0.0s\n",
      "[CV] END ...........alpha=0.8990196457474859, fit_prior=True; total time=   0.0s\n",
      "[CV] END ...........alpha=0.8990196457474859, fit_prior=True; total time=   0.0s\n",
      "[CV] END ...........alpha=0.8990196457474859, fit_prior=True; total time=   0.0s\n",
      "[CV] END ...........alpha=0.8990196457474859, fit_prior=True; total time=   0.0s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV] END ..........alpha=0.9988730334910979, fit_prior=False; total time=   0.0s\n",
      "[CV] END ..........alpha=0.9988730334910979, fit_prior=False; total time=   0.0s\n",
      "[CV] END ..........alpha=0.9988730334910979, fit_prior=False; total time=   0.0s\n",
      "[CV] END ..........alpha=0.9988730334910979, fit_prior=False; total time=   0.0s\n",
      "[CV] END ..........alpha=0.9988730334910979, fit_prior=False; total time=   0.0s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV] END ...........alpha=1.001876362599294, fit_prior=False; total time=   0.0s\n",
      "[CV] END ...........alpha=1.001876362599294, fit_prior=False; total time=   0.0s\n",
      "[CV] END ...........alpha=1.001876362599294, fit_prior=False; total time=   0.0s\n",
      "[CV] END ...........alpha=1.001876362599294, fit_prior=False; total time=   0.1s\n",
      "[CV] END ...........alpha=1.001876362599294, fit_prior=False; total time=   0.0s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV] END ...........alpha=2.611314132146855, fit_prior=False; total time=   0.0s\n",
      "[CV] END ...........alpha=2.611314132146855, fit_prior=False; total time=   0.0s\n",
      "[CV] END ...........alpha=2.611314132146855, fit_prior=False; total time=   0.0s\n",
      "[CV] END ...........alpha=2.611314132146855, fit_prior=False; total time=   0.0s\n",
      "[CV] END ...........alpha=2.611314132146855, fit_prior=False; total time=   0.0s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV] END ...........alpha=2.350073369939508, fit_prior=False; total time=   0.0s\n",
      "[CV] END ...........alpha=2.350073369939508, fit_prior=False; total time=   0.0s\n",
      "[CV] END ...........alpha=2.350073369939508, fit_prior=False; total time=   0.0s\n",
      "[CV] END ...........alpha=2.350073369939508, fit_prior=False; total time=   0.0s\n",
      "[CV] END ...........alpha=2.350073369939508, fit_prior=False; total time=   0.0s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV] END ..........alpha=2.2544020004467433, fit_prior=False; total time=   0.0s\n",
      "[CV] END ..........alpha=2.2544020004467433, fit_prior=False; total time=   0.0s\n",
      "[CV] END ..........alpha=2.2544020004467433, fit_prior=False; total time=   0.0s\n",
      "[CV] END ..........alpha=2.2544020004467433, fit_prior=False; total time=   0.0s\n",
      "[CV] END ..........alpha=2.2544020004467433, fit_prior=False; total time=   0.0s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV] END ...........alpha=2.359398624544186, fit_prior=False; total time=   0.0s\n",
      "[CV] END ...........alpha=2.359398624544186, fit_prior=False; total time=   0.0s\n",
      "[CV] END ...........alpha=2.359398624544186, fit_prior=False; total time=   0.0s\n",
      "[CV] END ...........alpha=2.359398624544186, fit_prior=False; total time=   0.0s\n",
      "[CV] END ...........alpha=2.359398624544186, fit_prior=False; total time=   0.0s\n",
      "Best Model Parameters from Bayesian Search: OrderedDict({'alpha': 2.350073369939508, 'fit_prior': False})\n",
      "BoW Naive Bayes Model Performance after Bayesian Search:\n",
      "Accuracy: 0.8608\n",
      "Precision: 0.8728276426088735\n",
      "Recall: 0.847191903155388\n",
      "F1 Score: 0.8598187311178248\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "\n",
    "# Fit and transform the preprocessed reviews\n",
    "vectorizer = CountVectorizer()\n",
    "bow_features = vectorizer.fit_transform(df['preprocessed_review'])\n",
    "\n",
    "# Get the sentiment labels from the preprocessed DataFrame\n",
    "labels = df['sentiment']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(bow_features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the Naive Bayes model\n",
    "model = MultinomialNB()\n",
    "\n",
    "# Define the hyperparameter space\n",
    "param_space = {\n",
    "    'alpha': Real(0.01, 5.0, prior='log-uniform'),  # Log-uniform distribution is used for the smoothing parameter\n",
    "    'fit_prior': Categorical([True, False])  # Categorical distribution for fit_prior\n",
    "}\n",
    "\n",
    "# Set up Bayesian Search with cross-validation\n",
    "bayes_search = BayesSearchCV(\n",
    "    estimator=model,\n",
    "    search_spaces=param_space,\n",
    "    n_iter=32,  # Number of parameter settings that are sampled\n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    scoring='f1',\n",
    "    n_jobs=-1,  # Use all available cores\n",
    "    verbose=2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit Bayesian Search\n",
    "bayes_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model from Bayesian Search\n",
    "best_model = bayes_search.best_estimator_\n",
    "\n",
    "# Make predictions on the testing set with the best model\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)  # Specify the positive label\n",
    "recall = recall_score(y_test, y_pred)  # Specify the positive label\n",
    "f1 = f1_score(y_test, y_pred)  # Specify the positive label\n",
    "\n",
    "print(\"Best Model Parameters from Bayesian Search:\", bayes_search.best_params_)\n",
    "print(\"BoW Naive Bayes Model Performance after Bayesian Search:\")\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "[CV] END ...............C=0.01, penalty=l1, solver=liblinear; total time=   0.6s\n",
      "[CV] END ...............C=0.01, penalty=l1, solver=liblinear; total time=   0.6s\n",
      "[CV] END ...............C=0.01, penalty=l1, solver=liblinear; total time=   0.6s\n",
      "[CV] END ...............C=0.01, penalty=l1, solver=liblinear; total time=   0.6s\n",
      "[CV] END ...............C=0.01, penalty=l1, solver=liblinear; total time=   0.6s\n",
      "[CV] END ...............C=0.01, penalty=l2, solver=liblinear; total time=   1.3s\n",
      "[CV] END ...............C=0.01, penalty=l2, solver=liblinear; total time=   1.4s\n",
      "[CV] END ...............C=0.01, penalty=l2, solver=liblinear; total time=   1.6s\n",
      "[CV] END ...............C=0.01, penalty=l2, solver=liblinear; total time=   1.4s\n",
      "[CV] END ...............C=0.01, penalty=l2, solver=liblinear; total time=   1.4s\n",
      "[CV] END ....................C=0.01, penalty=l1, solver=saga; total time=   9.6s\n",
      "[CV] END ....................C=0.01, penalty=l2, solver=saga; total time=   9.4s\n",
      "[CV] END ....................C=0.01, penalty=l2, solver=saga; total time=   8.8s\n",
      "[CV] END .......C=0.01, penalty=elasticnet, solver=liblinear; total time=   0.0s\n",
      "[CV] END .......C=0.01, penalty=elasticnet, solver=liblinear; total time=   0.0s\n",
      "[CV] END .......C=0.01, penalty=elasticnet, solver=liblinear; total time=   0.0s\n",
      "[CV] END .......C=0.01, penalty=elasticnet, solver=liblinear; total time=   0.0s\n",
      "[CV] END .......C=0.01, penalty=elasticnet, solver=liblinear; total time=   0.0s\n",
      "[CV] END ............C=0.01, penalty=elasticnet, solver=saga; total time=   0.0s\n",
      "[CV] END ............C=0.01, penalty=elasticnet, solver=saga; total time=   0.0s\n",
      "[CV] END ............C=0.01, penalty=elasticnet, solver=saga; total time=   0.0s\n",
      "[CV] END ............C=0.01, penalty=elasticnet, solver=saga; total time=   0.0s\n",
      "[CV] END ............C=0.01, penalty=elasticnet, solver=saga; total time=   0.0s\n",
      "[CV] END .............C=0.01, penalty=none, solver=liblinear; total time=   0.0s\n",
      "[CV] END .............C=0.01, penalty=none, solver=liblinear; total time=   0.0s\n",
      "[CV] END .............C=0.01, penalty=none, solver=liblinear; total time=   0.0s\n",
      "[CV] END .............C=0.01, penalty=none, solver=liblinear; total time=   0.0s\n",
      "[CV] END ....................C=0.01, penalty=l2, solver=saga; total time=   9.1s\n",
      "[CV] END .............C=0.01, penalty=none, solver=liblinear; total time=   0.0s\n",
      "[CV] END ..................C=0.01, penalty=none, solver=saga; total time=   0.0s\n",
      "[CV] END ..................C=0.01, penalty=none, solver=saga; total time=   0.0s\n",
      "[CV] END ..................C=0.01, penalty=none, solver=saga; total time=   0.0s\n",
      "[CV] END ..................C=0.01, penalty=none, solver=saga; total time=   0.0s\n",
      "[CV] END ..................C=0.01, penalty=none, solver=saga; total time=   0.0s\n",
      "[CV] END ....................C=0.01, penalty=l1, solver=saga; total time=  12.9s\n",
      "[CV] END ................C=0.1, penalty=l1, solver=liblinear; total time=   0.5s\n",
      "[CV] END ................C=0.1, penalty=l1, solver=liblinear; total time=   0.6s\n",
      "[CV] END ................C=0.1, penalty=l1, solver=liblinear; total time=   0.6s\n",
      "[CV] END ....................C=0.01, penalty=l1, solver=saga; total time=  13.6s\n",
      "[CV] END ................C=0.1, penalty=l1, solver=liblinear; total time=   0.5s\n",
      "[CV] END ................C=0.1, penalty=l1, solver=liblinear; total time=   0.5s\n",
      "[CV] END ....................C=0.01, penalty=l1, solver=saga; total time=  13.8s\n",
      "[CV] END ....................C=0.01, penalty=l1, solver=saga; total time=  13.7s\n",
      "[CV] END ....................C=0.01, penalty=l2, solver=saga; total time=   5.9s\n",
      "[CV] END ................C=0.1, penalty=l2, solver=liblinear; total time=   1.9s\n",
      "[CV] END ................C=0.1, penalty=l2, solver=liblinear; total time=   2.0s\n",
      "[CV] END ................C=0.1, penalty=l2, solver=liblinear; total time=   2.1s\n",
      "[CV] END ....................C=0.01, penalty=l2, solver=saga; total time=   7.9s\n",
      "[CV] END ................C=0.1, penalty=l2, solver=liblinear; total time=   2.1s\n",
      "[CV] END ................C=0.1, penalty=l2, solver=liblinear; total time=   2.1s\n",
      "[CV] END .....................C=0.1, penalty=l2, solver=saga; total time=  39.0s\n",
      "[CV] END .....................C=0.1, penalty=l2, solver=saga; total time=  39.3s\n",
      "[CV] END .....................C=0.1, penalty=l2, solver=saga; total time=  41.4s\n",
      "[CV] END ........C=0.1, penalty=elasticnet, solver=liblinear; total time=   0.0s\n",
      "[CV] END ........C=0.1, penalty=elasticnet, solver=liblinear; total time=   0.0s\n",
      "[CV] END ........C=0.1, penalty=elasticnet, solver=liblinear; total time=   0.0s\n",
      "[CV] END ........C=0.1, penalty=elasticnet, solver=liblinear; total time=   0.0s\n",
      "[CV] END ........C=0.1, penalty=elasticnet, solver=liblinear; total time=   0.0s\n",
      "[CV] END .............C=0.1, penalty=elasticnet, solver=saga; total time=   0.0s\n",
      "[CV] END .............C=0.1, penalty=elasticnet, solver=saga; total time=   0.0s\n",
      "[CV] END .............C=0.1, penalty=elasticnet, solver=saga; total time=   0.0s\n",
      "[CV] END .............C=0.1, penalty=elasticnet, solver=saga; total time=   0.0s\n",
      "[CV] END .............C=0.1, penalty=elasticnet, solver=saga; total time=   0.0s\n",
      "[CV] END ..............C=0.1, penalty=none, solver=liblinear; total time=   0.0s\n",
      "[CV] END ..............C=0.1, penalty=none, solver=liblinear; total time=   0.0s\n",
      "[CV] END ..............C=0.1, penalty=none, solver=liblinear; total time=   0.0s\n",
      "[CV] END ..............C=0.1, penalty=none, solver=liblinear; total time=   0.0s\n",
      "[CV] END ..............C=0.1, penalty=none, solver=liblinear; total time=   0.0s\n",
      "[CV] END ...................C=0.1, penalty=none, solver=saga; total time=   0.0s\n",
      "[CV] END ...................C=0.1, penalty=none, solver=saga; total time=   0.0s\n",
      "[CV] END ...................C=0.1, penalty=none, solver=saga; total time=   0.0s\n",
      "[CV] END ...................C=0.1, penalty=none, solver=saga; total time=   0.0s\n",
      "[CV] END ...................C=0.1, penalty=none, solver=saga; total time=   0.0s\n",
      "[CV] END ..................C=1, penalty=l1, solver=liblinear; total time=   1.0s\n",
      "[CV] END ..................C=1, penalty=l1, solver=liblinear; total time=   0.9s\n",
      "[CV] END ..................C=1, penalty=l1, solver=liblinear; total time=   1.1s\n",
      "[CV] END ..................C=1, penalty=l1, solver=liblinear; total time=   1.1s\n",
      "[CV] END ..................C=1, penalty=l1, solver=liblinear; total time=   1.0s\n",
      "[CV] END .....................C=0.1, penalty=l2, solver=saga; total time=  28.0s\n",
      "[CV] END .....................C=0.1, penalty=l2, solver=saga; total time=  37.6s\n",
      "[CV] END .....................C=0.1, penalty=l1, solver=saga; total time= 3.4min\n",
      "[CV] END .....................C=0.1, penalty=l1, solver=saga; total time= 4.7min\n",
      "[CV] END .....................C=0.1, penalty=l1, solver=saga; total time= 4.8min\n",
      "[CV] END .....................C=0.1, penalty=l1, solver=saga; total time= 4.9min\n",
      "[CV] END ..................C=1, penalty=l2, solver=liblinear; total time=   3.9s\n",
      "[CV] END ..................C=1, penalty=l2, solver=liblinear; total time=   4.0s\n",
      "[CV] END ..................C=1, penalty=l2, solver=liblinear; total time=   3.8s\n",
      "[CV] END ..................C=1, penalty=l2, solver=liblinear; total time=   4.4s\n",
      "[CV] END ..................C=1, penalty=l2, solver=liblinear; total time=   4.9s\n",
      "[CV] END .......................C=1, penalty=l2, solver=saga; total time= 1.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fpolygon/.pyenv/versions/3.12.4/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END .......................C=1, penalty=l2, solver=saga; total time= 1.3min\n",
      "[CV] END .....................C=0.1, penalty=l1, solver=saga; total time= 6.3min\n",
      "[CV] END .......................C=1, penalty=l2, solver=saga; total time= 1.1min\n",
      "[CV] END ..........C=1, penalty=elasticnet, solver=liblinear; total time=   0.0s\n",
      "[CV] END ..........C=1, penalty=elasticnet, solver=liblinear; total time=   0.0s\n",
      "[CV] END ..........C=1, penalty=elasticnet, solver=liblinear; total time=   0.0s\n",
      "[CV] END ..........C=1, penalty=elasticnet, solver=liblinear; total time=   0.0s\n",
      "[CV] END ..........C=1, penalty=elasticnet, solver=liblinear; total time=   0.0s\n",
      "[CV] END ...............C=1, penalty=elasticnet, solver=saga; total time=   0.0s\n",
      "[CV] END ...............C=1, penalty=elasticnet, solver=saga; total time=   0.0s\n",
      "[CV] END ...............C=1, penalty=elasticnet, solver=saga; total time=   0.0s\n",
      "[CV] END ...............C=1, penalty=elasticnet, solver=saga; total time=   0.0s\n",
      "[CV] END ...............C=1, penalty=elasticnet, solver=saga; total time=   0.0s\n",
      "[CV] END ................C=1, penalty=none, solver=liblinear; total time=   0.0s\n",
      "[CV] END ................C=1, penalty=none, solver=liblinear; total time=   0.0s\n",
      "[CV] END ................C=1, penalty=none, solver=liblinear; total time=   0.0s\n",
      "[CV] END ................C=1, penalty=none, solver=liblinear; total time=   0.0s\n",
      "[CV] END ................C=1, penalty=none, solver=liblinear; total time=   0.0s\n",
      "[CV] END .....................C=1, penalty=none, solver=saga; total time=   0.0s\n",
      "[CV] END .....................C=1, penalty=none, solver=saga; total time=   0.0s\n",
      "[CV] END .....................C=1, penalty=none, solver=saga; total time=   0.0s\n",
      "[CV] END .....................C=1, penalty=none, solver=saga; total time=   0.0s\n",
      "[CV] END .....................C=1, penalty=none, solver=saga; total time=   0.0s\n",
      "[CV] END .................C=10, penalty=l1, solver=liblinear; total time=   1.3s\n",
      "[CV] END .................C=10, penalty=l1, solver=liblinear; total time=   1.4s\n",
      "[CV] END .................C=10, penalty=l1, solver=liblinear; total time=   1.4s\n",
      "[CV] END .................C=10, penalty=l1, solver=liblinear; total time=   1.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fpolygon/.pyenv/versions/3.12.4/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END .......................C=1, penalty=l2, solver=saga; total time= 1.4min\n",
      "[CV] END .................C=10, penalty=l1, solver=liblinear; total time=   1.3s\n",
      "[CV] END .......................C=1, penalty=l2, solver=saga; total time= 1.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fpolygon/.pyenv/versions/3.12.4/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END .......................C=1, penalty=l1, solver=saga; total time=57.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fpolygon/.pyenv/versions/3.12.4/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END .......................C=1, penalty=l1, solver=saga; total time=65.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fpolygon/.pyenv/versions/3.12.4/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END .......................C=1, penalty=l1, solver=saga; total time=66.4min\n",
      "[CV] END .................C=10, penalty=l2, solver=liblinear; total time=   4.9s\n",
      "[CV] END .................C=10, penalty=l2, solver=liblinear; total time=   4.8s\n",
      "[CV] END .................C=10, penalty=l2, solver=liblinear; total time=   4.4s\n",
      "[CV] END .................C=10, penalty=l2, solver=liblinear; total time=   4.0s\n",
      "[CV] END .................C=10, penalty=l2, solver=liblinear; total time=   4.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fpolygon/.pyenv/versions/3.12.4/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END .......................C=1, penalty=l1, solver=saga; total time=66.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fpolygon/.pyenv/versions/3.12.4/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ......................C=10, penalty=l2, solver=saga; total time= 1.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fpolygon/.pyenv/versions/3.12.4/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ......................C=10, penalty=l2, solver=saga; total time= 1.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fpolygon/.pyenv/versions/3.12.4/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ......................C=10, penalty=l2, solver=saga; total time= 1.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fpolygon/.pyenv/versions/3.12.4/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END .......................C=1, penalty=l1, solver=saga; total time=65.3min\n",
      "[CV] END .........C=10, penalty=elasticnet, solver=liblinear; total time=   0.0s\n",
      "[CV] END .........C=10, penalty=elasticnet, solver=liblinear; total time=   0.0s\n",
      "[CV] END .........C=10, penalty=elasticnet, solver=liblinear; total time=   0.0s\n",
      "[CV] END .........C=10, penalty=elasticnet, solver=liblinear; total time=   0.0s\n",
      "[CV] END .........C=10, penalty=elasticnet, solver=liblinear; total time=   0.0s\n",
      "[CV] END ..............C=10, penalty=elasticnet, solver=saga; total time=   0.0s\n",
      "[CV] END ..............C=10, penalty=elasticnet, solver=saga; total time=   0.0s\n",
      "[CV] END ..............C=10, penalty=elasticnet, solver=saga; total time=   0.0s\n",
      "[CV] END ..............C=10, penalty=elasticnet, solver=saga; total time=   0.0s\n",
      "[CV] END ..............C=10, penalty=elasticnet, solver=saga; total time=   0.0s\n",
      "[CV] END ...............C=10, penalty=none, solver=liblinear; total time=   0.0s\n",
      "[CV] END ...............C=10, penalty=none, solver=liblinear; total time=   0.0s\n",
      "[CV] END ...............C=10, penalty=none, solver=liblinear; total time=   0.0s\n",
      "[CV] END ...............C=10, penalty=none, solver=liblinear; total time=   0.0s\n",
      "[CV] END ...............C=10, penalty=none, solver=liblinear; total time=   0.0s\n",
      "[CV] END ....................C=10, penalty=none, solver=saga; total time=   0.0s\n",
      "[CV] END ....................C=10, penalty=none, solver=saga; total time=   0.0s\n",
      "[CV] END ....................C=10, penalty=none, solver=saga; total time=   0.0s\n",
      "[CV] END ....................C=10, penalty=none, solver=saga; total time=   0.0s\n",
      "[CV] END ....................C=10, penalty=none, solver=saga; total time=   0.0s\n",
      "[CV] END ................C=100, penalty=l1, solver=liblinear; total time=   1.3s\n",
      "[CV] END ................C=100, penalty=l1, solver=liblinear; total time=   1.7s\n",
      "[CV] END ................C=100, penalty=l1, solver=liblinear; total time=   1.7s\n",
      "[CV] END ................C=100, penalty=l1, solver=liblinear; total time=   1.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fpolygon/.pyenv/versions/3.12.4/lib/python3.12/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ................C=100, penalty=l1, solver=liblinear; total time=   1.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fpolygon/.pyenv/versions/3.12.4/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ......................C=10, penalty=l2, solver=saga; total time= 1.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fpolygon/.pyenv/versions/3.12.4/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ......................C=10, penalty=l2, solver=saga; total time= 1.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fpolygon/.pyenv/versions/3.12.4/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END .....................C=10, penalty=l1, solver=saga; total time=359.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fpolygon/.pyenv/versions/3.12.4/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END .....................C=10, penalty=l1, solver=saga; total time=417.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fpolygon/.pyenv/versions/3.12.4/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END .....................C=10, penalty=l1, solver=saga; total time=419.2min\n",
      "[CV] END ................C=100, penalty=l2, solver=liblinear; total time=   6.2s\n",
      "[CV] END ................C=100, penalty=l2, solver=liblinear; total time=   5.2s\n",
      "[CV] END ................C=100, penalty=l2, solver=liblinear; total time=   4.6s\n",
      "[CV] END ................C=100, penalty=l2, solver=liblinear; total time=   4.5s\n",
      "[CV] END ................C=100, penalty=l2, solver=liblinear; total time=   4.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fpolygon/.pyenv/versions/3.12.4/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END .....................C=100, penalty=l2, solver=saga; total time= 1.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fpolygon/.pyenv/versions/3.12.4/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END .....................C=10, penalty=l1, solver=saga; total time=421.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fpolygon/.pyenv/versions/3.12.4/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END .....................C=100, penalty=l2, solver=saga; total time= 1.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fpolygon/.pyenv/versions/3.12.4/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END .....................C=100, penalty=l2, solver=saga; total time= 1.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fpolygon/.pyenv/versions/3.12.4/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END .....................C=100, penalty=l2, solver=saga; total time= 1.3min\n",
      "[CV] END ........C=100, penalty=elasticnet, solver=liblinear; total time=   0.0s\n",
      "[CV] END ........C=100, penalty=elasticnet, solver=liblinear; total time=   0.0s\n",
      "[CV] END ........C=100, penalty=elasticnet, solver=liblinear; total time=   0.0s\n",
      "[CV] END ........C=100, penalty=elasticnet, solver=liblinear; total time=   0.0s\n",
      "[CV] END ........C=100, penalty=elasticnet, solver=liblinear; total time=   0.0s\n",
      "[CV] END .............C=100, penalty=elasticnet, solver=saga; total time=   0.0s\n",
      "[CV] END .............C=100, penalty=elasticnet, solver=saga; total time=   0.0s\n",
      "[CV] END .............C=100, penalty=elasticnet, solver=saga; total time=   0.0s\n",
      "[CV] END .............C=100, penalty=elasticnet, solver=saga; total time=   0.0s\n",
      "[CV] END .............C=100, penalty=elasticnet, solver=saga; total time=   0.0s\n",
      "[CV] END ..............C=100, penalty=none, solver=liblinear; total time=   0.0s\n",
      "[CV] END ..............C=100, penalty=none, solver=liblinear; total time=   0.0s\n",
      "[CV] END ..............C=100, penalty=none, solver=liblinear; total time=   0.0s\n",
      "[CV] END ..............C=100, penalty=none, solver=liblinear; total time=   0.0s\n",
      "[CV] END ..............C=100, penalty=none, solver=liblinear; total time=   0.0s\n",
      "[CV] END ...................C=100, penalty=none, solver=saga; total time=   0.0s\n",
      "[CV] END ...................C=100, penalty=none, solver=saga; total time=   0.0s\n",
      "[CV] END ...................C=100, penalty=none, solver=saga; total time=   0.0s\n",
      "[CV] END ...................C=100, penalty=none, solver=saga; total time=   0.0s\n",
      "[CV] END ...................C=100, penalty=none, solver=saga; total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fpolygon/.pyenv/versions/3.12.4/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END .....................C=100, penalty=l2, solver=saga; total time= 1.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fpolygon/.pyenv/versions/3.12.4/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END .....................C=10, penalty=l1, solver=saga; total time=412.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fpolygon/.pyenv/versions/3.12.4/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....................C=100, penalty=l1, solver=saga; total time=730.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fpolygon/.pyenv/versions/3.12.4/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....................C=100, penalty=l1, solver=saga; total time=732.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fpolygon/.pyenv/versions/3.12.4/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....................C=100, penalty=l1, solver=saga; total time=737.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fpolygon/.pyenv/versions/3.12.4/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....................C=100, penalty=l1, solver=saga; total time=623.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fpolygon/.pyenv/versions/3.12.4/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/fpolygon/.pyenv/versions/3.12.4/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:540: FitFailedWarning: \n",
      "100 fits failed out of a total of 200.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "25 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/fpolygon/.pyenv/versions/3.12.4/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/fpolygon/.pyenv/versions/3.12.4/lib/python3.12/site-packages/sklearn/base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/fpolygon/.pyenv/versions/3.12.4/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py\", line 1194, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/fpolygon/.pyenv/versions/3.12.4/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py\", line 75, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "25 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/fpolygon/.pyenv/versions/3.12.4/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/fpolygon/.pyenv/versions/3.12.4/lib/python3.12/site-packages/sklearn/base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/fpolygon/.pyenv/versions/3.12.4/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py\", line 1204, in fit\n",
      "    raise ValueError(\"l1_ratio must be specified when penalty is elasticnet.\")\n",
      "ValueError: l1_ratio must be specified when penalty is elasticnet.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "20 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/fpolygon/.pyenv/versions/3.12.4/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/fpolygon/.pyenv/versions/3.12.4/lib/python3.12/site-packages/sklearn/base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"/Users/fpolygon/.pyenv/versions/3.12.4/lib/python3.12/site-packages/sklearn/base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"/Users/fpolygon/.pyenv/versions/3.12.4/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'penalty' parameter of LogisticRegression must be a str among {'l1', 'l2', 'elasticnet'} or None. Got 'none' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "20 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/fpolygon/.pyenv/versions/3.12.4/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/fpolygon/.pyenv/versions/3.12.4/lib/python3.12/site-packages/sklearn/base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"/Users/fpolygon/.pyenv/versions/3.12.4/lib/python3.12/site-packages/sklearn/base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"/Users/fpolygon/.pyenv/versions/3.12.4/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'penalty' parameter of LogisticRegression must be a str among {'l1', 'elasticnet', 'l2'} or None. Got 'none' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "10 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/fpolygon/.pyenv/versions/3.12.4/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/fpolygon/.pyenv/versions/3.12.4/lib/python3.12/site-packages/sklearn/base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"/Users/fpolygon/.pyenv/versions/3.12.4/lib/python3.12/site-packages/sklearn/base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"/Users/fpolygon/.pyenv/versions/3.12.4/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'penalty' parameter of LogisticRegression must be a str among {'l2', 'l1', 'elasticnet'} or None. Got 'none' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/fpolygon/.pyenv/versions/3.12.4/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1052: UserWarning: One or more of the test scores are non-finite: [0.82722698 0.82723191 0.88361567 0.88360525        nan        nan\n",
      "        nan        nan 0.879794   0.87997808 0.88965706 0.88960796\n",
      "        nan        nan        nan        nan 0.87728121 0.88243345\n",
      " 0.88225144 0.8848577         nan        nan        nan        nan\n",
      " 0.86837296 0.88202276 0.87647154 0.88289951        nan        nan\n",
      "        nan        nan 0.8686632  0.88262727 0.87374104 0.88239537\n",
      "        nan        nan        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....................C=100, penalty=l1, solver=saga; total time=677.9min\n",
      "Best Model Parameters from Grid Search: {'C': 0.1, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "BoW Logistic Regression Model Performance after Grid Search:\n",
      "Accuracy: 0.8927\n",
      "Precision: 0.8871534556813745\n",
      "Recall: 0.9017662234570352\n",
      "F1 Score: 0.8944001574648165\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Get the sentiment labels from the preprocessed DataFrame\n",
    "labels = df['sentiment']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(bow_features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the Logistic Regression model\n",
    "model = LogisticRegression(max_iter=2000)\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],  # Regularization strength (inverse of regularization parameter)\n",
    "    'penalty': ['l1', 'l2', 'elasticnet', 'none'],  # Regularization types\n",
    "    'solver': ['liblinear', 'saga']  # Solvers that support l1 and elasticnet penalties\n",
    "}\n",
    "\n",
    "# Set up Grid Search with cross-validation\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='f1', n_jobs=-1, verbose=2)\n",
    "\n",
    "# Fit Grid Search\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model from Grid Search\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the testing set with the best model\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"Best Model Parameters from Grid Search:\", grid_search.best_params_)\n",
    "print(\"BoW Logistic Regression Model Performance after Grid Search:\")\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "[CV] END ..metric=euclidean, n_neighbors=3, weights=distance; total time=  23.3s\n",
      "[CV] END ...metric=euclidean, n_neighbors=3, weights=uniform; total time=  23.6s\n",
      "[CV] END ..metric=euclidean, n_neighbors=3, weights=distance; total time=  23.6s\n",
      "[CV] END ..metric=euclidean, n_neighbors=3, weights=distance; total time=  23.6s\n",
      "[CV] END ...metric=euclidean, n_neighbors=3, weights=uniform; total time=  25.5s\n",
      "[CV] END ...metric=euclidean, n_neighbors=3, weights=uniform; total time=  23.7s\n",
      "[CV] END ...metric=euclidean, n_neighbors=3, weights=uniform; total time=  23.7s\n",
      "[CV] END ...metric=euclidean, n_neighbors=3, weights=uniform; total time=  23.7s\n",
      "[CV] END ..metric=euclidean, n_neighbors=3, weights=distance; total time=  20.4s\n",
      "[CV] END ...metric=euclidean, n_neighbors=5, weights=uniform; total time=  24.7s\n",
      "[CV] END ..metric=euclidean, n_neighbors=5, weights=distance; total time=  24.8s\n",
      "[CV] END ...metric=euclidean, n_neighbors=5, weights=uniform; total time=  24.9s\n",
      "[CV] END ...metric=euclidean, n_neighbors=5, weights=uniform; total time=  24.9s\n",
      "[CV] END ...metric=euclidean, n_neighbors=5, weights=uniform; total time=  25.0s\n",
      "[CV] END ...metric=euclidean, n_neighbors=5, weights=uniform; total time=  25.1s\n",
      "[CV] END ..metric=euclidean, n_neighbors=3, weights=distance; total time=  25.7s\n",
      "[CV] END ..metric=euclidean, n_neighbors=5, weights=distance; total time=  15.3s\n",
      "[CV] END ..metric=euclidean, n_neighbors=5, weights=distance; total time=  22.7s\n",
      "[CV] END ..metric=euclidean, n_neighbors=5, weights=distance; total time=  22.7s\n",
      "[CV] END ..metric=euclidean, n_neighbors=5, weights=distance; total time=  22.9s\n",
      "[CV] END ...metric=euclidean, n_neighbors=7, weights=uniform; total time=  22.1s\n",
      "[CV] END ...metric=euclidean, n_neighbors=7, weights=uniform; total time=  22.8s\n",
      "[CV] END ...metric=euclidean, n_neighbors=7, weights=uniform; total time=  22.9s\n",
      "[CV] END ...metric=euclidean, n_neighbors=7, weights=uniform; total time=  22.9s\n",
      "[CV] END ...metric=euclidean, n_neighbors=7, weights=uniform; total time=  16.2s\n",
      "[CV] END ..metric=euclidean, n_neighbors=7, weights=distance; total time=  21.9s\n",
      "[CV] END ..metric=euclidean, n_neighbors=7, weights=distance; total time=  22.0s\n",
      "[CV] END ..metric=euclidean, n_neighbors=7, weights=distance; total time=  22.2s\n",
      "[CV] END ..metric=euclidean, n_neighbors=7, weights=distance; total time=  22.2s\n",
      "[CV] END ..metric=euclidean, n_neighbors=7, weights=distance; total time=  22.0s\n",
      "[CV] END ...metric=euclidean, n_neighbors=9, weights=uniform; total time=  22.0s\n",
      "[CV] END ...metric=euclidean, n_neighbors=9, weights=uniform; total time=  18.1s\n",
      "[CV] END ...metric=euclidean, n_neighbors=9, weights=uniform; total time=  22.2s\n",
      "[CV] END ...metric=euclidean, n_neighbors=9, weights=uniform; total time=  18.8s\n",
      "[CV] END ..metric=euclidean, n_neighbors=9, weights=distance; total time=  23.0s\n",
      "[CV] END ..metric=euclidean, n_neighbors=9, weights=distance; total time=  23.1s\n",
      "[CV] END ...metric=euclidean, n_neighbors=9, weights=uniform; total time=  23.2s\n",
      "[CV] END ..metric=euclidean, n_neighbors=9, weights=distance; total time=  23.2s\n",
      "[CV] END ..metric=euclidean, n_neighbors=9, weights=distance; total time=  23.3s\n",
      "[CV] END ..metric=euclidean, n_neighbors=9, weights=distance; total time=  23.3s\n",
      "[CV] END ..metric=euclidean, n_neighbors=11, weights=uniform; total time=  23.2s\n",
      "[CV] END ..metric=euclidean, n_neighbors=11, weights=uniform; total time=  15.6s\n",
      "[CV] END .metric=euclidean, n_neighbors=11, weights=distance; total time=  23.1s\n",
      "[CV] END .metric=euclidean, n_neighbors=11, weights=distance; total time=  23.0s\n",
      "[CV] END .metric=euclidean, n_neighbors=11, weights=distance; total time=  23.2s\n",
      "[CV] END .metric=euclidean, n_neighbors=11, weights=distance; total time=  23.2s\n",
      "[CV] END ..metric=euclidean, n_neighbors=11, weights=uniform; total time=  23.4s\n",
      "[CV] END ..metric=euclidean, n_neighbors=11, weights=uniform; total time=  23.6s\n",
      "[CV] END ..metric=euclidean, n_neighbors=11, weights=uniform; total time=  23.7s\n",
      "[CV] END .metric=euclidean, n_neighbors=11, weights=distance; total time=  16.3s\n",
      "[CV] END ...metric=manhattan, n_neighbors=3, weights=uniform; total time= 3.5min\n",
      "[CV] END ...metric=manhattan, n_neighbors=3, weights=uniform; total time= 3.5min\n",
      "[CV] END ..metric=manhattan, n_neighbors=3, weights=distance; total time= 3.5min\n",
      "[CV] END ...metric=manhattan, n_neighbors=3, weights=uniform; total time= 3.5min\n",
      "[CV] END ...metric=manhattan, n_neighbors=3, weights=uniform; total time= 3.6min\n",
      "[CV] END ...metric=manhattan, n_neighbors=3, weights=uniform; total time= 3.6min\n",
      "[CV] END ..metric=manhattan, n_neighbors=3, weights=distance; total time= 3.6min\n",
      "[CV] END ..metric=manhattan, n_neighbors=3, weights=distance; total time= 3.6min\n",
      "[CV] END ...metric=manhattan, n_neighbors=5, weights=uniform; total time= 3.5min\n",
      "[CV] END ..metric=manhattan, n_neighbors=3, weights=distance; total time= 3.5min\n",
      "[CV] END ...metric=manhattan, n_neighbors=5, weights=uniform; total time= 3.5min\n",
      "[CV] END ...metric=manhattan, n_neighbors=5, weights=uniform; total time= 3.5min\n",
      "[CV] END ...metric=manhattan, n_neighbors=5, weights=uniform; total time= 3.5min\n",
      "[CV] END ...metric=manhattan, n_neighbors=5, weights=uniform; total time= 3.5min\n",
      "[CV] END ..metric=manhattan, n_neighbors=3, weights=distance; total time= 3.5min\n",
      "[CV] END ..metric=manhattan, n_neighbors=5, weights=distance; total time= 3.5min\n",
      "[CV] END ..metric=manhattan, n_neighbors=5, weights=distance; total time= 3.5min\n",
      "[CV] END ..metric=manhattan, n_neighbors=5, weights=distance; total time= 3.5min\n",
      "[CV] END ..metric=manhattan, n_neighbors=5, weights=distance; total time= 3.5min\n",
      "[CV] END ..metric=manhattan, n_neighbors=5, weights=distance; total time= 3.5min\n",
      "[CV] END ...metric=manhattan, n_neighbors=7, weights=uniform; total time= 3.5min\n",
      "[CV] END ...metric=manhattan, n_neighbors=7, weights=uniform; total time= 3.5min\n",
      "[CV] END ...metric=manhattan, n_neighbors=7, weights=uniform; total time= 3.5min\n",
      "[CV] END ...metric=manhattan, n_neighbors=7, weights=uniform; total time= 3.5min\n",
      "[CV] END ...metric=manhattan, n_neighbors=7, weights=uniform; total time= 3.5min\n",
      "[CV] END ..metric=manhattan, n_neighbors=7, weights=distance; total time= 3.5min\n",
      "[CV] END ..metric=manhattan, n_neighbors=7, weights=distance; total time= 3.5min\n",
      "[CV] END ..metric=manhattan, n_neighbors=7, weights=distance; total time= 3.5min\n",
      "[CV] END ..metric=manhattan, n_neighbors=7, weights=distance; total time= 3.5min\n",
      "[CV] END ..metric=manhattan, n_neighbors=7, weights=distance; total time= 3.5min\n",
      "[CV] END ...metric=manhattan, n_neighbors=9, weights=uniform; total time= 3.5min\n",
      "[CV] END ...metric=manhattan, n_neighbors=9, weights=uniform; total time= 3.5min\n",
      "[CV] END ...metric=manhattan, n_neighbors=9, weights=uniform; total time= 3.5min\n",
      "[CV] END ...metric=manhattan, n_neighbors=9, weights=uniform; total time= 3.5min\n",
      "[CV] END ...metric=manhattan, n_neighbors=9, weights=uniform; total time= 3.5min\n",
      "[CV] END ..metric=manhattan, n_neighbors=9, weights=distance; total time= 3.5min\n",
      "[CV] END ..metric=manhattan, n_neighbors=9, weights=distance; total time= 3.5min\n",
      "[CV] END ..metric=manhattan, n_neighbors=9, weights=distance; total time= 3.5min\n",
      "[CV] END ..metric=manhattan, n_neighbors=9, weights=distance; total time= 3.5min\n",
      "[CV] END ..metric=manhattan, n_neighbors=9, weights=distance; total time= 3.5min\n",
      "[CV] END ..metric=manhattan, n_neighbors=11, weights=uniform; total time= 3.5min\n",
      "[CV] END ..metric=manhattan, n_neighbors=11, weights=uniform; total time= 3.5min\n",
      "[CV] END ..metric=manhattan, n_neighbors=11, weights=uniform; total time= 3.5min\n",
      "[CV] END ..metric=manhattan, n_neighbors=11, weights=uniform; total time= 3.5min\n",
      "[CV] END ..metric=manhattan, n_neighbors=11, weights=uniform; total time= 3.5min\n",
      "[CV] END .metric=manhattan, n_neighbors=11, weights=distance; total time= 3.5min\n",
      "[CV] END .metric=manhattan, n_neighbors=11, weights=distance; total time= 3.5min\n",
      "[CV] END .metric=manhattan, n_neighbors=11, weights=distance; total time= 3.5min\n",
      "[CV] END ...metric=minkowski, n_neighbors=3, weights=uniform; total time=  15.9s\n",
      "[CV] END ...metric=minkowski, n_neighbors=3, weights=uniform; total time=  16.5s\n",
      "[CV] END ...metric=minkowski, n_neighbors=3, weights=uniform; total time=  15.7s\n",
      "[CV] END ...metric=minkowski, n_neighbors=3, weights=uniform; total time=  16.0s\n",
      "[CV] END ..metric=minkowski, n_neighbors=3, weights=distance; total time=  16.2s\n",
      "[CV] END ...metric=minkowski, n_neighbors=3, weights=uniform; total time=  17.2s\n",
      "[CV] END ..metric=minkowski, n_neighbors=3, weights=distance; total time=  16.6s\n",
      "[CV] END ..metric=minkowski, n_neighbors=3, weights=distance; total time=  16.8s\n",
      "[CV] END ..metric=minkowski, n_neighbors=3, weights=distance; total time=  16.7s\n",
      "[CV] END ..metric=minkowski, n_neighbors=3, weights=distance; total time=  16.2s\n",
      "[CV] END ...metric=minkowski, n_neighbors=5, weights=uniform; total time=  16.1s\n",
      "[CV] END ...metric=minkowski, n_neighbors=5, weights=uniform; total time=  16.3s\n",
      "[CV] END ...metric=minkowski, n_neighbors=5, weights=uniform; total time=  14.5s\n",
      "[CV] END ...metric=minkowski, n_neighbors=5, weights=uniform; total time=  14.9s\n",
      "[CV] END ..metric=minkowski, n_neighbors=5, weights=distance; total time=  14.8s\n",
      "[CV] END ...metric=minkowski, n_neighbors=5, weights=uniform; total time=  15.1s\n",
      "[CV] END ..metric=minkowski, n_neighbors=5, weights=distance; total time=  14.6s\n",
      "[CV] END ..metric=minkowski, n_neighbors=5, weights=distance; total time=  14.9s\n",
      "[CV] END ..metric=minkowski, n_neighbors=5, weights=distance; total time=  14.4s\n",
      "[CV] END ..metric=minkowski, n_neighbors=5, weights=distance; total time=  15.4s\n",
      "[CV] END ...metric=minkowski, n_neighbors=7, weights=uniform; total time=  15.8s\n",
      "[CV] END ...metric=minkowski, n_neighbors=7, weights=uniform; total time=  16.4s\n",
      "[CV] END ...metric=minkowski, n_neighbors=7, weights=uniform; total time=  14.8s\n",
      "[CV] END ...metric=minkowski, n_neighbors=7, weights=uniform; total time=  15.0s\n",
      "[CV] END ...metric=minkowski, n_neighbors=7, weights=uniform; total time=  13.7s\n",
      "[CV] END ..metric=minkowski, n_neighbors=7, weights=distance; total time=  13.8s\n",
      "[CV] END ..metric=minkowski, n_neighbors=7, weights=distance; total time=  13.9s\n",
      "[CV] END ..metric=minkowski, n_neighbors=7, weights=distance; total time=  14.6s\n",
      "[CV] END ..metric=minkowski, n_neighbors=7, weights=distance; total time=  14.7s\n",
      "[CV] END ..metric=minkowski, n_neighbors=7, weights=distance; total time=  14.8s\n",
      "[CV] END ...metric=minkowski, n_neighbors=9, weights=uniform; total time=  14.2s\n",
      "[CV] END ...metric=minkowski, n_neighbors=9, weights=uniform; total time=  14.2s\n",
      "[CV] END ...metric=minkowski, n_neighbors=9, weights=uniform; total time=  14.2s\n",
      "[CV] END ...metric=minkowski, n_neighbors=9, weights=uniform; total time=  15.0s\n",
      "[CV] END ..metric=minkowski, n_neighbors=9, weights=distance; total time=  14.9s\n",
      "[CV] END ...metric=minkowski, n_neighbors=9, weights=uniform; total time=  15.2s\n",
      "[CV] END ..metric=minkowski, n_neighbors=9, weights=distance; total time=  14.2s\n",
      "[CV] END ..metric=minkowski, n_neighbors=9, weights=distance; total time=  13.9s\n",
      "[CV] END ..metric=minkowski, n_neighbors=9, weights=distance; total time=  13.7s\n",
      "[CV] END ..metric=minkowski, n_neighbors=9, weights=distance; total time=  14.4s\n",
      "[CV] END ..metric=minkowski, n_neighbors=11, weights=uniform; total time=  14.5s\n",
      "[CV] END ..metric=minkowski, n_neighbors=11, weights=uniform; total time=  14.7s\n",
      "[CV] END ..metric=minkowski, n_neighbors=11, weights=uniform; total time=  15.1s\n",
      "[CV] END ..metric=minkowski, n_neighbors=11, weights=uniform; total time=  14.0s\n",
      "[CV] END ..metric=minkowski, n_neighbors=11, weights=uniform; total time=  14.3s\n",
      "[CV] END .metric=minkowski, n_neighbors=11, weights=distance; total time=  13.7s\n",
      "[CV] END .metric=minkowski, n_neighbors=11, weights=distance; total time=  14.1s\n",
      "[CV] END .metric=minkowski, n_neighbors=11, weights=distance; total time=  14.2s\n",
      "[CV] END .metric=minkowski, n_neighbors=11, weights=distance; total time=  10.9s\n",
      "[CV] END .metric=minkowski, n_neighbors=11, weights=distance; total time=  11.0s\n",
      "[CV] END .metric=manhattan, n_neighbors=11, weights=distance; total time= 3.3min\n",
      "[CV] END .metric=manhattan, n_neighbors=11, weights=distance; total time= 3.3min\n",
      "Best Model Parameters from Grid Search: {'metric': 'euclidean', 'n_neighbors': 11, 'weights': 'distance'}\n",
      "BoW K-Nearest Neighbors Model Performance after Grid Search:\n",
      "Accuracy: 0.6377\n",
      "Precision: 0.6460396039603961\n",
      "Recall: 0.621551895217305\n",
      "F1 Score: 0.6335592191766967\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Fit and transform the preprocessed reviews\n",
    "vectorizer = CountVectorizer()\n",
    "bow_features = vectorizer.fit_transform(df['preprocessed_review'])\n",
    "\n",
    "# Get the sentiment labels from the preprocessed DataFrame\n",
    "labels = df['sentiment']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(bow_features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the K-Nearest Neighbors model\n",
    "model = KNeighborsClassifier()\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'n_neighbors': [3, 5, 7, 9, 11],  # Number of neighbors to use\n",
    "    'weights': ['uniform', 'distance'],  # Weight function used in prediction\n",
    "    'metric': ['euclidean', 'manhattan', 'minkowski']  # Distance metric\n",
    "}\n",
    "\n",
    "# Set up Grid Search with cross-validation\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='f1', n_jobs=-1, verbose=2)\n",
    "\n",
    "# Fit Grid Search\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model from Grid Search\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the testing set with the best model\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"Best Model Parameters from Grid Search:\", grid_search.best_params_)\n",
    "print(\"BoW K-Nearest Neighbors Model Performance after Grid Search:\")\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes: Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes: Bayes Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression: Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression: Bayes Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV] END ......C=0.4369339947510315, penalty=l2, solver=saga; total time=  56.0s\n",
      "[CV] END ......C=0.4369339947510315, penalty=l2, solver=saga; total time=  58.7s\n",
      "[CV] END ......C=0.4369339947510315, penalty=l2, solver=saga; total time= 1.1min\n",
      "[CV] END ......C=0.4369339947510315, penalty=l2, solver=saga; total time= 1.2min\n",
      "[CV] END ......C=0.4369339947510315, penalty=l2, solver=saga; total time= 1.3min\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV] END .C=22.364202820542708, penalty=l2, solver=liblinear; total time=   5.6s\n",
      "[CV] END .C=22.364202820542708, penalty=l2, solver=liblinear; total time=   6.1s\n",
      "[CV] END .C=22.364202820542708, penalty=l2, solver=liblinear; total time=   5.8s\n",
      "[CV] END .C=22.364202820542708, penalty=l2, solver=liblinear; total time=   5.9s\n",
      "[CV] END .C=22.364202820542708, penalty=l2, solver=liblinear; total time=   6.7s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV] END .C=0.6016307829589929, penalty=l2, solver=liblinear; total time=   3.2s\n",
      "[CV] END .C=0.6016307829589929, penalty=l2, solver=liblinear; total time=   3.9s\n",
      "[CV] END .C=0.6016307829589929, penalty=l2, solver=liblinear; total time=   4.2s\n",
      "[CV] END .C=0.6016307829589929, penalty=l2, solver=liblinear; total time=   4.4s\n",
      "[CV] END .C=0.6016307829589929, penalty=l2, solver=liblinear; total time=   4.4s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fpolygon/.pyenv/versions/3.12.4/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ......C=17.76576664980768, penalty=l1, solver=saga; total time=397.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fpolygon/.pyenv/versions/3.12.4/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ......C=17.76576664980768, penalty=l1, solver=saga; total time=453.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fpolygon/.pyenv/versions/3.12.4/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ......C=17.76576664980768, penalty=l1, solver=saga; total time=454.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fpolygon/.pyenv/versions/3.12.4/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ......C=17.76576664980768, penalty=l1, solver=saga; total time=455.3min\n",
      "[CV] END ......C=17.76576664980768, penalty=l1, solver=saga; total time=458.6min\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fpolygon/.pyenv/versions/3.12.4/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/fpolygon/.pyenv/versions/3.12.4/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END .....C=15.783879853890564, penalty=l1, solver=saga; total time=379.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fpolygon/.pyenv/versions/3.12.4/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END .....C=15.783879853890564, penalty=l1, solver=saga; total time=435.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fpolygon/.pyenv/versions/3.12.4/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END .....C=15.783879853890564, penalty=l1, solver=saga; total time=436.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fpolygon/.pyenv/versions/3.12.4/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END .....C=15.783879853890564, penalty=l1, solver=saga; total time=437.2min\n",
      "[CV] END .....C=15.783879853890564, penalty=l1, solver=saga; total time=440.1min\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fpolygon/.pyenv/versions/3.12.4/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ..C=8.632012725909878, penalty=l2, solver=liblinear; total time=   5.5s\n",
      "[CV] END ..C=8.632012725909878, penalty=l2, solver=liblinear; total time=   5.7s\n",
      "[CV] END ..C=8.632012725909878, penalty=l2, solver=liblinear; total time=   6.1s\n",
      "[CV] END ..C=8.632012725909878, penalty=l2, solver=liblinear; total time=   6.3s\n",
      "[CV] END ..C=8.632012725909878, penalty=l2, solver=liblinear; total time=   5.9s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV] END .C=2.9397976202716882, penalty=l2, solver=liblinear; total time=   4.7s\n",
      "[CV] END .C=2.9397976202716882, penalty=l2, solver=liblinear; total time=   5.5s\n",
      "[CV] END .C=2.9397976202716882, penalty=l2, solver=liblinear; total time=   5.7s\n",
      "[CV] END .C=2.9397976202716882, penalty=l2, solver=liblinear; total time=   5.8s\n",
      "[CV] END .C=2.9397976202716882, penalty=l2, solver=liblinear; total time=   6.1s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV] END ..C=1.491462267977867, penalty=l2, solver=liblinear; total time=   4.1s\n",
      "[CV] END ..C=1.491462267977867, penalty=l2, solver=liblinear; total time=   5.1s\n",
      "[CV] END ..C=1.491462267977867, penalty=l2, solver=liblinear; total time=   5.5s\n",
      "[CV] END ..C=1.491462267977867, penalty=l2, solver=liblinear; total time=   5.8s\n",
      "[CV] END ..C=1.491462267977867, penalty=l2, solver=liblinear; total time=   5.5s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fpolygon/.pyenv/versions/3.12.4/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END .......C=66.36085077612337, penalty=l2, solver=saga; total time= 1.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fpolygon/.pyenv/versions/3.12.4/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/fpolygon/.pyenv/versions/3.12.4/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END .......C=66.36085077612337, penalty=l2, solver=saga; total time= 1.4min\n",
      "[CV] END .......C=66.36085077612337, penalty=l2, solver=saga; total time= 1.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fpolygon/.pyenv/versions/3.12.4/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END .......C=66.36085077612337, penalty=l2, solver=saga; total time= 1.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fpolygon/.pyenv/versions/3.12.4/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END .......C=66.36085077612337, penalty=l2, solver=saga; total time= 1.4min\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV] END ....C=0.010340016434251914, penalty=l2, solver=saga; total time=   4.9s\n",
      "[CV] END ....C=0.010340016434251914, penalty=l2, solver=saga; total time=   6.6s\n",
      "[CV] END ....C=0.010340016434251914, penalty=l2, solver=saga; total time=   6.7s\n",
      "[CV] END ....C=0.010340016434251914, penalty=l2, solver=saga; total time=   6.7s\n",
      "[CV] END ....C=0.010340016434251914, penalty=l2, solver=saga; total time=   6.8s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV] END C=0.016694592785323554, penalty=l2, solver=liblinear; total time=   1.3s\n",
      "[CV] END C=0.016694592785323554, penalty=l2, solver=liblinear; total time=   1.3s\n",
      "[CV] END C=0.016694592785323554, penalty=l2, solver=liblinear; total time=   1.4s\n",
      "[CV] END C=0.016694592785323554, penalty=l2, solver=liblinear; total time=   1.4s\n",
      "[CV] END C=0.016694592785323554, penalty=l2, solver=liblinear; total time=   1.4s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV] END C=0.08605167187834277, penalty=l1, solver=liblinear; total time=   0.4s\n",
      "[CV] END C=0.08605167187834277, penalty=l1, solver=liblinear; total time=   0.5s\n",
      "[CV] END C=0.08605167187834277, penalty=l1, solver=liblinear; total time=   0.5s\n",
      "[CV] END C=0.08605167187834277, penalty=l1, solver=liblinear; total time=   0.5s\n",
      "[CV] END C=0.08605167187834277, penalty=l1, solver=liblinear; total time=   0.5s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV] END C=0.08343348114851637, penalty=l2, solver=liblinear; total time=   2.1s\n",
      "[CV] END C=0.08343348114851637, penalty=l2, solver=liblinear; total time=   2.1s\n",
      "[CV] END C=0.08343348114851637, penalty=l2, solver=liblinear; total time=   2.2s\n",
      "[CV] END C=0.08343348114851637, penalty=l2, solver=liblinear; total time=   2.2s\n",
      "[CV] END C=0.08343348114851637, penalty=l2, solver=liblinear; total time=   2.2s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV] END C=0.08274202019806566, penalty=l2, solver=liblinear; total time=   2.1s\n",
      "[CV] END C=0.08274202019806566, penalty=l2, solver=liblinear; total time=   2.1s\n",
      "[CV] END C=0.08274202019806566, penalty=l2, solver=liblinear; total time=   2.2s\n",
      "[CV] END C=0.08274202019806566, penalty=l2, solver=liblinear; total time=   2.2s\n",
      "[CV] END C=0.08274202019806566, penalty=l2, solver=liblinear; total time=   2.2s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV] END ..C=99.78136366417831, penalty=l1, solver=liblinear; total time=   1.3s\n",
      "[CV] END ..C=99.78136366417831, penalty=l1, solver=liblinear; total time=   1.3s\n",
      "[CV] END ..C=99.78136366417831, penalty=l1, solver=liblinear; total time=   1.3s\n",
      "[CV] END ..C=99.78136366417831, penalty=l1, solver=liblinear; total time=   1.3s\n",
      "[CV] END ..C=99.78136366417831, penalty=l1, solver=liblinear; total time=   1.5s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV] END .....C=0.01002437847517146, penalty=l1, solver=saga; total time=   6.2s\n",
      "[CV] END .....C=0.01002437847517146, penalty=l1, solver=saga; total time=   8.6s\n",
      "[CV] END .....C=0.01002437847517146, penalty=l1, solver=saga; total time=   9.1s\n",
      "[CV] END .....C=0.01002437847517146, penalty=l1, solver=saga; total time=   9.2s\n",
      "[CV] END .....C=0.01002437847517146, penalty=l1, solver=saga; total time=   9.3s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV] END ...............C=0.01, penalty=l1, solver=liblinear; total time=   0.4s\n",
      "[CV] END ...............C=0.01, penalty=l1, solver=liblinear; total time=   0.4s\n",
      "[CV] END ...............C=0.01, penalty=l1, solver=liblinear; total time=   0.5s\n",
      "[CV] END ...............C=0.01, penalty=l1, solver=liblinear; total time=   0.4s\n",
      "[CV] END ...............C=0.01, penalty=l1, solver=liblinear; total time=   0.5s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fpolygon/.pyenv/versions/3.12.4/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ......C=0.5105493247914744, penalty=l1, solver=saga; total time=25.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fpolygon/.pyenv/versions/3.12.4/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ......C=0.5105493247914744, penalty=l1, solver=saga; total time=28.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fpolygon/.pyenv/versions/3.12.4/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ......C=0.5105493247914744, penalty=l1, solver=saga; total time=29.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fpolygon/.pyenv/versions/3.12.4/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ......C=0.5105493247914744, penalty=l1, solver=saga; total time=29.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fpolygon/.pyenv/versions/3.12.4/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ......C=0.5105493247914744, penalty=l1, solver=saga; total time=29.2min\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV] END ..C=2.242158263729326, penalty=l1, solver=liblinear; total time=   1.1s\n",
      "[CV] END ..C=2.242158263729326, penalty=l1, solver=liblinear; total time=   1.2s\n",
      "[CV] END ..C=2.242158263729326, penalty=l1, solver=liblinear; total time=   1.1s\n",
      "[CV] END ..C=2.242158263729326, penalty=l1, solver=liblinear; total time=   1.2s\n",
      "[CV] END ..C=2.242158263729326, penalty=l1, solver=liblinear; total time=   1.2s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV] END .....C=0.21004231826213174, penalty=l1, solver=saga; total time= 8.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fpolygon/.pyenv/versions/3.12.4/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END .....C=0.21004231826213174, penalty=l1, solver=saga; total time=10.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fpolygon/.pyenv/versions/3.12.4/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END .....C=0.21004231826213174, penalty=l1, solver=saga; total time=11.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fpolygon/.pyenv/versions/3.12.4/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END .....C=0.21004231826213174, penalty=l1, solver=saga; total time=11.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fpolygon/.pyenv/versions/3.12.4/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END .....C=0.21004231826213174, penalty=l1, solver=saga; total time=11.4min\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV] END .....C=0.04225739818165358, penalty=l2, solver=saga; total time=  12.3s\n",
      "[CV] END .....C=0.04225739818165358, penalty=l2, solver=saga; total time=  17.0s\n",
      "[CV] END .....C=0.04225739818165358, penalty=l2, solver=saga; total time=  17.1s\n",
      "[CV] END .....C=0.04225739818165358, penalty=l2, solver=saga; total time=  17.2s\n",
      "[CV] END .....C=0.04225739818165358, penalty=l2, solver=saga; total time=  17.4s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV] END ..C=99.80972925829565, penalty=l2, solver=liblinear; total time=   4.7s\n",
      "[CV] END ..C=99.80972925829565, penalty=l2, solver=liblinear; total time=   5.8s\n",
      "[CV] END ..C=99.80972925829565, penalty=l2, solver=liblinear; total time=   6.2s\n",
      "[CV] END ..C=99.80972925829565, penalty=l2, solver=liblinear; total time=   6.1s\n",
      "[CV] END ..C=99.80972925829565, penalty=l2, solver=liblinear; total time=   6.2s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV] END C=0.29969226641286445, penalty=l1, solver=liblinear; total time=   0.5s\n",
      "[CV] END C=0.29969226641286445, penalty=l1, solver=liblinear; total time=   0.5s\n",
      "[CV] END C=0.29969226641286445, penalty=l1, solver=liblinear; total time=   0.5s\n",
      "[CV] END C=0.29969226641286445, penalty=l1, solver=liblinear; total time=   0.5s\n",
      "[CV] END C=0.29969226641286445, penalty=l1, solver=liblinear; total time=   0.5s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV] END C=0.18062334925292997, penalty=l2, solver=liblinear; total time=   2.5s\n",
      "[CV] END C=0.18062334925292997, penalty=l2, solver=liblinear; total time=   2.5s\n",
      "[CV] END C=0.18062334925292997, penalty=l2, solver=liblinear; total time=   2.5s\n",
      "[CV] END C=0.18062334925292997, penalty=l2, solver=liblinear; total time=   2.6s\n",
      "[CV] END C=0.18062334925292997, penalty=l2, solver=liblinear; total time=   2.8s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fpolygon/.pyenv/versions/3.12.4/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ......C=5.837075097063464, penalty=l1, solver=saga; total time=224.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fpolygon/.pyenv/versions/3.12.4/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ......C=5.837075097063464, penalty=l1, solver=saga; total time=266.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fpolygon/.pyenv/versions/3.12.4/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ......C=5.837075097063464, penalty=l1, solver=saga; total time=267.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fpolygon/.pyenv/versions/3.12.4/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ......C=5.837075097063464, penalty=l1, solver=saga; total time=268.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fpolygon/.pyenv/versions/3.12.4/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ......C=5.837075097063464, penalty=l1, solver=saga; total time=269.4min\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fpolygon/.pyenv/versions/3.12.4/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END .......C=99.6475897081206, penalty=l1, solver=saga; total time=627.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fpolygon/.pyenv/versions/3.12.4/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END .......C=99.6475897081206, penalty=l1, solver=saga; total time=678.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fpolygon/.pyenv/versions/3.12.4/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END .......C=99.6475897081206, penalty=l1, solver=saga; total time=679.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fpolygon/.pyenv/versions/3.12.4/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END .......C=99.6475897081206, penalty=l1, solver=saga; total time=684.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fpolygon/.pyenv/versions/3.12.4/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END .......C=99.6475897081206, penalty=l1, solver=saga; total time=685.5min\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fpolygon/.pyenv/versions/3.12.4/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END .......C=6.332280073417471, penalty=l2, solver=saga; total time= 1.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fpolygon/.pyenv/versions/3.12.4/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END .......C=6.332280073417471, penalty=l2, solver=saga; total time= 1.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fpolygon/.pyenv/versions/3.12.4/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END .......C=6.332280073417471, penalty=l2, solver=saga; total time= 1.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fpolygon/.pyenv/versions/3.12.4/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END .......C=6.332280073417471, penalty=l2, solver=saga; total time= 1.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fpolygon/.pyenv/versions/3.12.4/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END .......C=6.332280073417471, penalty=l2, solver=saga; total time= 1.3min\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV] END .....C=0.11441704003548388, penalty=l2, solver=saga; total time=  22.3s\n",
      "[CV] END .....C=0.11441704003548388, penalty=l2, solver=saga; total time=  28.7s\n",
      "[CV] END .....C=0.11441704003548388, penalty=l2, solver=saga; total time=  28.9s\n",
      "[CV] END .....C=0.11441704003548388, penalty=l2, solver=saga; total time=  29.7s\n",
      "[CV] END .....C=0.11441704003548388, penalty=l2, solver=saga; total time=  31.1s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV] END C=0.010073871059233849, penalty=l2, solver=liblinear; total time=   1.2s\n",
      "[CV] END C=0.010073871059233849, penalty=l2, solver=liblinear; total time=   1.2s\n",
      "[CV] END C=0.010073871059233849, penalty=l2, solver=liblinear; total time=   1.1s\n",
      "[CV] END C=0.010073871059233849, penalty=l2, solver=liblinear; total time=   1.2s\n",
      "[CV] END C=0.010073871059233849, penalty=l2, solver=liblinear; total time=   1.2s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV] END .C=11.104161617613466, penalty=l1, solver=liblinear; total time=   1.3s\n",
      "[CV] END .C=11.104161617613466, penalty=l1, solver=liblinear; total time=   1.4s\n",
      "[CV] END .C=11.104161617613466, penalty=l1, solver=liblinear; total time=   1.4s\n",
      "[CV] END .C=11.104161617613466, penalty=l1, solver=liblinear; total time=   1.4s\n",
      "[CV] END .C=11.104161617613466, penalty=l1, solver=liblinear; total time=   1.2s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV] END C=0.046900413283947125, penalty=l2, solver=liblinear; total time=   1.6s\n",
      "[CV] END C=0.046900413283947125, penalty=l2, solver=liblinear; total time=   1.6s\n",
      "[CV] END C=0.046900413283947125, penalty=l2, solver=liblinear; total time=   1.6s\n",
      "[CV] END C=0.046900413283947125, penalty=l2, solver=liblinear; total time=   1.6s\n",
      "[CV] END C=0.046900413283947125, penalty=l2, solver=liblinear; total time=   1.8s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV] END .....C=0.07506986057616404, penalty=l2, solver=saga; total time=  16.7s\n",
      "[CV] END .....C=0.07506986057616404, penalty=l2, solver=saga; total time=  22.7s\n",
      "[CV] END .....C=0.07506986057616404, penalty=l2, solver=saga; total time=  23.0s\n",
      "[CV] END .....C=0.07506986057616404, penalty=l2, solver=saga; total time=  23.0s\n",
      "[CV] END .....C=0.07506986057616404, penalty=l2, solver=saga; total time=  23.7s\n",
      "Best Model Parameters from Bayesian Search: OrderedDict({'C': 0.08274202019806566, 'penalty': 'l2', 'solver': 'liblinear'})\n",
      "BoW Logistic Regression Model Performance after Bayesian Search:\n",
      "Accuracy: 0.8932\n",
      "Precision: 0.8868108318721994\n",
      "Recall: 0.9033538400476285\n",
      "F1 Score: 0.8950058985450255\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical\n",
    "\n",
    "# Get the sentiment labels from the preprocessed DataFrame\n",
    "labels = df['sentiment']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(bow_features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the Logistic Regression model\n",
    "model = LogisticRegression(max_iter=2000)\n",
    "\n",
    "# Define the hyperparameter space\n",
    "param_space = {\n",
    "    'C': Real(0.01, 100, prior='log-uniform'),  # Regularization strength (log-uniform distribution)\n",
    "    'penalty': Categorical(['l1', 'l2']),  # Regularization types\n",
    "    'solver': Categorical(['liblinear', 'saga'])  # Solvers that support l1 and l2 penalties\n",
    "}\n",
    "\n",
    "# Set up Bayesian Search with cross-validation\n",
    "bayes_search = BayesSearchCV(\n",
    "    estimator=model,\n",
    "    search_spaces=param_space,\n",
    "    n_iter=32,  # Number of parameter settings that are sampled\n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    scoring='f1',\n",
    "    n_jobs=-1,  # Use all available cores\n",
    "    verbose=2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit Bayesian Search\n",
    "bayes_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model from Bayesian Search\n",
    "best_model = bayes_search.best_estimator_\n",
    "\n",
    "# Make predictions on the testing set with the best model\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"Best Model Parameters from Bayesian Search:\", bayes_search.best_params_)\n",
    "print(\"BoW Logistic Regression Model Performance after Bayesian Search:\")\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN: Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN: Bayes Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV] END .metric=manhattan, n_neighbors=12, weights=distance; total time= 3.0min\n",
      "[CV] END .metric=manhattan, n_neighbors=12, weights=distance; total time= 3.0min\n",
      "[CV] END .metric=manhattan, n_neighbors=12, weights=distance; total time= 3.0min\n",
      "[CV] END .metric=manhattan, n_neighbors=12, weights=distance; total time= 3.0min\n",
      "[CV] END .metric=manhattan, n_neighbors=12, weights=distance; total time= 3.0min\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV] END ..metric=minkowski, n_neighbors=14, weights=uniform; total time=  15.6s\n",
      "[CV] END ..metric=minkowski, n_neighbors=14, weights=uniform; total time=  15.6s\n",
      "[CV] END ..metric=minkowski, n_neighbors=14, weights=uniform; total time=  15.6s\n",
      "[CV] END ..metric=minkowski, n_neighbors=14, weights=uniform; total time=  15.7s\n",
      "[CV] END ..metric=minkowski, n_neighbors=14, weights=uniform; total time=  15.7s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV] END ..metric=manhattan, n_neighbors=14, weights=uniform; total time= 3.0min\n",
      "[CV] END ..metric=manhattan, n_neighbors=14, weights=uniform; total time= 3.1min\n",
      "[CV] END ..metric=manhattan, n_neighbors=14, weights=uniform; total time= 3.1min\n",
      "[CV] END ..metric=manhattan, n_neighbors=14, weights=uniform; total time= 3.1min\n",
      "[CV] END ..metric=manhattan, n_neighbors=14, weights=uniform; total time= 3.1min\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV] END ..metric=minkowski, n_neighbors=5, weights=distance; total time=  15.8s\n",
      "[CV] END ..metric=minkowski, n_neighbors=5, weights=distance; total time=  15.9s\n",
      "[CV] END ..metric=minkowski, n_neighbors=5, weights=distance; total time=  15.9s\n",
      "[CV] END ..metric=minkowski, n_neighbors=5, weights=distance; total time=  15.9s\n",
      "[CV] END ..metric=minkowski, n_neighbors=5, weights=distance; total time=  16.0s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV] END ..metric=minkowski, n_neighbors=8, weights=distance; total time=  16.0s\n",
      "[CV] END ..metric=minkowski, n_neighbors=8, weights=distance; total time=  16.1s\n",
      "[CV] END ..metric=minkowski, n_neighbors=8, weights=distance; total time=  16.1s\n",
      "[CV] END ..metric=minkowski, n_neighbors=8, weights=distance; total time=  16.1s\n",
      "[CV] END ..metric=minkowski, n_neighbors=8, weights=distance; total time=  16.2s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV] END ..metric=minkowski, n_neighbors=14, weights=uniform; total time=  17.2s\n",
      "[CV] END ..metric=minkowski, n_neighbors=14, weights=uniform; total time=  17.4s\n",
      "[CV] END ..metric=minkowski, n_neighbors=14, weights=uniform; total time=  17.4s\n",
      "[CV] END ..metric=minkowski, n_neighbors=14, weights=uniform; total time=  17.4s\n",
      "[CV] END ..metric=minkowski, n_neighbors=14, weights=uniform; total time=  17.4s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV] END ..metric=manhattan, n_neighbors=12, weights=uniform; total time= 3.0min\n",
      "[CV] END ..metric=manhattan, n_neighbors=12, weights=uniform; total time= 3.0min\n",
      "[CV] END ..metric=manhattan, n_neighbors=12, weights=uniform; total time= 3.0min\n",
      "[CV] END ..metric=manhattan, n_neighbors=12, weights=uniform; total time= 3.0min\n",
      "[CV] END ..metric=manhattan, n_neighbors=12, weights=uniform; total time= 3.0min\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV] END ..metric=manhattan, n_neighbors=14, weights=uniform; total time= 3.0min\n",
      "[CV] END ..metric=manhattan, n_neighbors=14, weights=uniform; total time= 3.0min\n",
      "[CV] END ..metric=manhattan, n_neighbors=14, weights=uniform; total time= 3.0min\n",
      "[CV] END ..metric=manhattan, n_neighbors=14, weights=uniform; total time= 3.0min\n",
      "[CV] END ..metric=manhattan, n_neighbors=14, weights=uniform; total time= 3.0min\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV] END .metric=minkowski, n_neighbors=11, weights=distance; total time=  15.8s\n",
      "[CV] END .metric=minkowski, n_neighbors=11, weights=distance; total time=  15.9s\n",
      "[CV] END .metric=minkowski, n_neighbors=11, weights=distance; total time=  15.9s\n",
      "[CV] END .metric=minkowski, n_neighbors=11, weights=distance; total time=  15.9s\n",
      "[CV] END .metric=minkowski, n_neighbors=11, weights=distance; total time=  15.9s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV] END .metric=euclidean, n_neighbors=13, weights=distance; total time=  15.6s\n",
      "[CV] END .metric=euclidean, n_neighbors=13, weights=distance; total time=  15.7s\n",
      "[CV] END .metric=euclidean, n_neighbors=13, weights=distance; total time=  15.6s\n",
      "[CV] END .metric=euclidean, n_neighbors=13, weights=distance; total time=  15.7s\n",
      "[CV] END .metric=euclidean, n_neighbors=13, weights=distance; total time=  15.7s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV] END .metric=minkowski, n_neighbors=15, weights=distance; total time=  15.2s\n",
      "[CV] END .metric=minkowski, n_neighbors=15, weights=distance; total time=  15.3s\n",
      "[CV] END .metric=minkowski, n_neighbors=15, weights=distance; total time=  15.3s\n",
      "[CV] END .metric=minkowski, n_neighbors=15, weights=distance; total time=  15.4s\n",
      "[CV] END .metric=minkowski, n_neighbors=15, weights=distance; total time=  15.4s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV] END ...metric=minkowski, n_neighbors=3, weights=uniform; total time=  16.9s\n",
      "[CV] END ...metric=minkowski, n_neighbors=3, weights=uniform; total time=  16.9s\n",
      "[CV] END ...metric=minkowski, n_neighbors=3, weights=uniform; total time=  16.9s\n",
      "[CV] END ...metric=minkowski, n_neighbors=3, weights=uniform; total time=  16.9s\n",
      "[CV] END ...metric=minkowski, n_neighbors=3, weights=uniform; total time=  17.0s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV] END ...metric=euclidean, n_neighbors=8, weights=uniform; total time=  15.4s\n",
      "[CV] END ...metric=euclidean, n_neighbors=8, weights=uniform; total time=  15.5s\n",
      "[CV] END ...metric=euclidean, n_neighbors=8, weights=uniform; total time=  15.5s\n",
      "[CV] END ...metric=euclidean, n_neighbors=8, weights=uniform; total time=  15.5s\n",
      "[CV] END ...metric=euclidean, n_neighbors=8, weights=uniform; total time=  15.5s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV] END .metric=euclidean, n_neighbors=15, weights=distance; total time=  15.2s\n",
      "[CV] END .metric=euclidean, n_neighbors=15, weights=distance; total time=  15.2s\n",
      "[CV] END .metric=euclidean, n_neighbors=15, weights=distance; total time=  15.3s\n",
      "[CV] END .metric=euclidean, n_neighbors=15, weights=distance; total time=  15.3s\n",
      "[CV] END .metric=euclidean, n_neighbors=15, weights=distance; total time=  15.3s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV] END ..metric=manhattan, n_neighbors=3, weights=distance; total time= 3.0min\n",
      "[CV] END ..metric=manhattan, n_neighbors=3, weights=distance; total time= 3.0min\n",
      "[CV] END ..metric=manhattan, n_neighbors=3, weights=distance; total time= 3.0min\n",
      "[CV] END ..metric=manhattan, n_neighbors=3, weights=distance; total time= 3.0min\n",
      "[CV] END ..metric=manhattan, n_neighbors=3, weights=distance; total time= 3.0min\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV] END ..metric=euclidean, n_neighbors=3, weights=distance; total time=  18.6s\n",
      "[CV] END ..metric=euclidean, n_neighbors=3, weights=distance; total time=  18.6s\n",
      "[CV] END ..metric=euclidean, n_neighbors=3, weights=distance; total time=  18.6s\n",
      "[CV] END ..metric=euclidean, n_neighbors=3, weights=distance; total time=  18.7s\n",
      "[CV] END ..metric=euclidean, n_neighbors=3, weights=distance; total time=  18.7s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV] END ..metric=euclidean, n_neighbors=15, weights=uniform; total time=  15.6s\n",
      "[CV] END ..metric=euclidean, n_neighbors=15, weights=uniform; total time=  15.8s\n",
      "[CV] END ..metric=euclidean, n_neighbors=15, weights=uniform; total time=  15.9s\n",
      "[CV] END ..metric=euclidean, n_neighbors=15, weights=uniform; total time=  15.8s\n",
      "[CV] END ..metric=euclidean, n_neighbors=15, weights=uniform; total time=  15.9s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV] END ...metric=euclidean, n_neighbors=3, weights=uniform; total time=  17.0s\n",
      "[CV] END ...metric=euclidean, n_neighbors=3, weights=uniform; total time=  17.1s\n",
      "[CV] END ...metric=euclidean, n_neighbors=3, weights=uniform; total time=  17.1s\n",
      "[CV] END ...metric=euclidean, n_neighbors=3, weights=uniform; total time=  17.2s\n",
      "[CV] END ...metric=euclidean, n_neighbors=3, weights=uniform; total time=  17.1s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV] END ..metric=euclidean, n_neighbors=9, weights=distance; total time=  16.0s\n",
      "[CV] END ..metric=euclidean, n_neighbors=9, weights=distance; total time=  16.0s\n",
      "[CV] END ..metric=euclidean, n_neighbors=9, weights=distance; total time=  16.1s\n",
      "[CV] END ..metric=euclidean, n_neighbors=9, weights=distance; total time=  16.1s\n",
      "[CV] END ..metric=euclidean, n_neighbors=9, weights=distance; total time=  16.2s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV] END ...metric=manhattan, n_neighbors=3, weights=uniform; total time= 3.0min\n",
      "[CV] END ...metric=manhattan, n_neighbors=3, weights=uniform; total time= 3.0min\n",
      "[CV] END ...metric=manhattan, n_neighbors=3, weights=uniform; total time= 3.1min\n",
      "[CV] END ...metric=manhattan, n_neighbors=3, weights=uniform; total time= 3.1min\n",
      "[CV] END ...metric=manhattan, n_neighbors=3, weights=uniform; total time= 3.1min\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV] END ...metric=minkowski, n_neighbors=5, weights=uniform; total time=  16.2s\n",
      "[CV] END ...metric=minkowski, n_neighbors=5, weights=uniform; total time=  16.2s\n",
      "[CV] END ...metric=minkowski, n_neighbors=5, weights=uniform; total time=  16.4s\n",
      "[CV] END ...metric=minkowski, n_neighbors=5, weights=uniform; total time=  16.3s\n",
      "[CV] END ...metric=minkowski, n_neighbors=5, weights=uniform; total time=  16.4s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV] END .metric=manhattan, n_neighbors=15, weights=distance; total time= 3.0min\n",
      "[CV] END .metric=manhattan, n_neighbors=15, weights=distance; total time= 3.0min\n",
      "[CV] END .metric=manhattan, n_neighbors=15, weights=distance; total time= 3.0min\n",
      "[CV] END .metric=manhattan, n_neighbors=15, weights=distance; total time= 3.0min\n",
      "[CV] END .metric=manhattan, n_neighbors=15, weights=distance; total time= 3.0min\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV] END .metric=minkowski, n_neighbors=14, weights=distance; total time=  16.1s\n",
      "[CV] END .metric=minkowski, n_neighbors=14, weights=distance; total time=  16.1s\n",
      "[CV] END .metric=minkowski, n_neighbors=14, weights=distance; total time=  16.3s\n",
      "[CV] END .metric=minkowski, n_neighbors=14, weights=distance; total time=  16.2s\n",
      "[CV] END .metric=minkowski, n_neighbors=14, weights=distance; total time=  16.2s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV] END ..metric=minkowski, n_neighbors=15, weights=uniform; total time=  15.2s\n",
      "[CV] END ..metric=minkowski, n_neighbors=15, weights=uniform; total time=  15.4s\n",
      "[CV] END ..metric=minkowski, n_neighbors=15, weights=uniform; total time=  15.4s\n",
      "[CV] END ..metric=minkowski, n_neighbors=15, weights=uniform; total time=  15.4s\n",
      "[CV] END ..metric=minkowski, n_neighbors=15, weights=uniform; total time=  15.5s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV] END ..metric=minkowski, n_neighbors=11, weights=uniform; total time=  15.4s\n",
      "[CV] END ..metric=minkowski, n_neighbors=11, weights=uniform; total time=  15.6s\n",
      "[CV] END ..metric=minkowski, n_neighbors=11, weights=uniform; total time=  15.7s\n",
      "[CV] END ..metric=minkowski, n_neighbors=11, weights=uniform; total time=  15.8s\n",
      "[CV] END ..metric=minkowski, n_neighbors=11, weights=uniform; total time=  15.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fpolygon/.pyenv/versions/3.12.4/lib/python3.12/site-packages/skopt/optimizer/optimizer.py:517: UserWarning: The objective has been evaluated at point ['minkowski', 15, 'distance'] before, using random point ['manhattan', 4, 'distance']\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV] END ..metric=manhattan, n_neighbors=4, weights=distance; total time= 3.0min\n",
      "[CV] END ..metric=manhattan, n_neighbors=4, weights=distance; total time= 3.0min\n",
      "[CV] END ..metric=manhattan, n_neighbors=4, weights=distance; total time= 3.0min\n",
      "[CV] END ..metric=manhattan, n_neighbors=4, weights=distance; total time= 3.0min\n",
      "[CV] END ..metric=manhattan, n_neighbors=4, weights=distance; total time= 3.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fpolygon/.pyenv/versions/3.12.4/lib/python3.12/site-packages/skopt/optimizer/optimizer.py:517: UserWarning: The objective has been evaluated at point ['minkowski', 15, 'distance'] before, using random point ['minkowski', 11, 'distance']\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV] END .metric=minkowski, n_neighbors=11, weights=distance; total time=  17.0s\n",
      "[CV] END .metric=minkowski, n_neighbors=11, weights=distance; total time=  17.1s\n",
      "[CV] END .metric=minkowski, n_neighbors=11, weights=distance; total time=  17.1s\n",
      "[CV] END .metric=minkowski, n_neighbors=11, weights=distance; total time=  17.1s\n",
      "[CV] END .metric=minkowski, n_neighbors=11, weights=distance; total time=  17.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fpolygon/.pyenv/versions/3.12.4/lib/python3.12/site-packages/skopt/optimizer/optimizer.py:517: UserWarning: The objective has been evaluated at point ['minkowski', 15, 'distance'] before, using random point ['manhattan', 5, 'distance']\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV] END ..metric=manhattan, n_neighbors=5, weights=distance; total time= 3.0min\n",
      "[CV] END ..metric=manhattan, n_neighbors=5, weights=distance; total time= 3.0min\n",
      "[CV] END ..metric=manhattan, n_neighbors=5, weights=distance; total time= 3.0min\n",
      "[CV] END ..metric=manhattan, n_neighbors=5, weights=distance; total time= 3.0min\n",
      "[CV] END ..metric=manhattan, n_neighbors=5, weights=distance; total time= 3.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fpolygon/.pyenv/versions/3.12.4/lib/python3.12/site-packages/skopt/optimizer/optimizer.py:517: UserWarning: The objective has been evaluated at point ['minkowski', 15, 'distance'] before, using random point ['minkowski', 5, 'distance']\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV] END ..metric=minkowski, n_neighbors=5, weights=distance; total time=  16.8s\n",
      "[CV] END ..metric=minkowski, n_neighbors=5, weights=distance; total time=  16.9s\n",
      "[CV] END ..metric=minkowski, n_neighbors=5, weights=distance; total time=  16.9s\n",
      "[CV] END ..metric=minkowski, n_neighbors=5, weights=distance; total time=  16.9s\n",
      "[CV] END ..metric=minkowski, n_neighbors=5, weights=distance; total time=  16.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fpolygon/.pyenv/versions/3.12.4/lib/python3.12/site-packages/skopt/optimizer/optimizer.py:517: UserWarning: The objective has been evaluated at point ['minkowski', 15, 'distance'] before, using random point ['manhattan', 7, 'distance']\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV] END ..metric=manhattan, n_neighbors=7, weights=distance; total time= 3.0min\n",
      "[CV] END ..metric=manhattan, n_neighbors=7, weights=distance; total time= 3.0min\n",
      "[CV] END ..metric=manhattan, n_neighbors=7, weights=distance; total time= 3.0min\n",
      "[CV] END ..metric=manhattan, n_neighbors=7, weights=distance; total time= 3.0min\n",
      "[CV] END ..metric=manhattan, n_neighbors=7, weights=distance; total time= 3.0min\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV] END ..metric=minkowski, n_neighbors=10, weights=uniform; total time=  15.7s\n",
      "[CV] END ..metric=minkowski, n_neighbors=10, weights=uniform; total time=  15.9s\n",
      "[CV] END ..metric=minkowski, n_neighbors=10, weights=uniform; total time=  15.9s\n",
      "[CV] END ..metric=minkowski, n_neighbors=10, weights=uniform; total time=  15.9s\n",
      "[CV] END ..metric=minkowski, n_neighbors=10, weights=uniform; total time=  15.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fpolygon/.pyenv/versions/3.12.4/lib/python3.12/site-packages/skopt/optimizer/optimizer.py:517: UserWarning: The objective has been evaluated at point ['minkowski', 15, 'distance'] before, using random point ['euclidean', 6, 'distance']\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV] END ..metric=euclidean, n_neighbors=6, weights=distance; total time=  13.1s\n",
      "[CV] END ..metric=euclidean, n_neighbors=6, weights=distance; total time=  14.7s\n",
      "[CV] END ..metric=euclidean, n_neighbors=6, weights=distance; total time=  14.7s\n",
      "[CV] END ..metric=euclidean, n_neighbors=6, weights=distance; total time=  14.9s\n",
      "[CV] END ..metric=euclidean, n_neighbors=6, weights=distance; total time=  14.9s\n",
      "Best Model Parameters from Bayesian Search: OrderedDict({'metric': 'minkowski', 'n_neighbors': 15, 'weights': 'distance'})\n",
      "BoW K-Nearest Neighbors Model Performance after Bayesian Search:\n",
      "Accuracy: 0.6463\n",
      "Precision: 0.6538303973781238\n",
      "Recall: 0.6334590196467553\n",
      "F1 Score: 0.6434835198064711\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Integer, Categorical\n",
    "\n",
    "# Fit and transform the preprocessed reviews\n",
    "vectorizer = CountVectorizer()\n",
    "bow_features = vectorizer.fit_transform(df['preprocessed_review'])\n",
    "\n",
    "# Get the sentiment labels from the preprocessed DataFrame\n",
    "labels = df['sentiment']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(bow_features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the K-Nearest Neighbors model\n",
    "model = KNeighborsClassifier()\n",
    "\n",
    "# Define the hyperparameter space\n",
    "param_space = {\n",
    "    'n_neighbors': Integer(3, 15),  # Range for number of neighbors\n",
    "    'weights': Categorical(['uniform', 'distance']),  # Weight function used in prediction\n",
    "    'metric': Categorical(['euclidean', 'manhattan', 'minkowski'])  # Distance metric\n",
    "}\n",
    "\n",
    "# Set up Bayesian Search with cross-validation\n",
    "bayes_search = BayesSearchCV(\n",
    "    estimator=model,\n",
    "    search_spaces=param_space,\n",
    "    n_iter=32,  # Number of parameter settings that are sampled\n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    scoring='f1',\n",
    "    n_jobs=-1,  # Use all available cores\n",
    "    verbose=2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit Bayesian Search\n",
    "bayes_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model from Bayesian Search\n",
    "best_model = bayes_search.best_estimator_\n",
    "\n",
    "# Make predictions on the testing set with the best model\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"Best Model Parameters from Bayesian Search:\", bayes_search.best_params_)\n",
    "print(\"BoW K-Nearest Neighbors Model Performance after Bayesian Search:\")\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree: Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-22 15:30:39,448 - INFO - Vectorizing text data.\n",
      "2024-08-22 15:30:42,242 - INFO - Text data vectorization completed.\n",
      "2024-08-22 15:30:42,243 - INFO - Extracting sentiment labels.\n",
      "2024-08-22 15:30:42,243 - INFO - Splitting the data into training and testing sets.\n",
      "2024-08-22 15:30:42,263 - INFO - Starting Grid Search to find the best hyperparameters.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 216 candidates, totalling 1080 fits\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_leaf=1, min_samples_split=2, splitter=random; total time=  37.1s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_leaf=1, min_samples_split=2, splitter=random; total time=  38.1s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_leaf=1, min_samples_split=2, splitter=random; total time=  38.3s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_leaf=1, min_samples_split=2, splitter=best; total time=  39.3s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_leaf=1, min_samples_split=2, splitter=best; total time=  40.0s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_leaf=1, min_samples_split=2, splitter=best; total time=  40.6s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_leaf=1, min_samples_split=2, splitter=best; total time=  40.6s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_leaf=1, min_samples_split=2, splitter=best; total time=  41.0s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_leaf=1, min_samples_split=2, splitter=random; total time=  42.3s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_leaf=1, min_samples_split=2, splitter=random; total time=  43.5s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_leaf=1, min_samples_split=5, splitter=best; total time=  44.4s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_leaf=1, min_samples_split=5, splitter=best; total time=  45.4s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_leaf=1, min_samples_split=5, splitter=best; total time=  44.5s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_leaf=1, min_samples_split=5, splitter=random; total time=  43.5s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_leaf=1, min_samples_split=5, splitter=best; total time=  44.8s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_leaf=1, min_samples_split=5, splitter=best; total time=  46.2s\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Thread-safe function to fit and transform the text data using CountVectorizer\n",
    "def vectorize_text(vectorizer, text_data):\n",
    "    logging.info(\"Vectorizing text data.\")\n",
    "    return vectorizer.fit_transform(text_data)\n",
    "\n",
    "# Prepare the vectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Use ThreadPoolExecutor to vectorize the text data in a separate thread\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    future = executor.submit(vectorize_text, vectorizer, df['preprocessed_review'])\n",
    "    bow_features = future.result()\n",
    "\n",
    "logging.info(\"Text data vectorization completed.\")\n",
    "\n",
    "# Get the sentiment labels from the preprocessed DataFrame\n",
    "logging.info(\"Extracting sentiment labels.\")\n",
    "labels = df['sentiment']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "logging.info(\"Splitting the data into training and testing sets.\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(bow_features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the Decision Tree model\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],  # Function to measure the quality of a split\n",
    "    'splitter': ['best', 'random'],  # Strategy used to choose the split at each node\n",
    "    'max_depth': [None, 10, 20, 30, 40, 50],  # Maximum depth of the tree\n",
    "    'min_samples_split': [2, 5, 10],  # Minimum number of samples required to split an internal node\n",
    "    'min_samples_leaf': [1, 2, 4]  # Minimum number of samples required to be at a leaf node\n",
    "}\n",
    "\n",
    "# Set up Grid Search with cross-validation\n",
    "logging.info(\"Starting Grid Search to find the best hyperparameters.\")\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='f1', n_jobs=-1, verbose=2)\n",
    "\n",
    "# Fit Grid Search\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model from Grid Search\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "logging.info(f\"Best hyperparameters found: {grid_search.best_params_}\")\n",
    "\n",
    "# Make predictions on the testing set with the best model\n",
    "logging.info(\"Making predictions on the testing set.\")\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "logging.info(\"Evaluating the model's performance.\")\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Log the results\n",
    "logging.info(\"BoW Decision Tree Model Performance after Grid Search:\")\n",
    "logging.info(f\"Accuracy: {accuracy}\")\n",
    "logging.info(f\"Precision: {precision}\")\n",
    "logging.info(f\"Recall: {recall}\")\n",
    "logging.info(f\"F1 Score: {f1}\")\n",
    "\n",
    "print(\"BoW Decision Tree Model Performance after Grid Search:\")\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree: Bayes Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-19 23:42:43,530 - INFO - Vectorizing text data.\n",
      "2024-08-19 23:42:46,631 - INFO - Text data vectorization completed.\n",
      "2024-08-19 23:42:46,632 - INFO - Extracting sentiment labels.\n",
      "2024-08-19 23:42:46,632 - INFO - Splitting the data into training and testing sets.\n",
      "2024-08-19 23:42:46,657 - INFO - Starting Bayesian Search to find the best hyperparameters.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 62\u001b[0m\n\u001b[1;32m     50\u001b[0m bayes_search \u001b[38;5;241m=\u001b[39m BayesSearchCV(\n\u001b[1;32m     51\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     52\u001b[0m     search_spaces\u001b[38;5;241m=\u001b[39mparam_space,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     58\u001b[0m     random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m\n\u001b[1;32m     59\u001b[0m )\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Fit Bayesian Search\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m \u001b[43mbayes_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# Get the best model from Bayesian Search\u001b[39;00m\n\u001b[1;32m     65\u001b[0m best_model \u001b[38;5;241m=\u001b[39m bayes_search\u001b[38;5;241m.\u001b[39mbest_estimator_\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.4/lib/python3.12/site-packages/skopt/searchcv.py:542\u001b[0m, in \u001b[0;36mBayesSearchCV.fit\u001b[0;34m(self, X, y, groups, callback, **fit_params)\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrefit):\n\u001b[1;32m    536\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    537\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBayesSearchCV doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt support a callable refit, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    538\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt define an implicit score to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    539\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimize\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    540\u001b[0m     )\n\u001b[0;32m--> 542\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    544\u001b[0m \u001b[38;5;66;03m# BaseSearchCV never ranked train scores,\u001b[39;00m\n\u001b[1;32m    545\u001b[0m \u001b[38;5;66;03m# but apparently we used to ship this (back-compat)\u001b[39;00m\n\u001b[1;32m    546\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_train_score:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.4/lib/python3.12/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.4/lib/python3.12/site-packages/sklearn/model_selection/_search.py:968\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    962\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m    963\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    964\u001b[0m     )\n\u001b[1;32m    966\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m--> 968\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    970\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    971\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    972\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.4/lib/python3.12/site-packages/skopt/searchcv.py:599\u001b[0m, in \u001b[0;36mBayesSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m    595\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m n_iter \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;66;03m# when n_iter < n_points points left for evaluation\u001b[39;00m\n\u001b[1;32m    597\u001b[0m     n_points_adjusted \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(n_iter, n_points)\n\u001b[0;32m--> 599\u001b[0m     optim_result, score_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    600\u001b[0m \u001b[43m        \u001b[49m\u001b[43msearch_space\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    602\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscore_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    603\u001b[0m \u001b[43m        \u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_points\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_points_adjusted\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    605\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    606\u001b[0m     n_iter \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m n_points\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m eval_callbacks(callbacks, optim_result):\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.4/lib/python3.12/site-packages/skopt/searchcv.py:453\u001b[0m, in \u001b[0;36mBayesSearchCV._step\u001b[0;34m(self, search_space, optimizer, score_name, evaluate_candidates, n_points)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;66;03m# make lists into dictionaries\u001b[39;00m\n\u001b[1;32m    451\u001b[0m params_dict \u001b[38;5;241m=\u001b[39m [point_asdict(search_space, p) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m params]\n\u001b[0;32m--> 453\u001b[0m all_results \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;66;03m# if self.scoring is a callable, we have to wait until here\u001b[39;00m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;66;03m# to get the score name\u001b[39;00m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m score_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.4/lib/python3.12/site-packages/sklearn/model_selection/_search.py:914\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    906\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    907\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    908\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    909\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    910\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[1;32m    911\u001b[0m         )\n\u001b[1;32m    912\u001b[0m     )\n\u001b[0;32m--> 914\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    915\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    916\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    922\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    924\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    929\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    930\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    933\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    934\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    935\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    936\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    937\u001b[0m     )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.4/lib/python3.12/site-packages/sklearn/utils/parallel.py:67\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     62\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     63\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     64\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     66\u001b[0m )\n\u001b[0;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.4/lib/python3.12/site-packages/joblib/parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[1;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.4/lib/python3.12/site-packages/joblib/parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.4/lib/python3.12/site-packages/joblib/parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[1;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[1;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[1;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Integer, Categorical\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Thread-safe function to fit and transform the text data using CountVectorizer\n",
    "def vectorize_text(vectorizer, text_data):\n",
    "    logging.info(\"Vectorizing text data.\")\n",
    "    return vectorizer.fit_transform(text_data)\n",
    "\n",
    "# Prepare the vectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Use ThreadPoolExecutor to vectorize the text data in a separate thread\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    future = executor.submit(vectorize_text, vectorizer, df['preprocessed_review'])\n",
    "    bow_features = future.result()\n",
    "\n",
    "logging.info(\"Text data vectorization completed.\")\n",
    "\n",
    "# Get the sentiment labels from the preprocessed DataFrame\n",
    "logging.info(\"Extracting sentiment labels.\")\n",
    "labels = df['sentiment']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "logging.info(\"Splitting the data into training and testing sets.\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(bow_features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the Decision Tree model\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Define the hyperparameter space\n",
    "param_space = {\n",
    "    'criterion': Categorical(['gini', 'entropy']),  # Function to measure the quality of a split\n",
    "    'splitter': Categorical(['best', 'random']),  # Strategy used to choose the split at each node\n",
    "    'max_depth': Integer(1, 50),  # Maximum depth of the tree\n",
    "    'min_samples_split': Integer(2, 10),  # Minimum number of samples required to split an internal node\n",
    "    'min_samples_leaf': Integer(1, 4)  # Minimum number of samples required to be at a leaf node\n",
    "}\n",
    "\n",
    "# Set up Bayesian Search with cross-validation\n",
    "logging.info(\"Starting Bayesian Search to find the best hyperparameters.\")\n",
    "bayes_search = BayesSearchCV(\n",
    "    estimator=model,\n",
    "    search_spaces=param_space,\n",
    "    n_iter=32,  # Number of parameter settings that are sampled\n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    scoring='f1',\n",
    "    n_jobs=-1,  # Use all available cores\n",
    "    verbose=2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit Bayesian Search\n",
    "bayes_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model from Bayesian Search\n",
    "best_model = bayes_search.best_estimator_\n",
    "\n",
    "logging.info(f\"Best hyperparameters found: {bayes_search.best_params_}\")\n",
    "\n",
    "# Make predictions on the testing set with the best model\n",
    "logging.info(\"Making predictions on the testing set.\")\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "logging.info(\"Evaluating the model's performance.\")\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Log the results\n",
    "logging.info(\"BoW Decision Tree Model Performance after Bayesian Search:\")\n",
    "logging.info(f\"Accuracy: {accuracy}\")\n",
    "logging.info(f\"Precision: {precision}\")\n",
    "logging.info(f\"Recall: {recall}\")\n",
    "logging.info(f\"F1 Score: {f1}\")\n",
    "\n",
    "print(\"BoW Decision Tree Model Performance after Bayesian Search:\")\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging DT: Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-19 23:44:15,884 - INFO - Starting the Bagging Decision Tree model training with BoW.\n",
      "2024-08-19 23:44:15,884 - INFO - Fitting and transforming the text data using CountVectorizer.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting and transforming the text data using CountVectorizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m vectorizer \u001b[38;5;241m=\u001b[39m CountVectorizer()\n\u001b[0;32m---> 17\u001b[0m bow_features \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mfit_transform(\u001b[43mdf\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreprocessed_review\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Get the sentiment labels from the preprocessed DataFrame\u001b[39;00m\n\u001b[1;32m     20\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtracting sentiment labels.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Set up logging with a higher verbosity level\n",
    "logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Log the start of the process\n",
    "logging.info(\"Starting the Bagging Decision Tree model training with BoW.\")\n",
    "\n",
    "# Fit and transform the preprocessed reviews\n",
    "logging.info(\"Fitting and transforming the text data using CountVectorizer.\")\n",
    "vectorizer = CountVectorizer()\n",
    "bow_features = vectorizer.fit_transform(df['preprocessed_review'])\n",
    "\n",
    "# Get the sentiment labels from the preprocessed DataFrame\n",
    "logging.info(\"Extracting sentiment labels.\")\n",
    "labels = df['sentiment']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "logging.info(\"Splitting the data into training and testing sets.\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(bow_features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the base Decision Tree model\n",
    "base_estimator = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Define the BaggingClassifier\n",
    "model = BaggingClassifier(base_estimator=base_estimator, random_state=42)\n",
    "\n",
    "# Define the hyperparameter grid for Grid Search\n",
    "param_grid = {\n",
    "    'n_estimators': [10, 50, 100],  # Number of base estimators in the ensemble\n",
    "    'base_estimator__criterion': ['gini', 'entropy'],  # Criterion for splitting in the base estimator\n",
    "    'base_estimator__max_depth': [None, 10, 20, 30],  # Maximum depth of the base estimator\n",
    "    'base_estimator__min_samples_split': [2, 5, 10],  # Minimum number of samples required to split an internal node\n",
    "    'base_estimator__min_samples_leaf': [1, 2, 4]  # Minimum number of samples required to be at a leaf node\n",
    "}\n",
    "\n",
    "# Set up Grid Search with cross-validation\n",
    "logging.info(\"Starting Grid Search to find the best hyperparameters.\")\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='f1', n_jobs=-1, verbose=2)\n",
    "\n",
    "# Fit Grid Search\n",
    "logging.info(\"Fitting Grid Search...\")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model from Grid Search\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "logging.info(f\"Best hyperparameters found: {grid_search.best_params_}\")\n",
    "\n",
    "# Make predictions on the testing set with the best model\n",
    "logging.info(\"Making predictions on the testing set.\")\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "logging.info(\"Evaluating the model's performance.\")\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, pos_label='positive')  # Specify the positive label\n",
    "recall = recall_score(y_test, y_pred, pos_label='positive')  # Specify the positive label\n",
    "f1 = f1_score(y_test, y_pred, pos_label='positive')  # Specify the positive label\n",
    "\n",
    "# Log the results\n",
    "logging.info(\"BoW Bagging Decision Tree Model Performance after Grid Search:\")\n",
    "logging.info(f\"Accuracy: {accuracy}\")\n",
    "logging.info(f\"Precision: {precision}\")\n",
    "logging.info(f\"Recall: {recall}\")\n",
    "logging.info(f\"F1 Score: {f1}\")\n",
    "\n",
    "print(\"BoW Bagging Decision Tree Model Performance after Grid Search:\")\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging DT: Bayes Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Integer, Categorical\n",
    "\n",
    "# Set up logging with a higher verbosity level\n",
    "logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Log the start of the process\n",
    "logging.info(\"Starting the Bagging Decision Tree model training with BoW.\")\n",
    "\n",
    "# Fit and transform the preprocessed reviews\n",
    "logging.info(\"Fitting and transforming the text data using CountVectorizer.\")\n",
    "vectorizer = CountVectorizer()\n",
    "bow_features = vectorizer.fit_transform(df['preprocessed_review'])\n",
    "\n",
    "# Get the sentiment labels from the preprocessed DataFrame\n",
    "logging.info(\"Extracting sentiment labels.\")\n",
    "labels = df['sentiment']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "logging.info(\"Splitting the data into training and testing sets.\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(bow_features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the base Decision Tree model\n",
    "base_estimator = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Define the BaggingClassifier\n",
    "model = BaggingClassifier(base_estimator=base_estimator, random_state=42)\n",
    "\n",
    "# Define the hyperparameter space for Bayesian Search\n",
    "param_space = {\n",
    "    'n_estimators': Integer(10, 100),  # Number of base estimators in the ensemble\n",
    "    'base_estimator__criterion': Categorical(['gini', 'entropy']),  # Criterion for splitting in the base estimator\n",
    "    'base_estimator__max_depth': Integer(1, 50),  # Maximum depth of the base estimator\n",
    "    'base_estimator__min_samples_split': Integer(2, 10),  # Minimum number of samples required to split an internal node\n",
    "    'base_estimator__min_samples_leaf': Integer(1, 4)  # Minimum number of samples required to be at a leaf node\n",
    "}\n",
    "\n",
    "# Set up Bayesian Search with cross-validation\n",
    "logging.info(\"Starting Bayesian Search to find the best hyperparameters.\")\n",
    "bayes_search = BayesSearchCV(\n",
    "    estimator=model,\n",
    "    search_spaces=param_space,\n",
    "    n_iter=32,  # Number of parameter settings that are sampled\n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    scoring='f1',\n",
    "    n_jobs=-1,  # Use all available cores\n",
    "    verbose=2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit Bayesian Search\n",
    "logging.info(\"Fitting Bayesian Search...\")\n",
    "bayes_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model from Bayesian Search\n",
    "best_model = bayes_search.best_estimator_\n",
    "\n",
    "logging.info(f\"Best hyperparameters found: {bayes_search.best_params_}\")\n",
    "\n",
    "# Make predictions on the testing set with the best model\n",
    "logging.info(\"Making predictions on the testing set.\")\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "logging.info(\"Evaluating the model's performance.\")\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Log the results\n",
    "logging.info(\"BoW Bagging Decision Tree Model Performance after Bayesian Search:\")\n",
    "logging.info(f\"Accuracy: {accuracy}\")\n",
    "logging.info(f\"Precision: {precision}\")\n",
    "logging.info(f\"Recall: {recall}\")\n",
    "logging.info(f\"F1 Score: {f1}\")\n",
    "\n",
    "print(\"BoW Bagging Decision Tree Model Performance after Bayesian Search:\")\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosted DT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosted DT: Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Fit and transform the preprocessed reviews\n",
    "vectorizer = CountVectorizer()\n",
    "bow_features = vectorizer.fit_transform(df['preprocessed_review'])\n",
    "\n",
    "# Get the sentiment labels from the preprocessed DataFrame\n",
    "labels = df['sentiment']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(bow_features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the Gradient Boosting model\n",
    "model = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Define the hyperparameter grid for Grid Search\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],  # Number of boosting stages to be run\n",
    "    'learning_rate': [0.01, 0.1, 0.2],  # Learning rate shrinks the contribution of each tree\n",
    "    'max_depth': [3, 5, 7],  # Maximum depth of the individual trees\n",
    "    'subsample': [0.8, 1.0],  # Fraction of samples used for fitting individual base learners\n",
    "    'min_samples_split': [2, 5, 10],  # Minimum number of samples required to split an internal node\n",
    "    'min_samples_leaf': [1, 2, 4]  # Minimum number of samples required to be at a leaf node\n",
    "}\n",
    "\n",
    "# Set up Grid Search with cross-validation\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='f1', n_jobs=-1, verbose=2)\n",
    "\n",
    "# Fit Grid Search\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model from Grid Search\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the testing set with the best model\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, pos_label='positive')  # Specify the positive label\n",
    "recall = recall_score(y_test, y_pred, pos_label='positive')  # Specify the positive label\n",
    "f1 = f1_score(y_test, y_pred, pos_label='positive')  # Specify the positive label\n",
    "\n",
    "print(\"Best Model Parameters from Grid Search:\", grid_search.best_params_)\n",
    "print(\"BoW Gradient Boosting Model Performance after Grid Search:\")\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosted DT: Bayes Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Integer, Real\n",
    "\n",
    "# Fit and transform the preprocessed reviews\n",
    "vectorizer = CountVectorizer()\n",
    "bow_features = vectorizer.fit_transform(df['preprocessed_review'])\n",
    "\n",
    "# Get the sentiment labels from the preprocessed DataFrame\n",
    "labels = df['sentiment']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(bow_features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the Gradient Boosting model\n",
    "model = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Define the hyperparameter space for Bayesian Search\n",
    "param_space = {\n",
    "    'n_estimators': Integer(50, 200),  # Number of boosting stages to be run\n",
    "    'learning_rate': Real(0.01, 0.2, prior='log-uniform'),  # Learning rate shrinks the contribution of each tree\n",
    "    'max_depth': Integer(3, 10),  # Maximum depth of the individual trees\n",
    "    'subsample': Real(0.8, 1.0),  # Fraction of samples used for fitting individual base learners\n",
    "    'min_samples_split': Integer(2, 10),  # Minimum number of samples required to split an internal node\n",
    "    'min_samples_leaf': Integer(1, 4)  # Minimum number of samples required to be at a leaf node\n",
    "}\n",
    "\n",
    "# Set up Bayesian Search with cross-validation\n",
    "bayes_search = BayesSearchCV(\n",
    "    estimator=model,\n",
    "    search_spaces=param_space,\n",
    "    n_iter=32,  # Number of parameter settings that are sampled\n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    scoring='f1',\n",
    "    n_jobs=-1,  # Use all available cores\n",
    "    verbose=2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit Bayesian Search\n",
    "bayes_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model from Bayesian Search\n",
    "best_model = bayes_search.best_estimator_\n",
    "\n",
    "# Make predictions on the testing set with the best model\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, pos_label='positive')  # Specify the positive label\n",
    "recall = recall_score(y_test, y_pred, pos_label='positive')  # Specify the positive label\n",
    "f1 = f1_score(y_test, y_pred, pos_label='positive')  # Specify the positive label\n",
    "\n",
    "print(\"Best Model Parameters from Bayesian Search:\", bayes_search.best_params_)\n",
    "print(\"BoW Gradient Boosting Model Performance after Bayesian Search:\")\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest: Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Fit and transform the preprocessed reviews\n",
    "vectorizer = CountVectorizer()\n",
    "bow_features = vectorizer.fit_transform(df['preprocessed_review'])\n",
    "\n",
    "# Get the sentiment labels from the preprocessed DataFrame\n",
    "labels = df['sentiment']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(bow_features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the Random Forest model\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Define the hyperparameter grid for Grid Search\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],  # Number of trees in the forest\n",
    "    'max_depth': [None, 10, 20, 30],  # Maximum depth of the tree\n",
    "    'min_samples_split': [2, 5, 10],  # Minimum number of samples required to split an internal node\n",
    "    'min_samples_leaf': [1, 2, 4],  # Minimum number of samples required to be at a leaf node\n",
    "    'bootstrap': [True, False]  # Whether bootstrap samples are used when building trees\n",
    "}\n",
    "\n",
    "# Set up Grid Search with cross-validation\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='f1', n_jobs=-1, verbose=2)\n",
    "\n",
    "# Fit Grid Search\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model from Grid Search\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the testing set with the best model\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, pos_label='positive')  # Specify the positive label\n",
    "recall = recall_score(y_test, y_pred, pos_label='positive')  # Specify the positive label\n",
    "f1 = f1_score(y_test, y_pred, pos_label='positive')  # Specify the positive label\n",
    "\n",
    "print(\"Best Model Parameters from Grid Search:\", grid_search.best_params_)\n",
    "print(\"BoW Random Forest Model Performance after Grid Search:\")\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest: Bayes Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Integer, Categorical\n",
    "\n",
    "# Fit and transform the preprocessed reviews\n",
    "vectorizer = CountVectorizer()\n",
    "bow_features = vectorizer.fit_transform(df['preprocessed_review'])\n",
    "\n",
    "# Get the sentiment labels from the preprocessed DataFrame\n",
    "labels = df['sentiment']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(bow_features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the Random Forest model\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Define the hyperparameter space for Bayesian Search\n",
    "param_space = {\n",
    "    'n_estimators': Integer(100, 300),  # Number of trees in the forest\n",
    "    'max_depth': Integer(10, 30),  # Maximum depth of the tree\n",
    "    'min_samples_split': Integer(2, 10),  # Minimum number of samples required to split an internal node\n",
    "    'min_samples_leaf': Integer(1, 4),  # Minimum number of samples required to be at a leaf node\n",
    "    'bootstrap': Categorical([True, False])  # Whether bootstrap samples are used when building trees\n",
    "}\n",
    "\n",
    "# Set up Bayesian Search with cross-validation\n",
    "bayes_search = BayesSearchCV(\n",
    "    estimator=model,\n",
    "    search_spaces=param_space,\n",
    "    n_iter=32,  # Number of parameter settings that are sampled\n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    scoring='f1',\n",
    "    n_jobs=-1,  # Use all available cores\n",
    "    verbose=2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit Bayesian Search\n",
    "bayes_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model from Bayesian Search\n",
    "best_model = bayes_search.best_estimator_\n",
    "\n",
    "# Make predictions on the testing set with the best model\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, pos_label='positive')  # Specify the positive label\n",
    "recall = recall_score(y_test, y_pred, pos_label='positive')  # Specify the positive label\n",
    "f1 = f1_score(y_test, y_pred, pos_label='positive')  # Specify the positive label\n",
    "\n",
    "print(\"Best Model Parameters from Bayesian Search:\", bayes_search.best_params_)\n",
    "print(\"BoW Random Forest Model Performance after Bayesian Search:\")\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voting Classifier: Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Fit and transform the preprocessed reviews\n",
    "vectorizer = CountVectorizer()\n",
    "bow_features = vectorizer.fit_transform(df['preprocessed_review'])\n",
    "\n",
    "# Get the sentiment labels from the preprocessed DataFrame\n",
    "labels = df['sentiment']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(bow_features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the base models\n",
    "log_clf = LogisticRegression(random_state=42)\n",
    "dt_clf = DecisionTreeClassifier(random_state=42)\n",
    "svm_clf = SVC(probability=True, random_state=42)\n",
    "\n",
    "# Create the Voting Classifier\n",
    "model = VotingClassifier(estimators=[\n",
    "    ('lr', log_clf),\n",
    "    ('dt', dt_clf),\n",
    "    ('svm', svm_clf)\n",
    "], voting='soft')  # Use 'soft' voting to consider predicted probabilities\n",
    "\n",
    "# Define the hyperparameter grid for Grid Search\n",
    "param_grid = {\n",
    "    'lr__C': [0.01, 0.1, 1, 10],  # Logistic Regression regularization strength\n",
    "    'dt__max_depth': [None, 10, 20, 30],  # Maximum depth of the Decision Tree\n",
    "    'svm__C': [0.1, 1, 10],  # SVM regularization parameter\n",
    "    'svm__kernel': ['linear', 'rbf']  # SVM kernel type\n",
    "}\n",
    "\n",
    "# Set up Grid Search with cross-validation\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='f1', n_jobs=-1, verbose=2)\n",
    "\n",
    "# Fit Grid Search\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model from Grid Search\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the testing set with the best model\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, pos_label='positive')  # Specify the positive label\n",
    "recall = recall_score(y_test, y_pred, pos_label='positive')  # Specify the positive label\n",
    "f1 = f1_score(y_test, y_pred, pos_label='positive')  # Specify the positive label\n",
    "\n",
    "print(\"Best Model Parameters from Grid Search:\", grid_search.best_params_)\n",
    "print(\"BoW Voting Classifier Model Performance after Grid Search:\")\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voting Classifier: Bayes Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "\n",
    "# Fit and transform the preprocessed reviews\n",
    "vectorizer = CountVectorizer()\n",
    "bow_features = vectorizer.fit_transform(df['preprocessed_review'])\n",
    "\n",
    "# Get the sentiment labels from the preprocessed DataFrame\n",
    "labels = df['sentiment']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(bow_features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the base models\n",
    "log_clf = LogisticRegression(random_state=42, max_iter=1000)\n",
    "dt_clf = DecisionTreeClassifier(random_state=42)\n",
    "svm_clf = SVC(probability=True, random_state=42)\n",
    "\n",
    "# Create the Voting Classifier\n",
    "model = VotingClassifier(estimators=[\n",
    "    ('lr', log_clf),\n",
    "    ('dt', dt_clf),\n",
    "    ('svm', svm_clf)\n",
    "], voting='soft')  # Use 'soft' voting to consider predicted probabilities\n",
    "\n",
    "# Define the hyperparameter space for Bayesian Search\n",
    "param_space = {\n",
    "    'lr__C': Real(0.01, 10.0, prior='log-uniform'),  # Logistic Regression regularization strength\n",
    "    'dt__max_depth': Integer(3, 30),  # Maximum depth of the Decision Tree\n",
    "    'svm__C': Real(0.01, 10.0, prior='log-uniform'),  # SVM regularization parameter\n",
    "    'svm__kernel': Categorical(['linear', 'rbf'])  # SVM kernel type\n",
    "}\n",
    "\n",
    "# Set up Bayesian Search with cross-validation\n",
    "bayes_search = BayesSearchCV(\n",
    "    estimator=model,\n",
    "    search_spaces=param_space,\n",
    "    n_iter=32,  # Number of parameter settings that are sampled\n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    scoring='f1',\n",
    "    n_jobs=-1,  # Use all available cores\n",
    "    verbose=2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit Bayesian Search\n",
    "bayes_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model from Bayesian Search\n",
    "best_model = bayes_search.best_estimator_\n",
    "\n",
    "# Make predictions on the testing set with the best model\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, pos_label='positive')  # Specify the positive label\n",
    "recall = recall_score(y_test, y_pred, pos_label='positive')  # Specify the positive label\n",
    "f1 = f1_score(y_test, y_pred, pos_label='positive')  # Specify the positive label\n",
    "\n",
    "print(\"Best Model Parameters from Bayesian Search:\", bayes_search.best_params_)\n",
    "print(\"BoW Voting Classifier Model Performance after Bayesian Search:\")\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning with Neuronal Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DNN: Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Function to create the model, required for KerasClassifier\n",
    "def create_model(learning_rate=0.001, dropout_rate=0.5):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, input_dim=X_train.shape[1], activation='relu'))  # Input layer\n",
    "    model.add(Dropout(dropout_rate))  # Dropout for regularization\n",
    "    model.add(Dense(64, activation='relu'))  # Hidden layer\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1, activation='sigmoid'))  # Output layer for binary classification\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Fit and transform the preprocessed reviews\n",
    "vectorizer = CountVectorizer()\n",
    "bow_features = vectorizer.fit_transform(df['preprocessed_review'])\n",
    "\n",
    "# Get the sentiment labels from the preprocessed DataFrame and encode them\n",
    "labels = df['sentiment']\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(labels)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(bow_features.toarray(), encoded_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Wrap the model using KerasClassifier\n",
    "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "\n",
    "# Define the hyperparameter grid for Grid Search\n",
    "param_grid = {\n",
    "    'batch_size': [16, 32, 64],  # Batch size\n",
    "    'epochs': [10, 20],  # Number of epochs\n",
    "    'learning_rate': [0.001, 0.01, 0.1],  # Learning rate for the optimizer\n",
    "    'dropout_rate': [0.3, 0.5, 0.7]  # Dropout rate for regularization\n",
    "}\n",
    "\n",
    "# Set up Grid Search with cross-validation\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=3, scoring='accuracy', n_jobs=-1, verbose=2)\n",
    "\n",
    "# Fit Grid Search\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model from Grid Search\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the testing set with the best model\n",
    "y_pred_prob = best_model.predict(X_test)\n",
    "y_pred = (y_pred_prob > 0.5).astype(\"int32\").flatten()\n",
    "\n",
    "# Evaluate the model's performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"Best Model Parameters from Grid Search:\", grid_search.best_params_)\n",
    "print(\"Neural Network Model Performance after Grid Search:\")\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DNN: Bayes Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Integer\n",
    "\n",
    "# Function to create the model, required for KerasClassifier\n",
    "def create_model(learning_rate=0.001, dropout_rate=0.5):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, input_dim=X_train.shape[1], activation='relu'))  # Input layer\n",
    "    model.add(Dropout(dropout_rate))  # Dropout for regularization\n",
    "    model.add(Dense(64, activation='relu'))  # Hidden layer\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1, activation='sigmoid'))  # Output layer for binary classification\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Fit and transform the preprocessed reviews\n",
    "vectorizer = CountVectorizer()\n",
    "bow_features = vectorizer.fit_transform(df['preprocessed_review'])\n",
    "\n",
    "# Get the sentiment labels from the preprocessed DataFrame and encode them\n",
    "labels = df['sentiment']\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(labels)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(bow_features.toarray(), encoded_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Wrap the model using KerasClassifier\n",
    "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "\n",
    "# Define the hyperparameter space for Bayesian Search\n",
    "param_space = {\n",
    "    'batch_size': Integer(16, 64),  # Batch size\n",
    "    'epochs': Integer(10, 50),  # Number of epochs\n",
    "    'learning_rate': Real(0.001, 0.1, prior='log-uniform'),  # Learning rate for the optimizer\n",
    "    'dropout_rate': Real(0.3, 0.7)  # Dropout rate for regularization\n",
    "}\n",
    "\n",
    "# Set up Bayesian Search with cross-validation\n",
    "bayes_search = BayesSearchCV(\n",
    "    estimator=model,\n",
    "    search_spaces=param_space,\n",
    "    n_iter=32,  # Number of parameter settings that are sampled\n",
    "    cv=3,  # 3-fold cross-validation\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,  # Use all available cores\n",
    "    verbose=2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit Bayesian Search\n",
    "bayes_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model from Bayesian Search\n",
    "best_model = bayes_search.best_estimator_\n",
    "\n",
    "# Make predictions on the testing set with the best model\n",
    "y_pred_prob = best_model.predict(X_test)\n",
    "y_pred = (y_pred_prob > 0.5).astype(\"int32\").flatten()\n",
    "\n",
    "# Evaluate the model's performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"Best Model Parameters from Bayesian Search:\", bayes_search.best_params_)\n",
    "print(\"Neural Network Model Performance after Bayesian Search:\")\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN: Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Function to create the CNN model, required for KerasClassifier\n",
    "def create_cnn_model(learning_rate=0.001, dropout_rate=0.5, filters=128, kernel_size=5):\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=filters, kernel_size=kernel_size, activation='relu', input_shape=(X_train.shape[1], 1)))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1, activation='sigmoid'))  # Output layer for binary classification\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Convert the reviews into bag-of-words features\n",
    "vectorizer = CountVectorizer(max_features=10000)  # Limit to top 10,000 words\n",
    "bow_features = vectorizer.fit_transform(df['preprocessed_review'])\n",
    "\n",
    "# Get the sentiment labels from the preprocessed DataFrame and encode them\n",
    "labels = df['sentiment']\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(labels)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(bow_features.toarray(), encoded_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Reshape input data to 2D for Conv1D layer (samples, timesteps, features)\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "\n",
    "# Wrap the model using KerasClassifier\n",
    "model = KerasClassifier(build_fn=create_cnn_model, verbose=0)\n",
    "\n",
    "# Define the hyperparameter grid for Grid Search\n",
    "param_grid = {\n",
    "    'batch_size': [16, 32, 64],  # Batch size\n",
    "    'epochs': [10, 20],  # Number of epochs\n",
    "    'learning_rate': [0.001, 0.01],  # Learning rate for the optimizer\n",
    "    'dropout_rate': [0.3, 0.5],  # Dropout rate for regularization\n",
    "    'filters': [64, 128],  # Number of filters in Conv1D\n",
    "    'kernel_size': [3, 5]  # Kernel size in Conv1D\n",
    "}\n",
    "\n",
    "# Set up Grid Search with cross-validation\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=3, scoring='accuracy', n_jobs=-1, verbose=2)\n",
    "\n",
    "# Fit Grid Search\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model from Grid Search\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the testing set with the best model\n",
    "y_pred_prob = best_model.predict(X_test)\n",
    "y_pred = (y_pred_prob > 0.5).astype(\"int32\").flatten()\n",
    "\n",
    "# Evaluate the model's performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"Best Model Parameters from Grid Search:\", grid_search.best_params_)\n",
    "print(\"Convolutional Neural Network Model Performance after Grid Search:\")\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN: Bayes Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Integer\n",
    "\n",
    "# Function to create the CNN model, required for KerasClassifier\n",
    "def create_cnn_model(learning_rate=0.001, dropout_rate=0.5, filters=128, kernel_size=5):\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=filters, kernel_size=kernel_size, activation='relu', input_shape=(X_train.shape[1], 1)))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1, activation='sigmoid'))  # Output layer for binary classification\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Convert the reviews into bag-of-words features\n",
    "vectorizer = CountVectorizer(max_features=10000)  # Limit to top 10,000 words\n",
    "bow_features = vectorizer.fit_transform(df['preprocessed_review'])\n",
    "\n",
    "# Get the sentiment labels from the preprocessed DataFrame and encode them\n",
    "labels = df['sentiment']\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(labels)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(bow_features.toarray(), encoded_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Reshape input data to 2D for Conv1D layer (samples, timesteps, features)\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "\n",
    "# Wrap the model using KerasClassifier\n",
    "model = KerasClassifier(build_fn=create_cnn_model, verbose=0)\n",
    "\n",
    "# Define the hyperparameter space for Bayesian Search\n",
    "param_space = {\n",
    "    'batch_size': Integer(16, 64),  # Batch size\n",
    "    'epochs': Integer(10, 50),  # Number of epochs\n",
    "    'learning_rate': Real(0.001, 0.1, prior='log-uniform'),  # Learning rate for the optimizer\n",
    "    'dropout_rate': Real(0.3, 0.7),  # Dropout rate for regularization\n",
    "    'filters': Integer(64, 128),  # Number of filters in Conv1D\n",
    "    'kernel_size': Integer(3, 7)  # Kernel size in Conv1D\n",
    "}\n",
    "\n",
    "# Set up Bayesian Search with cross-validation\n",
    "bayes_search = BayesSearchCV(\n",
    "    estimator=model,\n",
    "    search_spaces=param_space,\n",
    "    n_iter=32,  # Number of parameter settings that are sampled\n",
    "    cv=3,  # 3-fold cross-validation\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,  # Use all available cores\n",
    "    verbose=2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit Bayesian Search\n",
    "bayes_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model from Bayesian Search\n",
    "best_model = bayes_search.best_estimator_\n",
    "\n",
    "# Make predictions on the testing set with the best model\n",
    "y_pred_prob = best_model.predict(X_test)\n",
    "y_pred = (y_pred_prob > 0.5).astype(\"int32\").flatten()\n",
    "\n",
    "# Evaluate the model's performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"Best Model Parameters from Bayesian Search:\", bayes_search.best_params_)\n",
    "print(\"Convolutional Neural Network Model Performance after Bayesian Search:\")\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN: Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Set parameters\n",
    "max_words = 10000  # Maximum number of words to consider in the tokenizer\n",
    "max_len = 100  # Maximum length of the sequences (padded/truncated)\n",
    "\n",
    "# Prepare the tokenizer and fit on the reviews\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(df['preprocessed_review'])\n",
    "\n",
    "# Convert text to sequences of integers\n",
    "sequences = tokenizer.texts_to_sequences(df['preprocessed_review'])\n",
    "\n",
    "# Pad sequences to ensure they all have the same length\n",
    "X = pad_sequences(sequences, maxlen=max_len)\n",
    "\n",
    "# Encode the sentiment labels\n",
    "labels = df['sentiment']\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(labels)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Function to create the RNN model (using LSTM), required for KerasClassifier\n",
    "def create_rnn_model(learning_rate=0.001, lstm_units=128, dropout_rate=0.5):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=max_words, output_dim=128, input_length=max_len))  # Embedding layer\n",
    "    model.add(LSTM(lstm_units, return_sequences=False))  # LSTM layer\n",
    "    model.add(Dropout(dropout_rate))  # Dropout for regularization\n",
    "    model.add(Dense(64, activation='relu'))  # Fully connected layer\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1, activation='sigmoid'))  # Output layer for binary classification\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Wrap the model using KerasClassifier\n",
    "model = KerasClassifier(build_fn=create_rnn_model, verbose=0)\n",
    "\n",
    "# Define the hyperparameter grid for Grid Search\n",
    "param_grid = {\n",
    "    'batch_size': [32, 64],  # Batch size\n",
    "    'epochs': [10, 20],  # Number of epochs\n",
    "    'learning_rate': [0.001, 0.01],  # Learning rate for the optimizer\n",
    "    'lstm_units': [64, 128],  # Number of LSTM units\n",
    "    'dropout_rate': [0.3, 0.5]  # Dropout rate for regularization\n",
    "}\n",
    "\n",
    "# Set up Grid Search with cross-validation\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=3, scoring='accuracy', n_jobs=-1, verbose=2)\n",
    "\n",
    "# Fit Grid Search\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model from Grid Search\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the testing set with the best model\n",
    "y_pred_prob = best_model.predict(X_test)\n",
    "y_pred = (y_pred_prob > 0.5).astype(\"int32\").flatten()\n",
    "\n",
    "# Evaluate the model's performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"Best Model Parameters from Grid Search:\", grid_search.best_params_)\n",
    "print(\"Recurrent Neural Network (LSTM) Model Performance after Grid Search:\")\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN: Bayes Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Integer\n",
    "\n",
    "# Set parameters\n",
    "max_words = 10000  # Maximum number of words to consider in the tokenizer\n",
    "max_len = 100  # Maximum length of the sequences (padded/truncated)\n",
    "\n",
    "# Prepare the tokenizer and fit on the reviews\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(df['preprocessed_review'])\n",
    "\n",
    "# Convert text to sequences of integers\n",
    "sequences = tokenizer.texts_to_sequences(df['preprocessed_review'])\n",
    "\n",
    "# Pad sequences to ensure they all have the same length\n",
    "X = pad_sequences(sequences, maxlen=max_len)\n",
    "\n",
    "# Encode the sentiment labels\n",
    "labels = df['sentiment']\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(labels)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Function to create the RNN model (using LSTM), required for KerasClassifier\n",
    "def create_rnn_model(learning_rate=0.001, lstm_units=128, dropout_rate=0.5):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=max_words, output_dim=128, input_length=max_len))  # Embedding layer\n",
    "    model.add(LSTM(lstm_units, return_sequences=False))  # LSTM layer\n",
    "    model.add(Dropout(dropout_rate))  # Dropout for regularization\n",
    "    model.add(Dense(64, activation='relu'))  # Fully connected layer\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1, activation='sigmoid'))  # Output layer for binary classification\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Wrap the model using KerasClassifier\n",
    "model = KerasClassifier(build_fn=create_rnn_model, verbose=0)\n",
    "\n",
    "# Define the hyperparameter space for Bayesian Search\n",
    "param_space = {\n",
    "    'batch_size': Integer(32, 64),  # Batch size\n",
    "    'epochs': Integer(10, 50),  # Number of epochs\n",
    "    'learning_rate': Real(0.001, 0.01, prior='log-uniform'),  # Learning rate for the optimizer\n",
    "    'lstm_units': Integer(64, 128),  # Number of LSTM units\n",
    "    'dropout_rate': Real(0.3, 0.7)  # Dropout rate for regularization\n",
    "}\n",
    "\n",
    "# Set up Bayesian Search with cross-validation\n",
    "bayes_search = BayesSearchCV(\n",
    "    estimator=model,\n",
    "    search_spaces=param_space,\n",
    "    n_iter=32,  # Number of parameter settings that are sampled\n",
    "    cv=3,  # 3-fold cross-validation\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,  # Use all available cores\n",
    "    verbose=2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit Bayesian Search\n",
    "bayes_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model from Bayesian Search\n",
    "best_model = bayes_search.best_estimator_\n",
    "\n",
    "# Make predictions on the testing set with the best model\n",
    "y_pred_prob = best_model.predict(X_test)\n",
    "y_pred = (y_pred_prob > 0.5).astype(\"int32\").flatten()\n",
    "\n",
    "# Evaluate the model's performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"Best Model Parameters from Bayesian Search:\", bayes_search.best_params_)\n",
    "print(\"Recurrent Neural Network (LSTM) Model Performance after Bayesian Search:\")\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Model Performance:\n",
      "Accuracy: 0.8949\n",
      "Precision: 0.8854\n",
      "Recall: 0.9091\n",
      "F1 Score: 0.8971\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_features = vectorizer.fit_transform(df['preprocessed_review'])\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Get the sentiment labels from the preprocessed DataFrame\n",
    "labels = df['sentiment']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(tfidf_features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, pos_label='positive')\n",
    "recall = recall_score(y_test, y_pred, pos_label='positive')\n",
    "f1 = f1_score(y_test, y_pred, pos_label='positive')\n",
    "\n",
    "print(\"TF-IDF Model Performance:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Embeddings Model Performance:\n",
      "Accuracy: 0.8484\n",
      "Precision: 0.8461\n",
      "Recall: 0.8545\n",
      "F1 Score: 0.8503\n"
     ]
    }
   ],
   "source": [
    "# Word Embeddings\n",
    "from gensim.models import KeyedVectors\n",
    "import gensim.downloader as api\n",
    "import numpy as np\n",
    "\n",
    "# Load the pre-trained Word2Vec model from Gensim's API\n",
    "path = api.load(\"word2vec-google-news-300\", return_path=True)\n",
    "word2vec_model = KeyedVectors.load_word2vec_format(path, binary=True)\n",
    "\n",
    "def get_review_embedding(review):\n",
    "    words = review.split()\n",
    "    word_embeddings = [word2vec_model[word] for word in words if word in word2vec_model.key_to_index]\n",
    "    if len(word_embeddings) > 0:\n",
    "        return np.mean(word_embeddings, axis=0)\n",
    "    else:\n",
    "        return np.zeros(word2vec_model.vector_size)\n",
    "\n",
    "# Assuming 'text' is the column in your DataFrame containing the reviews\n",
    "review_embeddings = [get_review_embedding(review) for review in df['preprocessed_review']]\n",
    "\n",
    "# Convert the list of embeddings to a numpy array for further processing\n",
    "review_embeddings = np.array(review_embeddings)\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Assuming 'sentiment' is the column in your DataFrame containing the labels (0 for negative, 1 for positive)\n",
    "labels = df['sentiment'].values\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(review_embeddings, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a logistic regression model\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, pos_label='positive')\n",
    "recall = recall_score(y_test, y_pred, pos_label='positive')\n",
    "f1 = f1_score(y_test, y_pred, pos_label='positive')\n",
    "\n",
    "# Print the results\n",
    "print(\"Word Embeddings Model Performance:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
